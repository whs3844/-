{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2eaf8bac",
   "metadata": {},
   "outputs": [],
   "source": [
    "#단순 선형 회귀\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "14cd4913",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on package sklearn.linear_model in sklearn:\n",
      "\n",
      "NAME\n",
      "    sklearn.linear_model - The :mod:`sklearn.linear_model` module implements a variety of linear models.\n",
      "\n",
      "PACKAGE CONTENTS\n",
      "    _base\n",
      "    _bayes\n",
      "    _cd_fast\n",
      "    _coordinate_descent\n",
      "    _glm (package)\n",
      "    _huber\n",
      "    _least_angle\n",
      "    _logistic\n",
      "    _omp\n",
      "    _passive_aggressive\n",
      "    _perceptron\n",
      "    _ransac\n",
      "    _ridge\n",
      "    _sag\n",
      "    _sag_fast\n",
      "    _sgd_fast\n",
      "    _stochastic_gradient\n",
      "    _theil_sen\n",
      "    setup\n",
      "    tests (package)\n",
      "\n",
      "CLASSES\n",
      "    sklearn.base.BaseEstimator(builtins.object)\n",
      "        sklearn.linear_model._huber.HuberRegressor(sklearn.linear_model._base.LinearModel, sklearn.base.RegressorMixin, sklearn.base.BaseEstimator)\n",
      "        sklearn.linear_model._logistic.LogisticRegression(sklearn.linear_model._base.LinearClassifierMixin, sklearn.linear_model._base.SparseCoefMixin, sklearn.base.BaseEstimator)\n",
      "            sklearn.linear_model._logistic.LogisticRegressionCV(sklearn.linear_model._logistic.LogisticRegression, sklearn.linear_model._base.LinearClassifierMixin, sklearn.base.BaseEstimator)\n",
      "        sklearn.linear_model._ransac.RANSACRegressor(sklearn.base.MetaEstimatorMixin, sklearn.base.RegressorMixin, sklearn.base.MultiOutputMixin, sklearn.base.BaseEstimator)\n",
      "    sklearn.base.MetaEstimatorMixin(builtins.object)\n",
      "        sklearn.linear_model._ransac.RANSACRegressor(sklearn.base.MetaEstimatorMixin, sklearn.base.RegressorMixin, sklearn.base.MultiOutputMixin, sklearn.base.BaseEstimator)\n",
      "    sklearn.base.MultiOutputMixin(builtins.object)\n",
      "        sklearn.linear_model._base.LinearRegression(sklearn.base.MultiOutputMixin, sklearn.base.RegressorMixin, sklearn.linear_model._base.LinearModel)\n",
      "        sklearn.linear_model._coordinate_descent.ElasticNet(sklearn.base.MultiOutputMixin, sklearn.base.RegressorMixin, sklearn.linear_model._base.LinearModel)\n",
      "            sklearn.linear_model._coordinate_descent.Lasso\n",
      "                sklearn.linear_model._coordinate_descent.MultiTaskElasticNet\n",
      "                    sklearn.linear_model._coordinate_descent.MultiTaskLasso\n",
      "        sklearn.linear_model._least_angle.Lars(sklearn.base.MultiOutputMixin, sklearn.base.RegressorMixin, sklearn.linear_model._base.LinearModel)\n",
      "            sklearn.linear_model._least_angle.LarsCV\n",
      "                sklearn.linear_model._least_angle.LassoLarsCV\n",
      "            sklearn.linear_model._least_angle.LassoLars\n",
      "                sklearn.linear_model._least_angle.LassoLarsIC\n",
      "        sklearn.linear_model._omp.OrthogonalMatchingPursuit(sklearn.base.MultiOutputMixin, sklearn.base.RegressorMixin, sklearn.linear_model._base.LinearModel)\n",
      "        sklearn.linear_model._ransac.RANSACRegressor(sklearn.base.MetaEstimatorMixin, sklearn.base.RegressorMixin, sklearn.base.MultiOutputMixin, sklearn.base.BaseEstimator)\n",
      "        sklearn.linear_model._ridge.Ridge(sklearn.base.MultiOutputMixin, sklearn.base.RegressorMixin, sklearn.linear_model._ridge._BaseRidge)\n",
      "        sklearn.linear_model._ridge.RidgeCV(sklearn.base.MultiOutputMixin, sklearn.base.RegressorMixin, sklearn.linear_model._ridge._BaseRidgeCV)\n",
      "    sklearn.base.RegressorMixin(builtins.object)\n",
      "        sklearn.linear_model._base.LinearRegression(sklearn.base.MultiOutputMixin, sklearn.base.RegressorMixin, sklearn.linear_model._base.LinearModel)\n",
      "        sklearn.linear_model._bayes.ARDRegression(sklearn.base.RegressorMixin, sklearn.linear_model._base.LinearModel)\n",
      "        sklearn.linear_model._bayes.BayesianRidge(sklearn.base.RegressorMixin, sklearn.linear_model._base.LinearModel)\n",
      "        sklearn.linear_model._coordinate_descent.ElasticNet(sklearn.base.MultiOutputMixin, sklearn.base.RegressorMixin, sklearn.linear_model._base.LinearModel)\n",
      "            sklearn.linear_model._coordinate_descent.Lasso\n",
      "                sklearn.linear_model._coordinate_descent.MultiTaskElasticNet\n",
      "                    sklearn.linear_model._coordinate_descent.MultiTaskLasso\n",
      "        sklearn.linear_model._coordinate_descent.ElasticNetCV(sklearn.base.RegressorMixin, sklearn.linear_model._coordinate_descent.LinearModelCV)\n",
      "        sklearn.linear_model._coordinate_descent.LassoCV(sklearn.base.RegressorMixin, sklearn.linear_model._coordinate_descent.LinearModelCV)\n",
      "        sklearn.linear_model._coordinate_descent.MultiTaskElasticNetCV(sklearn.base.RegressorMixin, sklearn.linear_model._coordinate_descent.LinearModelCV)\n",
      "        sklearn.linear_model._coordinate_descent.MultiTaskLassoCV(sklearn.base.RegressorMixin, sklearn.linear_model._coordinate_descent.LinearModelCV)\n",
      "        sklearn.linear_model._huber.HuberRegressor(sklearn.linear_model._base.LinearModel, sklearn.base.RegressorMixin, sklearn.base.BaseEstimator)\n",
      "        sklearn.linear_model._least_angle.Lars(sklearn.base.MultiOutputMixin, sklearn.base.RegressorMixin, sklearn.linear_model._base.LinearModel)\n",
      "            sklearn.linear_model._least_angle.LarsCV\n",
      "                sklearn.linear_model._least_angle.LassoLarsCV\n",
      "            sklearn.linear_model._least_angle.LassoLars\n",
      "                sklearn.linear_model._least_angle.LassoLarsIC\n",
      "        sklearn.linear_model._omp.OrthogonalMatchingPursuit(sklearn.base.MultiOutputMixin, sklearn.base.RegressorMixin, sklearn.linear_model._base.LinearModel)\n",
      "        sklearn.linear_model._omp.OrthogonalMatchingPursuitCV(sklearn.base.RegressorMixin, sklearn.linear_model._base.LinearModel)\n",
      "        sklearn.linear_model._ransac.RANSACRegressor(sklearn.base.MetaEstimatorMixin, sklearn.base.RegressorMixin, sklearn.base.MultiOutputMixin, sklearn.base.BaseEstimator)\n",
      "        sklearn.linear_model._ridge.Ridge(sklearn.base.MultiOutputMixin, sklearn.base.RegressorMixin, sklearn.linear_model._ridge._BaseRidge)\n",
      "        sklearn.linear_model._ridge.RidgeCV(sklearn.base.MultiOutputMixin, sklearn.base.RegressorMixin, sklearn.linear_model._ridge._BaseRidgeCV)\n",
      "        sklearn.linear_model._theil_sen.TheilSenRegressor(sklearn.base.RegressorMixin, sklearn.linear_model._base.LinearModel)\n",
      "    sklearn.linear_model._base.LinearClassifierMixin(sklearn.base.ClassifierMixin)\n",
      "        sklearn.linear_model._logistic.LogisticRegression(sklearn.linear_model._base.LinearClassifierMixin, sklearn.linear_model._base.SparseCoefMixin, sklearn.base.BaseEstimator)\n",
      "            sklearn.linear_model._logistic.LogisticRegressionCV(sklearn.linear_model._logistic.LogisticRegression, sklearn.linear_model._base.LinearClassifierMixin, sklearn.base.BaseEstimator)\n",
      "        sklearn.linear_model._ridge.RidgeClassifier(sklearn.linear_model._base.LinearClassifierMixin, sklearn.linear_model._ridge._BaseRidge)\n",
      "        sklearn.linear_model._ridge.RidgeClassifierCV(sklearn.linear_model._base.LinearClassifierMixin, sklearn.linear_model._ridge._BaseRidgeCV)\n",
      "    sklearn.linear_model._base.LinearModel(sklearn.base.BaseEstimator)\n",
      "        sklearn.linear_model._base.LinearRegression(sklearn.base.MultiOutputMixin, sklearn.base.RegressorMixin, sklearn.linear_model._base.LinearModel)\n",
      "        sklearn.linear_model._bayes.ARDRegression(sklearn.base.RegressorMixin, sklearn.linear_model._base.LinearModel)\n",
      "        sklearn.linear_model._bayes.BayesianRidge(sklearn.base.RegressorMixin, sklearn.linear_model._base.LinearModel)\n",
      "        sklearn.linear_model._coordinate_descent.ElasticNet(sklearn.base.MultiOutputMixin, sklearn.base.RegressorMixin, sklearn.linear_model._base.LinearModel)\n",
      "            sklearn.linear_model._coordinate_descent.Lasso\n",
      "                sklearn.linear_model._coordinate_descent.MultiTaskElasticNet\n",
      "                    sklearn.linear_model._coordinate_descent.MultiTaskLasso\n",
      "        sklearn.linear_model._huber.HuberRegressor(sklearn.linear_model._base.LinearModel, sklearn.base.RegressorMixin, sklearn.base.BaseEstimator)\n",
      "        sklearn.linear_model._least_angle.Lars(sklearn.base.MultiOutputMixin, sklearn.base.RegressorMixin, sklearn.linear_model._base.LinearModel)\n",
      "            sklearn.linear_model._least_angle.LarsCV\n",
      "                sklearn.linear_model._least_angle.LassoLarsCV\n",
      "            sklearn.linear_model._least_angle.LassoLars\n",
      "                sklearn.linear_model._least_angle.LassoLarsIC\n",
      "        sklearn.linear_model._omp.OrthogonalMatchingPursuit(sklearn.base.MultiOutputMixin, sklearn.base.RegressorMixin, sklearn.linear_model._base.LinearModel)\n",
      "        sklearn.linear_model._omp.OrthogonalMatchingPursuitCV(sklearn.base.RegressorMixin, sklearn.linear_model._base.LinearModel)\n",
      "        sklearn.linear_model._theil_sen.TheilSenRegressor(sklearn.base.RegressorMixin, sklearn.linear_model._base.LinearModel)\n",
      "    sklearn.linear_model._base.SparseCoefMixin(builtins.object)\n",
      "        sklearn.linear_model._logistic.LogisticRegression(sklearn.linear_model._base.LinearClassifierMixin, sklearn.linear_model._base.SparseCoefMixin, sklearn.base.BaseEstimator)\n",
      "            sklearn.linear_model._logistic.LogisticRegressionCV(sklearn.linear_model._logistic.LogisticRegression, sklearn.linear_model._base.LinearClassifierMixin, sklearn.base.BaseEstimator)\n",
      "    sklearn.linear_model._coordinate_descent.LinearModelCV(sklearn.base.MultiOutputMixin, sklearn.linear_model._base.LinearModel)\n",
      "        sklearn.linear_model._coordinate_descent.ElasticNetCV(sklearn.base.RegressorMixin, sklearn.linear_model._coordinate_descent.LinearModelCV)\n",
      "        sklearn.linear_model._coordinate_descent.LassoCV(sklearn.base.RegressorMixin, sklearn.linear_model._coordinate_descent.LinearModelCV)\n",
      "        sklearn.linear_model._coordinate_descent.MultiTaskElasticNetCV(sklearn.base.RegressorMixin, sklearn.linear_model._coordinate_descent.LinearModelCV)\n",
      "        sklearn.linear_model._coordinate_descent.MultiTaskLassoCV(sklearn.base.RegressorMixin, sklearn.linear_model._coordinate_descent.LinearModelCV)\n",
      "    sklearn.linear_model._glm.glm.GeneralizedLinearRegressor(sklearn.base.RegressorMixin, sklearn.base.BaseEstimator)\n",
      "        sklearn.linear_model._glm.glm.GammaRegressor\n",
      "        sklearn.linear_model._glm.glm.PoissonRegressor\n",
      "        sklearn.linear_model._glm.glm.TweedieRegressor\n",
      "    sklearn.linear_model._ridge._BaseRidge(sklearn.linear_model._base.LinearModel)\n",
      "        sklearn.linear_model._ridge.Ridge(sklearn.base.MultiOutputMixin, sklearn.base.RegressorMixin, sklearn.linear_model._ridge._BaseRidge)\n",
      "        sklearn.linear_model._ridge.RidgeClassifier(sklearn.linear_model._base.LinearClassifierMixin, sklearn.linear_model._ridge._BaseRidge)\n",
      "    sklearn.linear_model._ridge._BaseRidgeCV(sklearn.linear_model._base.LinearModel)\n",
      "        sklearn.linear_model._ridge.RidgeCV(sklearn.base.MultiOutputMixin, sklearn.base.RegressorMixin, sklearn.linear_model._ridge._BaseRidgeCV)\n",
      "        sklearn.linear_model._ridge.RidgeClassifierCV(sklearn.linear_model._base.LinearClassifierMixin, sklearn.linear_model._ridge._BaseRidgeCV)\n",
      "    sklearn.linear_model._sgd_fast.Classification(sklearn.linear_model._sgd_fast.LossFunction)\n",
      "        sklearn.linear_model._sgd_fast.Hinge\n",
      "        sklearn.linear_model._sgd_fast.Log\n",
      "        sklearn.linear_model._sgd_fast.ModifiedHuber\n",
      "    sklearn.linear_model._sgd_fast.Regression(sklearn.linear_model._sgd_fast.LossFunction)\n",
      "        sklearn.linear_model._sgd_fast.Huber\n",
      "        sklearn.linear_model._sgd_fast.SquaredLoss\n",
      "    sklearn.linear_model._stochastic_gradient.BaseSGDClassifier(sklearn.linear_model._base.LinearClassifierMixin, sklearn.linear_model._stochastic_gradient.BaseSGD)\n",
      "        sklearn.linear_model._passive_aggressive.PassiveAggressiveClassifier\n",
      "        sklearn.linear_model._perceptron.Perceptron\n",
      "        sklearn.linear_model._stochastic_gradient.SGDClassifier\n",
      "    sklearn.linear_model._stochastic_gradient.BaseSGDRegressor(sklearn.base.RegressorMixin, sklearn.linear_model._stochastic_gradient.BaseSGD)\n",
      "        sklearn.linear_model._passive_aggressive.PassiveAggressiveRegressor\n",
      "        sklearn.linear_model._stochastic_gradient.SGDRegressor\n",
      "    \n",
      "    class ARDRegression(sklearn.base.RegressorMixin, sklearn.linear_model._base.LinearModel)\n",
      "     |  ARDRegression(*, n_iter=300, tol=0.001, alpha_1=1e-06, alpha_2=1e-06, lambda_1=1e-06, lambda_2=1e-06, compute_score=False, threshold_lambda=10000.0, fit_intercept=True, normalize=False, copy_X=True, verbose=False)\n",
      "     |  \n",
      "     |  Bayesian ARD regression.\n",
      "     |  \n",
      "     |  Fit the weights of a regression model, using an ARD prior. The weights of\n",
      "     |  the regression model are assumed to be in Gaussian distributions.\n",
      "     |  Also estimate the parameters lambda (precisions of the distributions of the\n",
      "     |  weights) and alpha (precision of the distribution of the noise).\n",
      "     |  The estimation is done by an iterative procedures (Evidence Maximization)\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <bayesian_regression>`.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  n_iter : int, default=300\n",
      "     |      Maximum number of iterations.\n",
      "     |  \n",
      "     |  tol : float, default=1e-3\n",
      "     |      Stop the algorithm if w has converged.\n",
      "     |  \n",
      "     |  alpha_1 : float, default=1e-6\n",
      "     |      Hyper-parameter : shape parameter for the Gamma distribution prior\n",
      "     |      over the alpha parameter.\n",
      "     |  \n",
      "     |  alpha_2 : float, default=1e-6\n",
      "     |      Hyper-parameter : inverse scale parameter (rate parameter) for the\n",
      "     |      Gamma distribution prior over the alpha parameter.\n",
      "     |  \n",
      "     |  lambda_1 : float, default=1e-6\n",
      "     |      Hyper-parameter : shape parameter for the Gamma distribution prior\n",
      "     |      over the lambda parameter.\n",
      "     |  \n",
      "     |  lambda_2 : float, default=1e-6\n",
      "     |      Hyper-parameter : inverse scale parameter (rate parameter) for the\n",
      "     |      Gamma distribution prior over the lambda parameter.\n",
      "     |  \n",
      "     |  compute_score : bool, default=False\n",
      "     |      If True, compute the objective function at each step of the model.\n",
      "     |  \n",
      "     |  threshold_lambda : float, default=10 000\n",
      "     |      threshold for removing (pruning) weights with high precision from\n",
      "     |      the computation.\n",
      "     |  \n",
      "     |  fit_intercept : bool, default=True\n",
      "     |      whether to calculate the intercept for this model. If set\n",
      "     |      to false, no intercept will be used in calculations\n",
      "     |      (i.e. data is expected to be centered).\n",
      "     |  \n",
      "     |  normalize : bool, default=False\n",
      "     |      This parameter is ignored when ``fit_intercept`` is set to False.\n",
      "     |      If True, the regressors X will be normalized before regression by\n",
      "     |      subtracting the mean and dividing by the l2-norm.\n",
      "     |      If you wish to standardize, please use\n",
      "     |      :class:`~sklearn.preprocessing.StandardScaler` before calling ``fit``\n",
      "     |      on an estimator with ``normalize=False``.\n",
      "     |  \n",
      "     |  copy_X : bool, default=True\n",
      "     |      If True, X will be copied; else, it may be overwritten.\n",
      "     |  \n",
      "     |  verbose : bool, default=False\n",
      "     |      Verbose mode when fitting the model.\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  coef_ : array-like of shape (n_features,)\n",
      "     |      Coefficients of the regression model (mean of distribution)\n",
      "     |  \n",
      "     |  alpha_ : float\n",
      "     |     estimated precision of the noise.\n",
      "     |  \n",
      "     |  lambda_ : array-like of shape (n_features,)\n",
      "     |     estimated precisions of the weights.\n",
      "     |  \n",
      "     |  sigma_ : array-like of shape (n_features, n_features)\n",
      "     |      estimated variance-covariance matrix of the weights\n",
      "     |  \n",
      "     |  scores_ : float\n",
      "     |      if computed, value of the objective function (to be maximized)\n",
      "     |  \n",
      "     |  intercept_ : float\n",
      "     |      Independent term in decision function. Set to 0.0 if\n",
      "     |      ``fit_intercept = False``.\n",
      "     |  \n",
      "     |  X_offset_ : float\n",
      "     |      If `normalize=True`, offset subtracted for centering data to a\n",
      "     |      zero mean.\n",
      "     |  \n",
      "     |  X_scale_ : float\n",
      "     |      If `normalize=True`, parameter used to scale data to a unit\n",
      "     |      standard deviation.\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  >>> from sklearn import linear_model\n",
      "     |  >>> clf = linear_model.ARDRegression()\n",
      "     |  >>> clf.fit([[0,0], [1, 1], [2, 2]], [0, 1, 2])\n",
      "     |  ARDRegression()\n",
      "     |  >>> clf.predict([[1, 1]])\n",
      "     |  array([1.])\n",
      "     |  \n",
      "     |  Notes\n",
      "     |  -----\n",
      "     |  For an example, see :ref:`examples/linear_model/plot_ard.py\n",
      "     |  <sphx_glr_auto_examples_linear_model_plot_ard.py>`.\n",
      "     |  \n",
      "     |  References\n",
      "     |  ----------\n",
      "     |  D. J. C. MacKay, Bayesian nonlinear modeling for the prediction\n",
      "     |  competition, ASHRAE Transactions, 1994.\n",
      "     |  \n",
      "     |  R. Salakhutdinov, Lecture notes on Statistical Machine Learning,\n",
      "     |  http://www.utstat.toronto.edu/~rsalakhu/sta4273/notes/Lecture2.pdf#page=15\n",
      "     |  Their beta is our ``self.alpha_``\n",
      "     |  Their alpha is our ``self.lambda_``\n",
      "     |  ARD is a little different than the slide: only dimensions/features for\n",
      "     |  which ``self.lambda_ < self.threshold_lambda`` are kept and the rest are\n",
      "     |  discarded.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      ARDRegression\n",
      "     |      sklearn.base.RegressorMixin\n",
      "     |      sklearn.linear_model._base.LinearModel\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, *, n_iter=300, tol=0.001, alpha_1=1e-06, alpha_2=1e-06, lambda_1=1e-06, lambda_2=1e-06, compute_score=False, threshold_lambda=10000.0, fit_intercept=True, normalize=False, copy_X=True, verbose=False)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  fit(self, X, y)\n",
      "     |      Fit the ARDRegression model according to the given training data\n",
      "     |      and parameters.\n",
      "     |      \n",
      "     |      Iterative procedure to maximize the evidence\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          Training vector, where n_samples in the number of samples and\n",
      "     |          n_features is the number of features.\n",
      "     |      y : array-like of shape (n_samples,)\n",
      "     |          Target values (integers). Will be cast to X's dtype if necessary\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : returns an instance of self.\n",
      "     |  \n",
      "     |  predict(self, X, return_std=False)\n",
      "     |      Predict using the linear model.\n",
      "     |      \n",
      "     |      In addition to the mean of the predictive distribution, also its\n",
      "     |      standard deviation can be returned.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          Samples.\n",
      "     |      \n",
      "     |      return_std : bool, default=False\n",
      "     |          Whether to return the standard deviation of posterior prediction.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      y_mean : array-like of shape (n_samples,)\n",
      "     |          Mean of predictive distribution of query points.\n",
      "     |      \n",
      "     |      y_std : array-like of shape (n_samples,)\n",
      "     |          Standard deviation of predictive distribution of query points.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.RegressorMixin:\n",
      "     |  \n",
      "     |  score(self, X, y, sample_weight=None)\n",
      "     |      Return the coefficient of determination :math:`R^2` of the\n",
      "     |      prediction.\n",
      "     |      \n",
      "     |      The coefficient :math:`R^2` is defined as :math:`(1 - \\frac{u}{v})`,\n",
      "     |      where :math:`u` is the residual sum of squares ``((y_true - y_pred)\n",
      "     |      ** 2).sum()`` and :math:`v` is the total sum of squares ``((y_true -\n",
      "     |      y_true.mean()) ** 2).sum()``. The best possible score is 1.0 and it\n",
      "     |      can be negative (because the model can be arbitrarily worse). A\n",
      "     |      constant model that always predicts the expected value of `y`,\n",
      "     |      disregarding the input features, would get a :math:`R^2` score of\n",
      "     |      0.0.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          Test samples. For some estimators this may be a precomputed\n",
      "     |          kernel matrix or a list of generic objects instead with shape\n",
      "     |          ``(n_samples, n_samples_fitted)``, where ``n_samples_fitted``\n",
      "     |          is the number of samples used in the fitting for the estimator.\n",
      "     |      \n",
      "     |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      "     |          True values for `X`.\n",
      "     |      \n",
      "     |      sample_weight : array-like of shape (n_samples,), default=None\n",
      "     |          Sample weights.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      score : float\n",
      "     |          :math:`R^2` of ``self.predict(X)`` wrt. `y`.\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      The :math:`R^2` score used when calling ``score`` on a regressor uses\n",
      "     |      ``multioutput='uniform_average'`` from version 0.23 to keep consistent\n",
      "     |      with default value of :func:`~sklearn.metrics.r2_score`.\n",
      "     |      This influences the ``score`` method of all the multioutput\n",
      "     |      regressors (except for\n",
      "     |      :class:`~sklearn.multioutput.MultiOutputRegressor`).\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.base.RegressorMixin:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |  \n",
      "     |  __repr__(self, N_CHAR_MAX=700)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  get_params(self, deep=True)\n",
      "     |      Get parameters for this estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      deep : bool, default=True\n",
      "     |          If True, will return the parameters for this estimator and\n",
      "     |          contained subobjects that are estimators.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      params : dict\n",
      "     |          Parameter names mapped to their values.\n",
      "     |  \n",
      "     |  set_params(self, **params)\n",
      "     |      Set the parameters of this estimator.\n",
      "     |      \n",
      "     |      The method works on simple estimators as well as on nested objects\n",
      "     |      (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n",
      "     |      parameters of the form ``<component>__<parameter>`` so that it's\n",
      "     |      possible to update each component of a nested object.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      **params : dict\n",
      "     |          Estimator parameters.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : estimator instance\n",
      "     |          Estimator instance.\n",
      "    \n",
      "    class BayesianRidge(sklearn.base.RegressorMixin, sklearn.linear_model._base.LinearModel)\n",
      "     |  BayesianRidge(*, n_iter=300, tol=0.001, alpha_1=1e-06, alpha_2=1e-06, lambda_1=1e-06, lambda_2=1e-06, alpha_init=None, lambda_init=None, compute_score=False, fit_intercept=True, normalize=False, copy_X=True, verbose=False)\n",
      "     |  \n",
      "     |  Bayesian ridge regression.\n",
      "     |  \n",
      "     |  Fit a Bayesian ridge model. See the Notes section for details on this\n",
      "     |  implementation and the optimization of the regularization parameters\n",
      "     |  lambda (precision of the weights) and alpha (precision of the noise).\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <bayesian_regression>`.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  n_iter : int, default=300\n",
      "     |      Maximum number of iterations. Should be greater than or equal to 1.\n",
      "     |  \n",
      "     |  tol : float, default=1e-3\n",
      "     |      Stop the algorithm if w has converged.\n",
      "     |  \n",
      "     |  alpha_1 : float, default=1e-6\n",
      "     |      Hyper-parameter : shape parameter for the Gamma distribution prior\n",
      "     |      over the alpha parameter.\n",
      "     |  \n",
      "     |  alpha_2 : float, default=1e-6\n",
      "     |      Hyper-parameter : inverse scale parameter (rate parameter) for the\n",
      "     |      Gamma distribution prior over the alpha parameter.\n",
      "     |  \n",
      "     |  lambda_1 : float, default=1e-6\n",
      "     |      Hyper-parameter : shape parameter for the Gamma distribution prior\n",
      "     |      over the lambda parameter.\n",
      "     |  \n",
      "     |  lambda_2 : float, default=1e-6\n",
      "     |      Hyper-parameter : inverse scale parameter (rate parameter) for the\n",
      "     |      Gamma distribution prior over the lambda parameter.\n",
      "     |  \n",
      "     |  alpha_init : float, default=None\n",
      "     |      Initial value for alpha (precision of the noise).\n",
      "     |      If not set, alpha_init is 1/Var(y).\n",
      "     |  \n",
      "     |          .. versionadded:: 0.22\n",
      "     |  \n",
      "     |  lambda_init : float, default=None\n",
      "     |      Initial value for lambda (precision of the weights).\n",
      "     |      If not set, lambda_init is 1.\n",
      "     |  \n",
      "     |          .. versionadded:: 0.22\n",
      "     |  \n",
      "     |  compute_score : bool, default=False\n",
      "     |      If True, compute the log marginal likelihood at each iteration of the\n",
      "     |      optimization.\n",
      "     |  \n",
      "     |  fit_intercept : bool, default=True\n",
      "     |      Whether to calculate the intercept for this model.\n",
      "     |      The intercept is not treated as a probabilistic parameter\n",
      "     |      and thus has no associated variance. If set\n",
      "     |      to False, no intercept will be used in calculations\n",
      "     |      (i.e. data is expected to be centered).\n",
      "     |  \n",
      "     |  normalize : bool, default=False\n",
      "     |      This parameter is ignored when ``fit_intercept`` is set to False.\n",
      "     |      If True, the regressors X will be normalized before regression by\n",
      "     |      subtracting the mean and dividing by the l2-norm.\n",
      "     |      If you wish to standardize, please use\n",
      "     |      :class:`~sklearn.preprocessing.StandardScaler` before calling ``fit``\n",
      "     |      on an estimator with ``normalize=False``.\n",
      "     |  \n",
      "     |  copy_X : bool, default=True\n",
      "     |      If True, X will be copied; else, it may be overwritten.\n",
      "     |  \n",
      "     |  verbose : bool, default=False\n",
      "     |      Verbose mode when fitting the model.\n",
      "     |  \n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  coef_ : array-like of shape (n_features,)\n",
      "     |      Coefficients of the regression model (mean of distribution)\n",
      "     |  \n",
      "     |  intercept_ : float\n",
      "     |      Independent term in decision function. Set to 0.0 if\n",
      "     |      ``fit_intercept = False``.\n",
      "     |  \n",
      "     |  alpha_ : float\n",
      "     |     Estimated precision of the noise.\n",
      "     |  \n",
      "     |  lambda_ : float\n",
      "     |     Estimated precision of the weights.\n",
      "     |  \n",
      "     |  sigma_ : array-like of shape (n_features, n_features)\n",
      "     |      Estimated variance-covariance matrix of the weights\n",
      "     |  \n",
      "     |  scores_ : array-like of shape (n_iter_+1,)\n",
      "     |      If computed_score is True, value of the log marginal likelihood (to be\n",
      "     |      maximized) at each iteration of the optimization. The array starts\n",
      "     |      with the value of the log marginal likelihood obtained for the initial\n",
      "     |      values of alpha and lambda and ends with the value obtained for the\n",
      "     |      estimated alpha and lambda.\n",
      "     |  \n",
      "     |  n_iter_ : int\n",
      "     |      The actual number of iterations to reach the stopping criterion.\n",
      "     |  \n",
      "     |  X_offset_ : float\n",
      "     |      If `normalize=True`, offset subtracted for centering data to a\n",
      "     |      zero mean.\n",
      "     |  \n",
      "     |  X_scale_ : float\n",
      "     |      If `normalize=True`, parameter used to scale data to a unit\n",
      "     |      standard deviation.\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  >>> from sklearn import linear_model\n",
      "     |  >>> clf = linear_model.BayesianRidge()\n",
      "     |  >>> clf.fit([[0,0], [1, 1], [2, 2]], [0, 1, 2])\n",
      "     |  BayesianRidge()\n",
      "     |  >>> clf.predict([[1, 1]])\n",
      "     |  array([1.])\n",
      "     |  \n",
      "     |  Notes\n",
      "     |  -----\n",
      "     |  There exist several strategies to perform Bayesian ridge regression. This\n",
      "     |  implementation is based on the algorithm described in Appendix A of\n",
      "     |  (Tipping, 2001) where updates of the regularization parameters are done as\n",
      "     |  suggested in (MacKay, 1992). Note that according to A New\n",
      "     |  View of Automatic Relevance Determination (Wipf and Nagarajan, 2008) these\n",
      "     |  update rules do not guarantee that the marginal likelihood is increasing\n",
      "     |  between two consecutive iterations of the optimization.\n",
      "     |  \n",
      "     |  References\n",
      "     |  ----------\n",
      "     |  D. J. C. MacKay, Bayesian Interpolation, Computation and Neural Systems,\n",
      "     |  Vol. 4, No. 3, 1992.\n",
      "     |  \n",
      "     |  M. E. Tipping, Sparse Bayesian Learning and the Relevance Vector Machine,\n",
      "     |  Journal of Machine Learning Research, Vol. 1, 2001.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      BayesianRidge\n",
      "     |      sklearn.base.RegressorMixin\n",
      "     |      sklearn.linear_model._base.LinearModel\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, *, n_iter=300, tol=0.001, alpha_1=1e-06, alpha_2=1e-06, lambda_1=1e-06, lambda_2=1e-06, alpha_init=None, lambda_init=None, compute_score=False, fit_intercept=True, normalize=False, copy_X=True, verbose=False)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  fit(self, X, y, sample_weight=None)\n",
      "     |      Fit the model\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : ndarray of shape (n_samples, n_features)\n",
      "     |          Training data\n",
      "     |      y : ndarray of shape (n_samples,)\n",
      "     |          Target values. Will be cast to X's dtype if necessary\n",
      "     |      \n",
      "     |      sample_weight : ndarray of shape (n_samples,), default=None\n",
      "     |          Individual weights for each sample\n",
      "     |      \n",
      "     |          .. versionadded:: 0.20\n",
      "     |             parameter *sample_weight* support to BayesianRidge.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : returns an instance of self.\n",
      "     |  \n",
      "     |  predict(self, X, return_std=False)\n",
      "     |      Predict using the linear model.\n",
      "     |      \n",
      "     |      In addition to the mean of the predictive distribution, also its\n",
      "     |      standard deviation can be returned.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          Samples.\n",
      "     |      \n",
      "     |      return_std : bool, default=False\n",
      "     |          Whether to return the standard deviation of posterior prediction.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      y_mean : array-like of shape (n_samples,)\n",
      "     |          Mean of predictive distribution of query points.\n",
      "     |      \n",
      "     |      y_std : array-like of shape (n_samples,)\n",
      "     |          Standard deviation of predictive distribution of query points.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.RegressorMixin:\n",
      "     |  \n",
      "     |  score(self, X, y, sample_weight=None)\n",
      "     |      Return the coefficient of determination :math:`R^2` of the\n",
      "     |      prediction.\n",
      "     |      \n",
      "     |      The coefficient :math:`R^2` is defined as :math:`(1 - \\frac{u}{v})`,\n",
      "     |      where :math:`u` is the residual sum of squares ``((y_true - y_pred)\n",
      "     |      ** 2).sum()`` and :math:`v` is the total sum of squares ``((y_true -\n",
      "     |      y_true.mean()) ** 2).sum()``. The best possible score is 1.0 and it\n",
      "     |      can be negative (because the model can be arbitrarily worse). A\n",
      "     |      constant model that always predicts the expected value of `y`,\n",
      "     |      disregarding the input features, would get a :math:`R^2` score of\n",
      "     |      0.0.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          Test samples. For some estimators this may be a precomputed\n",
      "     |          kernel matrix or a list of generic objects instead with shape\n",
      "     |          ``(n_samples, n_samples_fitted)``, where ``n_samples_fitted``\n",
      "     |          is the number of samples used in the fitting for the estimator.\n",
      "     |      \n",
      "     |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      "     |          True values for `X`.\n",
      "     |      \n",
      "     |      sample_weight : array-like of shape (n_samples,), default=None\n",
      "     |          Sample weights.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      score : float\n",
      "     |          :math:`R^2` of ``self.predict(X)`` wrt. `y`.\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      The :math:`R^2` score used when calling ``score`` on a regressor uses\n",
      "     |      ``multioutput='uniform_average'`` from version 0.23 to keep consistent\n",
      "     |      with default value of :func:`~sklearn.metrics.r2_score`.\n",
      "     |      This influences the ``score`` method of all the multioutput\n",
      "     |      regressors (except for\n",
      "     |      :class:`~sklearn.multioutput.MultiOutputRegressor`).\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.base.RegressorMixin:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |  \n",
      "     |  __repr__(self, N_CHAR_MAX=700)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  get_params(self, deep=True)\n",
      "     |      Get parameters for this estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      deep : bool, default=True\n",
      "     |          If True, will return the parameters for this estimator and\n",
      "     |          contained subobjects that are estimators.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      params : dict\n",
      "     |          Parameter names mapped to their values.\n",
      "     |  \n",
      "     |  set_params(self, **params)\n",
      "     |      Set the parameters of this estimator.\n",
      "     |      \n",
      "     |      The method works on simple estimators as well as on nested objects\n",
      "     |      (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n",
      "     |      parameters of the form ``<component>__<parameter>`` so that it's\n",
      "     |      possible to update each component of a nested object.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      **params : dict\n",
      "     |          Estimator parameters.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : estimator instance\n",
      "     |          Estimator instance.\n",
      "    \n",
      "    class ElasticNet(sklearn.base.MultiOutputMixin, sklearn.base.RegressorMixin, sklearn.linear_model._base.LinearModel)\n",
      "     |  ElasticNet(alpha=1.0, *, l1_ratio=0.5, fit_intercept=True, normalize=False, precompute=False, max_iter=1000, copy_X=True, tol=0.0001, warm_start=False, positive=False, random_state=None, selection='cyclic')\n",
      "     |  \n",
      "     |  Linear regression with combined L1 and L2 priors as regularizer.\n",
      "     |  \n",
      "     |  Minimizes the objective function::\n",
      "     |  \n",
      "     |          1 / (2 * n_samples) * ||y - Xw||^2_2\n",
      "     |          + alpha * l1_ratio * ||w||_1\n",
      "     |          + 0.5 * alpha * (1 - l1_ratio) * ||w||^2_2\n",
      "     |  \n",
      "     |  If you are interested in controlling the L1 and L2 penalty\n",
      "     |  separately, keep in mind that this is equivalent to::\n",
      "     |  \n",
      "     |          a * ||w||_1 + 0.5 * b * ||w||_2^2\n",
      "     |  \n",
      "     |  where::\n",
      "     |  \n",
      "     |          alpha = a + b and l1_ratio = a / (a + b)\n",
      "     |  \n",
      "     |  The parameter l1_ratio corresponds to alpha in the glmnet R package while\n",
      "     |  alpha corresponds to the lambda parameter in glmnet. Specifically, l1_ratio\n",
      "     |  = 1 is the lasso penalty. Currently, l1_ratio <= 0.01 is not reliable,\n",
      "     |  unless you supply your own sequence of alpha.\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <elastic_net>`.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  alpha : float, default=1.0\n",
      "     |      Constant that multiplies the penalty terms. Defaults to 1.0.\n",
      "     |      See the notes for the exact mathematical meaning of this\n",
      "     |      parameter. ``alpha = 0`` is equivalent to an ordinary least square,\n",
      "     |      solved by the :class:`LinearRegression` object. For numerical\n",
      "     |      reasons, using ``alpha = 0`` with the ``Lasso`` object is not advised.\n",
      "     |      Given this, you should use the :class:`LinearRegression` object.\n",
      "     |  \n",
      "     |  l1_ratio : float, default=0.5\n",
      "     |      The ElasticNet mixing parameter, with ``0 <= l1_ratio <= 1``. For\n",
      "     |      ``l1_ratio = 0`` the penalty is an L2 penalty. ``For l1_ratio = 1`` it\n",
      "     |      is an L1 penalty.  For ``0 < l1_ratio < 1``, the penalty is a\n",
      "     |      combination of L1 and L2.\n",
      "     |  \n",
      "     |  fit_intercept : bool, default=True\n",
      "     |      Whether the intercept should be estimated or not. If ``False``, the\n",
      "     |      data is assumed to be already centered.\n",
      "     |  \n",
      "     |  normalize : bool, default=False\n",
      "     |      This parameter is ignored when ``fit_intercept`` is set to False.\n",
      "     |      If True, the regressors X will be normalized before regression by\n",
      "     |      subtracting the mean and dividing by the l2-norm.\n",
      "     |      If you wish to standardize, please use\n",
      "     |      :class:`~sklearn.preprocessing.StandardScaler` before calling ``fit``\n",
      "     |      on an estimator with ``normalize=False``.\n",
      "     |  \n",
      "     |  precompute : bool or array-like of shape (n_features, n_features),                 default=False\n",
      "     |      Whether to use a precomputed Gram matrix to speed up\n",
      "     |      calculations. The Gram matrix can also be passed as argument.\n",
      "     |      For sparse input this option is always ``False`` to preserve sparsity.\n",
      "     |  \n",
      "     |  max_iter : int, default=1000\n",
      "     |      The maximum number of iterations.\n",
      "     |  \n",
      "     |  copy_X : bool, default=True\n",
      "     |      If ``True``, X will be copied; else, it may be overwritten.\n",
      "     |  \n",
      "     |  tol : float, default=1e-4\n",
      "     |      The tolerance for the optimization: if the updates are\n",
      "     |      smaller than ``tol``, the optimization code checks the\n",
      "     |      dual gap for optimality and continues until it is smaller\n",
      "     |      than ``tol``.\n",
      "     |  \n",
      "     |  warm_start : bool, default=False\n",
      "     |      When set to ``True``, reuse the solution of the previous call to fit as\n",
      "     |      initialization, otherwise, just erase the previous solution.\n",
      "     |      See :term:`the Glossary <warm_start>`.\n",
      "     |  \n",
      "     |  positive : bool, default=False\n",
      "     |      When set to ``True``, forces the coefficients to be positive.\n",
      "     |  \n",
      "     |  random_state : int, RandomState instance, default=None\n",
      "     |      The seed of the pseudo random number generator that selects a random\n",
      "     |      feature to update. Used when ``selection`` == 'random'.\n",
      "     |      Pass an int for reproducible output across multiple function calls.\n",
      "     |      See :term:`Glossary <random_state>`.\n",
      "     |  \n",
      "     |  selection : {'cyclic', 'random'}, default='cyclic'\n",
      "     |      If set to 'random', a random coefficient is updated every iteration\n",
      "     |      rather than looping over features sequentially by default. This\n",
      "     |      (setting to 'random') often leads to significantly faster convergence\n",
      "     |      especially when tol is higher than 1e-4.\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  coef_ : ndarray of shape (n_features,) or (n_targets, n_features)\n",
      "     |      Parameter vector (w in the cost function formula).\n",
      "     |  \n",
      "     |  sparse_coef_ : sparse matrix of shape (n_features,) or             (n_tasks, n_features)\n",
      "     |      Sparse representation of the `coef_`.\n",
      "     |  \n",
      "     |  intercept_ : float or ndarray of shape (n_targets,)\n",
      "     |      Independent term in decision function.\n",
      "     |  \n",
      "     |  n_iter_ : list of int\n",
      "     |      Number of iterations run by the coordinate descent solver to reach\n",
      "     |      the specified tolerance.\n",
      "     |  \n",
      "     |  dual_gap_ : float or ndarray of shape (n_targets,)\n",
      "     |      Given param alpha, the dual gaps at the end of the optimization,\n",
      "     |      same shape as each observation of y.\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  >>> from sklearn.linear_model import ElasticNet\n",
      "     |  >>> from sklearn.datasets import make_regression\n",
      "     |  \n",
      "     |  >>> X, y = make_regression(n_features=2, random_state=0)\n",
      "     |  >>> regr = ElasticNet(random_state=0)\n",
      "     |  >>> regr.fit(X, y)\n",
      "     |  ElasticNet(random_state=0)\n",
      "     |  >>> print(regr.coef_)\n",
      "     |  [18.83816048 64.55968825]\n",
      "     |  >>> print(regr.intercept_)\n",
      "     |  1.451...\n",
      "     |  >>> print(regr.predict([[0, 0]]))\n",
      "     |  [1.451...]\n",
      "     |  \n",
      "     |  \n",
      "     |  Notes\n",
      "     |  -----\n",
      "     |  To avoid unnecessary memory duplication the X argument of the fit method\n",
      "     |  should be directly passed as a Fortran-contiguous numpy array.\n",
      "     |  \n",
      "     |  See Also\n",
      "     |  --------\n",
      "     |  ElasticNetCV : Elastic net model with best model selection by\n",
      "     |      cross-validation.\n",
      "     |  SGDRegressor : Implements elastic net regression with incremental training.\n",
      "     |  SGDClassifier : Implements logistic regression with elastic net penalty\n",
      "     |      (``SGDClassifier(loss=\"log\", penalty=\"elasticnet\")``).\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      ElasticNet\n",
      "     |      sklearn.base.MultiOutputMixin\n",
      "     |      sklearn.base.RegressorMixin\n",
      "     |      sklearn.linear_model._base.LinearModel\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, alpha=1.0, *, l1_ratio=0.5, fit_intercept=True, normalize=False, precompute=False, max_iter=1000, copy_X=True, tol=0.0001, warm_start=False, positive=False, random_state=None, selection='cyclic')\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  fit(self, X, y, sample_weight=None, check_input=True)\n",
      "     |      Fit model with coordinate descent.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {ndarray, sparse matrix} of (n_samples, n_features)\n",
      "     |          Data.\n",
      "     |      \n",
      "     |      y : {ndarray, sparse matrix} of shape (n_samples,) or             (n_samples, n_targets)\n",
      "     |          Target. Will be cast to X's dtype if necessary.\n",
      "     |      \n",
      "     |      sample_weight : float or array-like of shape (n_samples,), default=None\n",
      "     |          Sample weight.\n",
      "     |      \n",
      "     |          .. versionadded:: 0.23\n",
      "     |      \n",
      "     |      check_input : bool, default=True\n",
      "     |          Allow to bypass several input checking.\n",
      "     |          Don't use this parameter unless you know what you do.\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      \n",
      "     |      Coordinate descent is an algorithm that considers each column of\n",
      "     |      data at a time hence it will automatically convert the X input\n",
      "     |      as a Fortran-contiguous numpy array if necessary.\n",
      "     |      \n",
      "     |      To avoid memory re-allocation it is advised to allocate the\n",
      "     |      initial data in memory directly using that format.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods defined here:\n",
      "     |  \n",
      "     |  path = enet_path(X, y, *, l1_ratio=0.5, eps=0.001, n_alphas=100, alphas=None, precompute='auto', Xy=None, copy_X=True, coef_init=None, verbose=False, return_n_iter=False, positive=False, check_input=True, **params)\n",
      "     |      Compute elastic net path with coordinate descent.\n",
      "     |      \n",
      "     |      The elastic net optimization function varies for mono and multi-outputs.\n",
      "     |      \n",
      "     |      For mono-output tasks it is::\n",
      "     |      \n",
      "     |          1 / (2 * n_samples) * ||y - Xw||^2_2\n",
      "     |          + alpha * l1_ratio * ||w||_1\n",
      "     |          + 0.5 * alpha * (1 - l1_ratio) * ||w||^2_2\n",
      "     |      \n",
      "     |      For multi-output tasks it is::\n",
      "     |      \n",
      "     |          (1 / (2 * n_samples)) * ||Y - XW||_Fro^2\n",
      "     |          + alpha * l1_ratio * ||W||_21\n",
      "     |          + 0.5 * alpha * (1 - l1_ratio) * ||W||_Fro^2\n",
      "     |      \n",
      "     |      Where::\n",
      "     |      \n",
      "     |          ||W||_21 = \\sum_i \\sqrt{\\sum_j w_{ij}^2}\n",
      "     |      \n",
      "     |      i.e. the sum of norm of each row.\n",
      "     |      \n",
      "     |      Read more in the :ref:`User Guide <elastic_net>`.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          Training data. Pass directly as Fortran-contiguous data to avoid\n",
      "     |          unnecessary memory duplication. If ``y`` is mono-output then ``X``\n",
      "     |          can be sparse.\n",
      "     |      \n",
      "     |      y : {array-like, sparse matrix} of shape (n_samples,) or         (n_samples, n_outputs)\n",
      "     |          Target values.\n",
      "     |      \n",
      "     |      l1_ratio : float, default=0.5\n",
      "     |          Number between 0 and 1 passed to elastic net (scaling between\n",
      "     |          l1 and l2 penalties). ``l1_ratio=1`` corresponds to the Lasso.\n",
      "     |      \n",
      "     |      eps : float, default=1e-3\n",
      "     |          Length of the path. ``eps=1e-3`` means that\n",
      "     |          ``alpha_min / alpha_max = 1e-3``.\n",
      "     |      \n",
      "     |      n_alphas : int, default=100\n",
      "     |          Number of alphas along the regularization path.\n",
      "     |      \n",
      "     |      alphas : ndarray, default=None\n",
      "     |          List of alphas where to compute the models.\n",
      "     |          If None alphas are set automatically.\n",
      "     |      \n",
      "     |      precompute : 'auto', bool or array-like of shape (n_features, n_features),                 default='auto'\n",
      "     |          Whether to use a precomputed Gram matrix to speed up\n",
      "     |          calculations. If set to ``'auto'`` let us decide. The Gram\n",
      "     |          matrix can also be passed as argument.\n",
      "     |      \n",
      "     |      Xy : array-like of shape (n_features,) or (n_features, n_outputs),         default=None\n",
      "     |          Xy = np.dot(X.T, y) that can be precomputed. It is useful\n",
      "     |          only when the Gram matrix is precomputed.\n",
      "     |      \n",
      "     |      copy_X : bool, default=True\n",
      "     |          If ``True``, X will be copied; else, it may be overwritten.\n",
      "     |      \n",
      "     |      coef_init : ndarray of shape (n_features, ), default=None\n",
      "     |          The initial values of the coefficients.\n",
      "     |      \n",
      "     |      verbose : bool or int, default=False\n",
      "     |          Amount of verbosity.\n",
      "     |      \n",
      "     |      return_n_iter : bool, default=False\n",
      "     |          Whether to return the number of iterations or not.\n",
      "     |      \n",
      "     |      positive : bool, default=False\n",
      "     |          If set to True, forces coefficients to be positive.\n",
      "     |          (Only allowed when ``y.ndim == 1``).\n",
      "     |      \n",
      "     |      check_input : bool, default=True\n",
      "     |          If set to False, the input validation checks are skipped (including the\n",
      "     |          Gram matrix when provided). It is assumed that they are handled\n",
      "     |          by the caller.\n",
      "     |      \n",
      "     |      **params : kwargs\n",
      "     |          Keyword arguments passed to the coordinate descent solver.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      alphas : ndarray of shape (n_alphas,)\n",
      "     |          The alphas along the path where models are computed.\n",
      "     |      \n",
      "     |      coefs : ndarray of shape (n_features, n_alphas) or             (n_outputs, n_features, n_alphas)\n",
      "     |          Coefficients along the path.\n",
      "     |      \n",
      "     |      dual_gaps : ndarray of shape (n_alphas,)\n",
      "     |          The dual gaps at the end of the optimization for each alpha.\n",
      "     |      \n",
      "     |      n_iters : list of int\n",
      "     |          The number of iterations taken by the coordinate descent optimizer to\n",
      "     |          reach the specified tolerance for each alpha.\n",
      "     |          (Is returned when ``return_n_iter`` is set to True).\n",
      "     |      \n",
      "     |      See Also\n",
      "     |      --------\n",
      "     |      MultiTaskElasticNet\n",
      "     |      MultiTaskElasticNetCV\n",
      "     |      ElasticNet\n",
      "     |      ElasticNetCV\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      For an example, see\n",
      "     |      :ref:`examples/linear_model/plot_lasso_coordinate_descent_path.py\n",
      "     |      <sphx_glr_auto_examples_linear_model_plot_lasso_coordinate_descent_path.py>`.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties defined here:\n",
      "     |  \n",
      "     |  sparse_coef_\n",
      "     |      Sparse representation of the fitted `coef_`.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.base.MultiOutputMixin:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.RegressorMixin:\n",
      "     |  \n",
      "     |  score(self, X, y, sample_weight=None)\n",
      "     |      Return the coefficient of determination :math:`R^2` of the\n",
      "     |      prediction.\n",
      "     |      \n",
      "     |      The coefficient :math:`R^2` is defined as :math:`(1 - \\frac{u}{v})`,\n",
      "     |      where :math:`u` is the residual sum of squares ``((y_true - y_pred)\n",
      "     |      ** 2).sum()`` and :math:`v` is the total sum of squares ``((y_true -\n",
      "     |      y_true.mean()) ** 2).sum()``. The best possible score is 1.0 and it\n",
      "     |      can be negative (because the model can be arbitrarily worse). A\n",
      "     |      constant model that always predicts the expected value of `y`,\n",
      "     |      disregarding the input features, would get a :math:`R^2` score of\n",
      "     |      0.0.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          Test samples. For some estimators this may be a precomputed\n",
      "     |          kernel matrix or a list of generic objects instead with shape\n",
      "     |          ``(n_samples, n_samples_fitted)``, where ``n_samples_fitted``\n",
      "     |          is the number of samples used in the fitting for the estimator.\n",
      "     |      \n",
      "     |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      "     |          True values for `X`.\n",
      "     |      \n",
      "     |      sample_weight : array-like of shape (n_samples,), default=None\n",
      "     |          Sample weights.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      score : float\n",
      "     |          :math:`R^2` of ``self.predict(X)`` wrt. `y`.\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      The :math:`R^2` score used when calling ``score`` on a regressor uses\n",
      "     |      ``multioutput='uniform_average'`` from version 0.23 to keep consistent\n",
      "     |      with default value of :func:`~sklearn.metrics.r2_score`.\n",
      "     |      This influences the ``score`` method of all the multioutput\n",
      "     |      regressors (except for\n",
      "     |      :class:`~sklearn.multioutput.MultiOutputRegressor`).\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.linear_model._base.LinearModel:\n",
      "     |  \n",
      "     |  predict(self, X)\n",
      "     |      Predict using the linear model.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like or sparse matrix, shape (n_samples, n_features)\n",
      "     |          Samples.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      C : array, shape (n_samples,)\n",
      "     |          Returns predicted values.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |  \n",
      "     |  __repr__(self, N_CHAR_MAX=700)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  get_params(self, deep=True)\n",
      "     |      Get parameters for this estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      deep : bool, default=True\n",
      "     |          If True, will return the parameters for this estimator and\n",
      "     |          contained subobjects that are estimators.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      params : dict\n",
      "     |          Parameter names mapped to their values.\n",
      "     |  \n",
      "     |  set_params(self, **params)\n",
      "     |      Set the parameters of this estimator.\n",
      "     |      \n",
      "     |      The method works on simple estimators as well as on nested objects\n",
      "     |      (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n",
      "     |      parameters of the form ``<component>__<parameter>`` so that it's\n",
      "     |      possible to update each component of a nested object.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      **params : dict\n",
      "     |          Estimator parameters.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : estimator instance\n",
      "     |          Estimator instance.\n",
      "    \n",
      "    class ElasticNetCV(sklearn.base.RegressorMixin, LinearModelCV)\n",
      "     |  ElasticNetCV(*, l1_ratio=0.5, eps=0.001, n_alphas=100, alphas=None, fit_intercept=True, normalize=False, precompute='auto', max_iter=1000, tol=0.0001, cv=None, copy_X=True, verbose=0, n_jobs=None, positive=False, random_state=None, selection='cyclic')\n",
      "     |  \n",
      "     |  Elastic Net model with iterative fitting along a regularization path.\n",
      "     |  \n",
      "     |  See glossary entry for :term:`cross-validation estimator`.\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <elastic_net>`.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  l1_ratio : float or list of float, default=0.5\n",
      "     |      float between 0 and 1 passed to ElasticNet (scaling between\n",
      "     |      l1 and l2 penalties). For ``l1_ratio = 0``\n",
      "     |      the penalty is an L2 penalty. For ``l1_ratio = 1`` it is an L1 penalty.\n",
      "     |      For ``0 < l1_ratio < 1``, the penalty is a combination of L1 and L2\n",
      "     |      This parameter can be a list, in which case the different\n",
      "     |      values are tested by cross-validation and the one giving the best\n",
      "     |      prediction score is used. Note that a good choice of list of\n",
      "     |      values for l1_ratio is often to put more values close to 1\n",
      "     |      (i.e. Lasso) and less close to 0 (i.e. Ridge), as in ``[.1, .5, .7,\n",
      "     |      .9, .95, .99, 1]``.\n",
      "     |  \n",
      "     |  eps : float, default=1e-3\n",
      "     |      Length of the path. ``eps=1e-3`` means that\n",
      "     |      ``alpha_min / alpha_max = 1e-3``.\n",
      "     |  \n",
      "     |  n_alphas : int, default=100\n",
      "     |      Number of alphas along the regularization path, used for each l1_ratio.\n",
      "     |  \n",
      "     |  alphas : ndarray, default=None\n",
      "     |      List of alphas where to compute the models.\n",
      "     |      If None alphas are set automatically.\n",
      "     |  \n",
      "     |  fit_intercept : bool, default=True\n",
      "     |      Whether to calculate the intercept for this model. If set\n",
      "     |      to false, no intercept will be used in calculations\n",
      "     |      (i.e. data is expected to be centered).\n",
      "     |  \n",
      "     |  normalize : bool, default=False\n",
      "     |      This parameter is ignored when ``fit_intercept`` is set to False.\n",
      "     |      If True, the regressors X will be normalized before regression by\n",
      "     |      subtracting the mean and dividing by the l2-norm.\n",
      "     |      If you wish to standardize, please use\n",
      "     |      :class:`~sklearn.preprocessing.StandardScaler` before calling ``fit``\n",
      "     |      on an estimator with ``normalize=False``.\n",
      "     |  \n",
      "     |  precompute : 'auto', bool or array-like of shape (n_features, n_features),                 default='auto'\n",
      "     |      Whether to use a precomputed Gram matrix to speed up\n",
      "     |      calculations. If set to ``'auto'`` let us decide. The Gram\n",
      "     |      matrix can also be passed as argument.\n",
      "     |  \n",
      "     |  max_iter : int, default=1000\n",
      "     |      The maximum number of iterations.\n",
      "     |  \n",
      "     |  tol : float, default=1e-4\n",
      "     |      The tolerance for the optimization: if the updates are\n",
      "     |      smaller than ``tol``, the optimization code checks the\n",
      "     |      dual gap for optimality and continues until it is smaller\n",
      "     |      than ``tol``.\n",
      "     |  \n",
      "     |  cv : int, cross-validation generator or iterable, default=None\n",
      "     |      Determines the cross-validation splitting strategy.\n",
      "     |      Possible inputs for cv are:\n",
      "     |  \n",
      "     |      - None, to use the default 5-fold cross-validation,\n",
      "     |      - int, to specify the number of folds.\n",
      "     |      - :term:`CV splitter`,\n",
      "     |      - An iterable yielding (train, test) splits as arrays of indices.\n",
      "     |  \n",
      "     |      For int/None inputs, :class:`KFold` is used.\n",
      "     |  \n",
      "     |      Refer :ref:`User Guide <cross_validation>` for the various\n",
      "     |      cross-validation strategies that can be used here.\n",
      "     |  \n",
      "     |      .. versionchanged:: 0.22\n",
      "     |          ``cv`` default value if None changed from 3-fold to 5-fold.\n",
      "     |  \n",
      "     |  copy_X : bool, default=True\n",
      "     |      If ``True``, X will be copied; else, it may be overwritten.\n",
      "     |  \n",
      "     |  verbose : bool or int, default=0\n",
      "     |      Amount of verbosity.\n",
      "     |  \n",
      "     |  n_jobs : int, default=None\n",
      "     |      Number of CPUs to use during the cross validation.\n",
      "     |      ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n",
      "     |      ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n",
      "     |      for more details.\n",
      "     |  \n",
      "     |  positive : bool, default=False\n",
      "     |      When set to ``True``, forces the coefficients to be positive.\n",
      "     |  \n",
      "     |  random_state : int, RandomState instance, default=None\n",
      "     |      The seed of the pseudo random number generator that selects a random\n",
      "     |      feature to update. Used when ``selection`` == 'random'.\n",
      "     |      Pass an int for reproducible output across multiple function calls.\n",
      "     |      See :term:`Glossary <random_state>`.\n",
      "     |  \n",
      "     |  selection : {'cyclic', 'random'}, default='cyclic'\n",
      "     |      If set to 'random', a random coefficient is updated every iteration\n",
      "     |      rather than looping over features sequentially by default. This\n",
      "     |      (setting to 'random') often leads to significantly faster convergence\n",
      "     |      especially when tol is higher than 1e-4.\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  alpha_ : float\n",
      "     |      The amount of penalization chosen by cross validation.\n",
      "     |  \n",
      "     |  l1_ratio_ : float\n",
      "     |      The compromise between l1 and l2 penalization chosen by\n",
      "     |      cross validation.\n",
      "     |  \n",
      "     |  coef_ : ndarray of shape (n_features,) or (n_targets, n_features)\n",
      "     |      Parameter vector (w in the cost function formula).\n",
      "     |  \n",
      "     |  intercept_ : float or ndarray of shape (n_targets, n_features)\n",
      "     |      Independent term in the decision function.\n",
      "     |  \n",
      "     |  mse_path_ : ndarray of shape (n_l1_ratio, n_alpha, n_folds)\n",
      "     |      Mean square error for the test set on each fold, varying l1_ratio and\n",
      "     |      alpha.\n",
      "     |  \n",
      "     |  alphas_ : ndarray of shape (n_alphas,) or (n_l1_ratio, n_alphas)\n",
      "     |      The grid of alphas used for fitting, for each l1_ratio.\n",
      "     |  \n",
      "     |  dual_gap_ : float\n",
      "     |      The dual gaps at the end of the optimization for the optimal alpha.\n",
      "     |  \n",
      "     |  n_iter_ : int\n",
      "     |      Number of iterations run by the coordinate descent solver to reach\n",
      "     |      the specified tolerance for the optimal alpha.\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  >>> from sklearn.linear_model import ElasticNetCV\n",
      "     |  >>> from sklearn.datasets import make_regression\n",
      "     |  \n",
      "     |  >>> X, y = make_regression(n_features=2, random_state=0)\n",
      "     |  >>> regr = ElasticNetCV(cv=5, random_state=0)\n",
      "     |  >>> regr.fit(X, y)\n",
      "     |  ElasticNetCV(cv=5, random_state=0)\n",
      "     |  >>> print(regr.alpha_)\n",
      "     |  0.199...\n",
      "     |  >>> print(regr.intercept_)\n",
      "     |  0.398...\n",
      "     |  >>> print(regr.predict([[0, 0]]))\n",
      "     |  [0.398...]\n",
      "     |  \n",
      "     |  \n",
      "     |  Notes\n",
      "     |  -----\n",
      "     |  For an example, see\n",
      "     |  :ref:`examples/linear_model/plot_lasso_model_selection.py\n",
      "     |  <sphx_glr_auto_examples_linear_model_plot_lasso_model_selection.py>`.\n",
      "     |  \n",
      "     |  To avoid unnecessary memory duplication the X argument of the fit method\n",
      "     |  should be directly passed as a Fortran-contiguous numpy array.\n",
      "     |  \n",
      "     |  The parameter l1_ratio corresponds to alpha in the glmnet R package\n",
      "     |  while alpha corresponds to the lambda parameter in glmnet.\n",
      "     |  More specifically, the optimization objective is::\n",
      "     |  \n",
      "     |      1 / (2 * n_samples) * ||y - Xw||^2_2\n",
      "     |      + alpha * l1_ratio * ||w||_1\n",
      "     |      + 0.5 * alpha * (1 - l1_ratio) * ||w||^2_2\n",
      "     |  \n",
      "     |  If you are interested in controlling the L1 and L2 penalty\n",
      "     |  separately, keep in mind that this is equivalent to::\n",
      "     |  \n",
      "     |      a * L1 + b * L2\n",
      "     |  \n",
      "     |  for::\n",
      "     |  \n",
      "     |      alpha = a + b and l1_ratio = a / (a + b).\n",
      "     |  \n",
      "     |  See Also\n",
      "     |  --------\n",
      "     |  enet_path\n",
      "     |  ElasticNet\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      ElasticNetCV\n",
      "     |      sklearn.base.RegressorMixin\n",
      "     |      LinearModelCV\n",
      "     |      sklearn.base.MultiOutputMixin\n",
      "     |      sklearn.linear_model._base.LinearModel\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, *, l1_ratio=0.5, eps=0.001, n_alphas=100, alphas=None, fit_intercept=True, normalize=False, precompute='auto', max_iter=1000, tol=0.0001, cv=None, copy_X=True, verbose=0, n_jobs=None, positive=False, random_state=None, selection='cyclic')\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods defined here:\n",
      "     |  \n",
      "     |  path = enet_path(X, y, *, l1_ratio=0.5, eps=0.001, n_alphas=100, alphas=None, precompute='auto', Xy=None, copy_X=True, coef_init=None, verbose=False, return_n_iter=False, positive=False, check_input=True, **params)\n",
      "     |      Compute elastic net path with coordinate descent.\n",
      "     |      \n",
      "     |      The elastic net optimization function varies for mono and multi-outputs.\n",
      "     |      \n",
      "     |      For mono-output tasks it is::\n",
      "     |      \n",
      "     |          1 / (2 * n_samples) * ||y - Xw||^2_2\n",
      "     |          + alpha * l1_ratio * ||w||_1\n",
      "     |          + 0.5 * alpha * (1 - l1_ratio) * ||w||^2_2\n",
      "     |      \n",
      "     |      For multi-output tasks it is::\n",
      "     |      \n",
      "     |          (1 / (2 * n_samples)) * ||Y - XW||_Fro^2\n",
      "     |          + alpha * l1_ratio * ||W||_21\n",
      "     |          + 0.5 * alpha * (1 - l1_ratio) * ||W||_Fro^2\n",
      "     |      \n",
      "     |      Where::\n",
      "     |      \n",
      "     |          ||W||_21 = \\sum_i \\sqrt{\\sum_j w_{ij}^2}\n",
      "     |      \n",
      "     |      i.e. the sum of norm of each row.\n",
      "     |      \n",
      "     |      Read more in the :ref:`User Guide <elastic_net>`.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          Training data. Pass directly as Fortran-contiguous data to avoid\n",
      "     |          unnecessary memory duplication. If ``y`` is mono-output then ``X``\n",
      "     |          can be sparse.\n",
      "     |      \n",
      "     |      y : {array-like, sparse matrix} of shape (n_samples,) or         (n_samples, n_outputs)\n",
      "     |          Target values.\n",
      "     |      \n",
      "     |      l1_ratio : float, default=0.5\n",
      "     |          Number between 0 and 1 passed to elastic net (scaling between\n",
      "     |          l1 and l2 penalties). ``l1_ratio=1`` corresponds to the Lasso.\n",
      "     |      \n",
      "     |      eps : float, default=1e-3\n",
      "     |          Length of the path. ``eps=1e-3`` means that\n",
      "     |          ``alpha_min / alpha_max = 1e-3``.\n",
      "     |      \n",
      "     |      n_alphas : int, default=100\n",
      "     |          Number of alphas along the regularization path.\n",
      "     |      \n",
      "     |      alphas : ndarray, default=None\n",
      "     |          List of alphas where to compute the models.\n",
      "     |          If None alphas are set automatically.\n",
      "     |      \n",
      "     |      precompute : 'auto', bool or array-like of shape (n_features, n_features),                 default='auto'\n",
      "     |          Whether to use a precomputed Gram matrix to speed up\n",
      "     |          calculations. If set to ``'auto'`` let us decide. The Gram\n",
      "     |          matrix can also be passed as argument.\n",
      "     |      \n",
      "     |      Xy : array-like of shape (n_features,) or (n_features, n_outputs),         default=None\n",
      "     |          Xy = np.dot(X.T, y) that can be precomputed. It is useful\n",
      "     |          only when the Gram matrix is precomputed.\n",
      "     |      \n",
      "     |      copy_X : bool, default=True\n",
      "     |          If ``True``, X will be copied; else, it may be overwritten.\n",
      "     |      \n",
      "     |      coef_init : ndarray of shape (n_features, ), default=None\n",
      "     |          The initial values of the coefficients.\n",
      "     |      \n",
      "     |      verbose : bool or int, default=False\n",
      "     |          Amount of verbosity.\n",
      "     |      \n",
      "     |      return_n_iter : bool, default=False\n",
      "     |          Whether to return the number of iterations or not.\n",
      "     |      \n",
      "     |      positive : bool, default=False\n",
      "     |          If set to True, forces coefficients to be positive.\n",
      "     |          (Only allowed when ``y.ndim == 1``).\n",
      "     |      \n",
      "     |      check_input : bool, default=True\n",
      "     |          If set to False, the input validation checks are skipped (including the\n",
      "     |          Gram matrix when provided). It is assumed that they are handled\n",
      "     |          by the caller.\n",
      "     |      \n",
      "     |      **params : kwargs\n",
      "     |          Keyword arguments passed to the coordinate descent solver.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      alphas : ndarray of shape (n_alphas,)\n",
      "     |          The alphas along the path where models are computed.\n",
      "     |      \n",
      "     |      coefs : ndarray of shape (n_features, n_alphas) or             (n_outputs, n_features, n_alphas)\n",
      "     |          Coefficients along the path.\n",
      "     |      \n",
      "     |      dual_gaps : ndarray of shape (n_alphas,)\n",
      "     |          The dual gaps at the end of the optimization for each alpha.\n",
      "     |      \n",
      "     |      n_iters : list of int\n",
      "     |          The number of iterations taken by the coordinate descent optimizer to\n",
      "     |          reach the specified tolerance for each alpha.\n",
      "     |          (Is returned when ``return_n_iter`` is set to True).\n",
      "     |      \n",
      "     |      See Also\n",
      "     |      --------\n",
      "     |      MultiTaskElasticNet\n",
      "     |      MultiTaskElasticNetCV\n",
      "     |      ElasticNet\n",
      "     |      ElasticNetCV\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      For an example, see\n",
      "     |      :ref:`examples/linear_model/plot_lasso_coordinate_descent_path.py\n",
      "     |      <sphx_glr_auto_examples_linear_model_plot_lasso_coordinate_descent_path.py>`.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.RegressorMixin:\n",
      "     |  \n",
      "     |  score(self, X, y, sample_weight=None)\n",
      "     |      Return the coefficient of determination :math:`R^2` of the\n",
      "     |      prediction.\n",
      "     |      \n",
      "     |      The coefficient :math:`R^2` is defined as :math:`(1 - \\frac{u}{v})`,\n",
      "     |      where :math:`u` is the residual sum of squares ``((y_true - y_pred)\n",
      "     |      ** 2).sum()`` and :math:`v` is the total sum of squares ``((y_true -\n",
      "     |      y_true.mean()) ** 2).sum()``. The best possible score is 1.0 and it\n",
      "     |      can be negative (because the model can be arbitrarily worse). A\n",
      "     |      constant model that always predicts the expected value of `y`,\n",
      "     |      disregarding the input features, would get a :math:`R^2` score of\n",
      "     |      0.0.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          Test samples. For some estimators this may be a precomputed\n",
      "     |          kernel matrix or a list of generic objects instead with shape\n",
      "     |          ``(n_samples, n_samples_fitted)``, where ``n_samples_fitted``\n",
      "     |          is the number of samples used in the fitting for the estimator.\n",
      "     |      \n",
      "     |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      "     |          True values for `X`.\n",
      "     |      \n",
      "     |      sample_weight : array-like of shape (n_samples,), default=None\n",
      "     |          Sample weights.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      score : float\n",
      "     |          :math:`R^2` of ``self.predict(X)`` wrt. `y`.\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      The :math:`R^2` score used when calling ``score`` on a regressor uses\n",
      "     |      ``multioutput='uniform_average'`` from version 0.23 to keep consistent\n",
      "     |      with default value of :func:`~sklearn.metrics.r2_score`.\n",
      "     |      This influences the ``score`` method of all the multioutput\n",
      "     |      regressors (except for\n",
      "     |      :class:`~sklearn.multioutput.MultiOutputRegressor`).\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.base.RegressorMixin:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from LinearModelCV:\n",
      "     |  \n",
      "     |  fit(self, X, y)\n",
      "     |      Fit linear model with coordinate descent.\n",
      "     |      \n",
      "     |      Fit is on grid of alphas and best alpha estimated by cross-validation.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          Training data. Pass directly as Fortran-contiguous data\n",
      "     |          to avoid unnecessary memory duplication. If y is mono-output,\n",
      "     |          X can be sparse.\n",
      "     |      \n",
      "     |      y : array-like of shape (n_samples,) or (n_samples, n_targets)\n",
      "     |          Target values.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.linear_model._base.LinearModel:\n",
      "     |  \n",
      "     |  predict(self, X)\n",
      "     |      Predict using the linear model.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like or sparse matrix, shape (n_samples, n_features)\n",
      "     |          Samples.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      C : array, shape (n_samples,)\n",
      "     |          Returns predicted values.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |  \n",
      "     |  __repr__(self, N_CHAR_MAX=700)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  get_params(self, deep=True)\n",
      "     |      Get parameters for this estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      deep : bool, default=True\n",
      "     |          If True, will return the parameters for this estimator and\n",
      "     |          contained subobjects that are estimators.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      params : dict\n",
      "     |          Parameter names mapped to their values.\n",
      "     |  \n",
      "     |  set_params(self, **params)\n",
      "     |      Set the parameters of this estimator.\n",
      "     |      \n",
      "     |      The method works on simple estimators as well as on nested objects\n",
      "     |      (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n",
      "     |      parameters of the form ``<component>__<parameter>`` so that it's\n",
      "     |      possible to update each component of a nested object.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      **params : dict\n",
      "     |          Estimator parameters.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : estimator instance\n",
      "     |          Estimator instance.\n",
      "    \n",
      "    class GammaRegressor(GeneralizedLinearRegressor)\n",
      "     |  GammaRegressor(*, alpha=1.0, fit_intercept=True, max_iter=100, tol=0.0001, warm_start=False, verbose=0)\n",
      "     |  \n",
      "     |  Generalized Linear Model with a Gamma distribution.\n",
      "     |  \n",
      "     |  This regressor uses the 'log' link function.\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <Generalized_linear_regression>`.\n",
      "     |  \n",
      "     |  .. versionadded:: 0.23\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  alpha : float, default=1\n",
      "     |      Constant that multiplies the penalty term and thus determines the\n",
      "     |      regularization strength. ``alpha = 0`` is equivalent to unpenalized\n",
      "     |      GLMs. In this case, the design matrix `X` must have full column rank\n",
      "     |      (no collinearities).\n",
      "     |  \n",
      "     |  fit_intercept : bool, default=True\n",
      "     |      Specifies if a constant (a.k.a. bias or intercept) should be\n",
      "     |      added to the linear predictor (X @ coef + intercept).\n",
      "     |  \n",
      "     |  max_iter : int, default=100\n",
      "     |      The maximal number of iterations for the solver.\n",
      "     |  \n",
      "     |  tol : float, default=1e-4\n",
      "     |      Stopping criterion. For the lbfgs solver,\n",
      "     |      the iteration will stop when ``max{|g_j|, j = 1, ..., d} <= tol``\n",
      "     |      where ``g_j`` is the j-th component of the gradient (derivative) of\n",
      "     |      the objective function.\n",
      "     |  \n",
      "     |  warm_start : bool, default=False\n",
      "     |      If set to ``True``, reuse the solution of the previous call to ``fit``\n",
      "     |      as initialization for ``coef_`` and ``intercept_`` .\n",
      "     |  \n",
      "     |  verbose : int, default=0\n",
      "     |      For the lbfgs solver set verbose to any positive number for verbosity.\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  coef_ : array of shape (n_features,)\n",
      "     |      Estimated coefficients for the linear predictor (`X * coef_ +\n",
      "     |      intercept_`) in the GLM.\n",
      "     |  \n",
      "     |  intercept_ : float\n",
      "     |      Intercept (a.k.a. bias) added to linear predictor.\n",
      "     |  \n",
      "     |  n_iter_ : int\n",
      "     |      Actual number of iterations used in the solver.\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  >>> from sklearn import linear_model\n",
      "     |  >>> clf = linear_model.GammaRegressor()\n",
      "     |  >>> X = [[1, 2], [2, 3], [3, 4], [4, 3]]\n",
      "     |  >>> y = [19, 26, 33, 30]\n",
      "     |  >>> clf.fit(X, y)\n",
      "     |  GammaRegressor()\n",
      "     |  >>> clf.score(X, y)\n",
      "     |  0.773...\n",
      "     |  >>> clf.coef_\n",
      "     |  array([0.072..., 0.066...])\n",
      "     |  >>> clf.intercept_\n",
      "     |  2.896...\n",
      "     |  >>> clf.predict([[1, 0], [2, 8]])\n",
      "     |  array([19.483..., 35.795...])\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      GammaRegressor\n",
      "     |      GeneralizedLinearRegressor\n",
      "     |      sklearn.base.RegressorMixin\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, *, alpha=1.0, fit_intercept=True, max_iter=100, tol=0.0001, warm_start=False, verbose=0)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  family\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from GeneralizedLinearRegressor:\n",
      "     |  \n",
      "     |  fit(self, X, y, sample_weight=None)\n",
      "     |      Fit a Generalized Linear Model.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          Training data.\n",
      "     |      \n",
      "     |      y : array-like of shape (n_samples,)\n",
      "     |          Target values.\n",
      "     |      \n",
      "     |      sample_weight : array-like of shape (n_samples,), default=None\n",
      "     |          Sample weights.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : returns an instance of self.\n",
      "     |  \n",
      "     |  predict(self, X)\n",
      "     |      Predict using GLM with feature matrix X.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          Samples.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      y_pred : array of shape (n_samples,)\n",
      "     |          Returns predicted values.\n",
      "     |  \n",
      "     |  score(self, X, y, sample_weight=None)\n",
      "     |      Compute D^2, the percentage of deviance explained.\n",
      "     |      \n",
      "     |      D^2 is a generalization of the coefficient of determination R^2.\n",
      "     |      R^2 uses squared error and D^2 deviance. Note that those two are equal\n",
      "     |      for ``family='normal'``.\n",
      "     |      \n",
      "     |      D^2 is defined as\n",
      "     |      :math:`D^2 = 1-\\frac{D(y_{true},y_{pred})}{D_{null}}`,\n",
      "     |      :math:`D_{null}` is the null deviance, i.e. the deviance of a model\n",
      "     |      with intercept alone, which corresponds to :math:`y_{pred} = \\bar{y}`.\n",
      "     |      The mean :math:`\\bar{y}` is averaged by sample_weight.\n",
      "     |      Best possible score is 1.0 and it can be negative (because the model\n",
      "     |      can be arbitrarily worse).\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          Test samples.\n",
      "     |      \n",
      "     |      y : array-like of shape (n_samples,)\n",
      "     |          True values of target.\n",
      "     |      \n",
      "     |      sample_weight : array-like of shape (n_samples,), default=None\n",
      "     |          Sample weights.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      score : float\n",
      "     |          D^2 of self.predict(X) w.r.t. y.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.base.RegressorMixin:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |  \n",
      "     |  __repr__(self, N_CHAR_MAX=700)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  get_params(self, deep=True)\n",
      "     |      Get parameters for this estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      deep : bool, default=True\n",
      "     |          If True, will return the parameters for this estimator and\n",
      "     |          contained subobjects that are estimators.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      params : dict\n",
      "     |          Parameter names mapped to their values.\n",
      "     |  \n",
      "     |  set_params(self, **params)\n",
      "     |      Set the parameters of this estimator.\n",
      "     |      \n",
      "     |      The method works on simple estimators as well as on nested objects\n",
      "     |      (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n",
      "     |      parameters of the form ``<component>__<parameter>`` so that it's\n",
      "     |      possible to update each component of a nested object.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      **params : dict\n",
      "     |          Estimator parameters.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : estimator instance\n",
      "     |          Estimator instance.\n",
      "    \n",
      "    class Hinge(Classification)\n",
      "     |  Hinge loss for binary classification tasks with y in {-1,1}\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  \n",
      "     |  threshold : float > 0.0\n",
      "     |      Margin threshold. When threshold=1.0, one gets the loss used by SVM.\n",
      "     |      When threshold=0.0, one gets the loss used by the Perceptron.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      Hinge\n",
      "     |      Classification\n",
      "     |      LossFunction\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, /, *args, **kwargs)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  __reduce__(...)\n",
      "     |      Helper for pickle.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods defined here:\n",
      "     |  \n",
      "     |  __new__(*args, **kwargs) from builtins.type\n",
      "     |      Create and return a new object.  See help(type) for accurate signature.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __pyx_vtable__ = <capsule object NULL>\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from Classification:\n",
      "     |  \n",
      "     |  __setstate__ = __setstate_cython__(...)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from LossFunction:\n",
      "     |  \n",
      "     |  py_dloss(...)\n",
      "     |      Python version of `dloss` for testing.\n",
      "     |      \n",
      "     |      Pytest needs a python function and can't use cdef functions.\n",
      "    \n",
      "    class Huber(Regression)\n",
      "     |  Huber regression loss\n",
      "     |  \n",
      "     |  Variant of the SquaredLoss that is robust to outliers (quadratic near zero,\n",
      "     |  linear in for large errors).\n",
      "     |  \n",
      "     |  https://en.wikipedia.org/wiki/Huber_Loss_Function\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      Huber\n",
      "     |      Regression\n",
      "     |      LossFunction\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, /, *args, **kwargs)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  __reduce__(...)\n",
      "     |      Helper for pickle.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods defined here:\n",
      "     |  \n",
      "     |  __new__(*args, **kwargs) from builtins.type\n",
      "     |      Create and return a new object.  See help(type) for accurate signature.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __pyx_vtable__ = <capsule object NULL>\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from Regression:\n",
      "     |  \n",
      "     |  __setstate__ = __setstate_cython__(...)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from LossFunction:\n",
      "     |  \n",
      "     |  py_dloss(...)\n",
      "     |      Python version of `dloss` for testing.\n",
      "     |      \n",
      "     |      Pytest needs a python function and can't use cdef functions.\n",
      "    \n",
      "    class HuberRegressor(sklearn.linear_model._base.LinearModel, sklearn.base.RegressorMixin, sklearn.base.BaseEstimator)\n",
      "     |  HuberRegressor(*, epsilon=1.35, max_iter=100, alpha=0.0001, warm_start=False, fit_intercept=True, tol=1e-05)\n",
      "     |  \n",
      "     |  Linear regression model that is robust to outliers.\n",
      "     |  \n",
      "     |  The Huber Regressor optimizes the squared loss for the samples where\n",
      "     |  ``|(y - X'w) / sigma| < epsilon`` and the absolute loss for the samples\n",
      "     |  where ``|(y - X'w) / sigma| > epsilon``, where w and sigma are parameters\n",
      "     |  to be optimized. The parameter sigma makes sure that if y is scaled up\n",
      "     |  or down by a certain factor, one does not need to rescale epsilon to\n",
      "     |  achieve the same robustness. Note that this does not take into account\n",
      "     |  the fact that the different features of X may be of different scales.\n",
      "     |  \n",
      "     |  This makes sure that the loss function is not heavily influenced by the\n",
      "     |  outliers while not completely ignoring their effect.\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <huber_regression>`\n",
      "     |  \n",
      "     |  .. versionadded:: 0.18\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  epsilon : float, greater than 1.0, default=1.35\n",
      "     |      The parameter epsilon controls the number of samples that should be\n",
      "     |      classified as outliers. The smaller the epsilon, the more robust it is\n",
      "     |      to outliers.\n",
      "     |  \n",
      "     |  max_iter : int, default=100\n",
      "     |      Maximum number of iterations that\n",
      "     |      ``scipy.optimize.minimize(method=\"L-BFGS-B\")`` should run for.\n",
      "     |  \n",
      "     |  alpha : float, default=0.0001\n",
      "     |      Regularization parameter.\n",
      "     |  \n",
      "     |  warm_start : bool, default=False\n",
      "     |      This is useful if the stored attributes of a previously used model\n",
      "     |      has to be reused. If set to False, then the coefficients will\n",
      "     |      be rewritten for every call to fit.\n",
      "     |      See :term:`the Glossary <warm_start>`.\n",
      "     |  \n",
      "     |  fit_intercept : bool, default=True\n",
      "     |      Whether or not to fit the intercept. This can be set to False\n",
      "     |      if the data is already centered around the origin.\n",
      "     |  \n",
      "     |  tol : float, default=1e-05\n",
      "     |      The iteration will stop when\n",
      "     |      ``max{|proj g_i | i = 1, ..., n}`` <= ``tol``\n",
      "     |      where pg_i is the i-th component of the projected gradient.\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  coef_ : array, shape (n_features,)\n",
      "     |      Features got by optimizing the Huber loss.\n",
      "     |  \n",
      "     |  intercept_ : float\n",
      "     |      Bias.\n",
      "     |  \n",
      "     |  scale_ : float\n",
      "     |      The value by which ``|y - X'w - c|`` is scaled down.\n",
      "     |  \n",
      "     |  n_iter_ : int\n",
      "     |      Number of iterations that\n",
      "     |      ``scipy.optimize.minimize(method=\"L-BFGS-B\")`` has run for.\n",
      "     |  \n",
      "     |      .. versionchanged:: 0.20\n",
      "     |  \n",
      "     |          In SciPy <= 1.0.0 the number of lbfgs iterations may exceed\n",
      "     |          ``max_iter``. ``n_iter_`` will now report at most ``max_iter``.\n",
      "     |  \n",
      "     |  outliers_ : array, shape (n_samples,)\n",
      "     |      A boolean mask which is set to True where the samples are identified\n",
      "     |      as outliers.\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  >>> import numpy as np\n",
      "     |  >>> from sklearn.linear_model import HuberRegressor, LinearRegression\n",
      "     |  >>> from sklearn.datasets import make_regression\n",
      "     |  >>> rng = np.random.RandomState(0)\n",
      "     |  >>> X, y, coef = make_regression(\n",
      "     |  ...     n_samples=200, n_features=2, noise=4.0, coef=True, random_state=0)\n",
      "     |  >>> X[:4] = rng.uniform(10, 20, (4, 2))\n",
      "     |  >>> y[:4] = rng.uniform(10, 20, 4)\n",
      "     |  >>> huber = HuberRegressor().fit(X, y)\n",
      "     |  >>> huber.score(X, y)\n",
      "     |  -7.284...\n",
      "     |  >>> huber.predict(X[:1,])\n",
      "     |  array([806.7200...])\n",
      "     |  >>> linear = LinearRegression().fit(X, y)\n",
      "     |  >>> print(\"True coefficients:\", coef)\n",
      "     |  True coefficients: [20.4923...  34.1698...]\n",
      "     |  >>> print(\"Huber coefficients:\", huber.coef_)\n",
      "     |  Huber coefficients: [17.7906... 31.0106...]\n",
      "     |  >>> print(\"Linear Regression coefficients:\", linear.coef_)\n",
      "     |  Linear Regression coefficients: [-1.9221...  7.0226...]\n",
      "     |  \n",
      "     |  References\n",
      "     |  ----------\n",
      "     |  .. [1] Peter J. Huber, Elvezio M. Ronchetti, Robust Statistics\n",
      "     |         Concomitant scale estimates, pg 172\n",
      "     |  .. [2] Art B. Owen (2006), A robust hybrid of lasso and ridge regression.\n",
      "     |         https://statweb.stanford.edu/~owen/reports/hhu.pdf\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      HuberRegressor\n",
      "     |      sklearn.linear_model._base.LinearModel\n",
      "     |      sklearn.base.RegressorMixin\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, *, epsilon=1.35, max_iter=100, alpha=0.0001, warm_start=False, fit_intercept=True, tol=1e-05)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  fit(self, X, y, sample_weight=None)\n",
      "     |      Fit the model according to the given training data.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like, shape (n_samples, n_features)\n",
      "     |          Training vector, where n_samples in the number of samples and\n",
      "     |          n_features is the number of features.\n",
      "     |      \n",
      "     |      y : array-like, shape (n_samples,)\n",
      "     |          Target vector relative to X.\n",
      "     |      \n",
      "     |      sample_weight : array-like, shape (n_samples,)\n",
      "     |          Weight given to each sample.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.linear_model._base.LinearModel:\n",
      "     |  \n",
      "     |  predict(self, X)\n",
      "     |      Predict using the linear model.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like or sparse matrix, shape (n_samples, n_features)\n",
      "     |          Samples.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      C : array, shape (n_samples,)\n",
      "     |          Returns predicted values.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.RegressorMixin:\n",
      "     |  \n",
      "     |  score(self, X, y, sample_weight=None)\n",
      "     |      Return the coefficient of determination :math:`R^2` of the\n",
      "     |      prediction.\n",
      "     |      \n",
      "     |      The coefficient :math:`R^2` is defined as :math:`(1 - \\frac{u}{v})`,\n",
      "     |      where :math:`u` is the residual sum of squares ``((y_true - y_pred)\n",
      "     |      ** 2).sum()`` and :math:`v` is the total sum of squares ``((y_true -\n",
      "     |      y_true.mean()) ** 2).sum()``. The best possible score is 1.0 and it\n",
      "     |      can be negative (because the model can be arbitrarily worse). A\n",
      "     |      constant model that always predicts the expected value of `y`,\n",
      "     |      disregarding the input features, would get a :math:`R^2` score of\n",
      "     |      0.0.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          Test samples. For some estimators this may be a precomputed\n",
      "     |          kernel matrix or a list of generic objects instead with shape\n",
      "     |          ``(n_samples, n_samples_fitted)``, where ``n_samples_fitted``\n",
      "     |          is the number of samples used in the fitting for the estimator.\n",
      "     |      \n",
      "     |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      "     |          True values for `X`.\n",
      "     |      \n",
      "     |      sample_weight : array-like of shape (n_samples,), default=None\n",
      "     |          Sample weights.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      score : float\n",
      "     |          :math:`R^2` of ``self.predict(X)`` wrt. `y`.\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      The :math:`R^2` score used when calling ``score`` on a regressor uses\n",
      "     |      ``multioutput='uniform_average'`` from version 0.23 to keep consistent\n",
      "     |      with default value of :func:`~sklearn.metrics.r2_score`.\n",
      "     |      This influences the ``score`` method of all the multioutput\n",
      "     |      regressors (except for\n",
      "     |      :class:`~sklearn.multioutput.MultiOutputRegressor`).\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.base.RegressorMixin:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |  \n",
      "     |  __repr__(self, N_CHAR_MAX=700)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  get_params(self, deep=True)\n",
      "     |      Get parameters for this estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      deep : bool, default=True\n",
      "     |          If True, will return the parameters for this estimator and\n",
      "     |          contained subobjects that are estimators.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      params : dict\n",
      "     |          Parameter names mapped to their values.\n",
      "     |  \n",
      "     |  set_params(self, **params)\n",
      "     |      Set the parameters of this estimator.\n",
      "     |      \n",
      "     |      The method works on simple estimators as well as on nested objects\n",
      "     |      (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n",
      "     |      parameters of the form ``<component>__<parameter>`` so that it's\n",
      "     |      possible to update each component of a nested object.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      **params : dict\n",
      "     |          Estimator parameters.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : estimator instance\n",
      "     |          Estimator instance.\n",
      "    \n",
      "    class Lars(sklearn.base.MultiOutputMixin, sklearn.base.RegressorMixin, sklearn.linear_model._base.LinearModel)\n",
      "     |  Lars(*, fit_intercept=True, verbose=False, normalize=True, precompute='auto', n_nonzero_coefs=500, eps=2.220446049250313e-16, copy_X=True, fit_path=True, jitter=None, random_state=None)\n",
      "     |  \n",
      "     |  Least Angle Regression model a.k.a. LAR\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <least_angle_regression>`.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  fit_intercept : bool, default=True\n",
      "     |      Whether to calculate the intercept for this model. If set\n",
      "     |      to false, no intercept will be used in calculations\n",
      "     |      (i.e. data is expected to be centered).\n",
      "     |  \n",
      "     |  verbose : bool or int, default=False\n",
      "     |      Sets the verbosity amount.\n",
      "     |  \n",
      "     |  normalize : bool, default=True\n",
      "     |      This parameter is ignored when ``fit_intercept`` is set to False.\n",
      "     |      If True, the regressors X will be normalized before regression by\n",
      "     |      subtracting the mean and dividing by the l2-norm.\n",
      "     |      If you wish to standardize, please use\n",
      "     |      :class:`~sklearn.preprocessing.StandardScaler` before calling ``fit``\n",
      "     |      on an estimator with ``normalize=False``.\n",
      "     |  \n",
      "     |  precompute : bool, 'auto' or array-like , default='auto'\n",
      "     |      Whether to use a precomputed Gram matrix to speed up\n",
      "     |      calculations. If set to ``'auto'`` let us decide. The Gram\n",
      "     |      matrix can also be passed as argument.\n",
      "     |  \n",
      "     |  n_nonzero_coefs : int, default=500\n",
      "     |      Target number of non-zero coefficients. Use ``np.inf`` for no limit.\n",
      "     |  \n",
      "     |  eps : float, default=np.finfo(float).eps\n",
      "     |      The machine-precision regularization in the computation of the\n",
      "     |      Cholesky diagonal factors. Increase this for very ill-conditioned\n",
      "     |      systems. Unlike the ``tol`` parameter in some iterative\n",
      "     |      optimization-based algorithms, this parameter does not control\n",
      "     |      the tolerance of the optimization.\n",
      "     |  \n",
      "     |  copy_X : bool, default=True\n",
      "     |      If ``True``, X will be copied; else, it may be overwritten.\n",
      "     |  \n",
      "     |  fit_path : bool, default=True\n",
      "     |      If True the full path is stored in the ``coef_path_`` attribute.\n",
      "     |      If you compute the solution for a large problem or many targets,\n",
      "     |      setting ``fit_path`` to ``False`` will lead to a speedup, especially\n",
      "     |      with a small alpha.\n",
      "     |  \n",
      "     |  jitter : float, default=None\n",
      "     |      Upper bound on a uniform noise parameter to be added to the\n",
      "     |      `y` values, to satisfy the model's assumption of\n",
      "     |      one-at-a-time computations. Might help with stability.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.23\n",
      "     |  \n",
      "     |  random_state : int, RandomState instance or None, default=None\n",
      "     |      Determines random number generation for jittering. Pass an int\n",
      "     |      for reproducible output across multiple function calls.\n",
      "     |      See :term:`Glossary <random_state>`. Ignored if `jitter` is None.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.23\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  alphas_ : array-like of shape (n_alphas + 1,) or list of such arrays\n",
      "     |      Maximum of covariances (in absolute value) at each iteration.\n",
      "     |      ``n_alphas`` is either ``max_iter``, ``n_features`` or the\n",
      "     |      number of nodes in the path with ``alpha >= alpha_min``, whichever\n",
      "     |      is smaller. If this is a list of array-like, the length of the outer\n",
      "     |      list is `n_targets`.\n",
      "     |  \n",
      "     |  active_ : list of shape (n_alphas,) or list of such lists\n",
      "     |      Indices of active variables at the end of the path.\n",
      "     |      If this is a list of list, the length of the outer list is `n_targets`.\n",
      "     |  \n",
      "     |  coef_path_ : array-like of shape (n_features, n_alphas + 1) or list             of such arrays\n",
      "     |      The varying values of the coefficients along the path. It is not\n",
      "     |      present if the ``fit_path`` parameter is ``False``. If this is a list\n",
      "     |      of array-like, the length of the outer list is `n_targets`.\n",
      "     |  \n",
      "     |  coef_ : array-like of shape (n_features,) or (n_targets, n_features)\n",
      "     |      Parameter vector (w in the formulation formula).\n",
      "     |  \n",
      "     |  intercept_ : float or array-like of shape (n_targets,)\n",
      "     |      Independent term in decision function.\n",
      "     |  \n",
      "     |  n_iter_ : array-like or int\n",
      "     |      The number of iterations taken by lars_path to find the\n",
      "     |      grid of alphas for each target.\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  >>> from sklearn import linear_model\n",
      "     |  >>> reg = linear_model.Lars(n_nonzero_coefs=1)\n",
      "     |  >>> reg.fit([[-1, 1], [0, 0], [1, 1]], [-1.1111, 0, -1.1111])\n",
      "     |  Lars(n_nonzero_coefs=1)\n",
      "     |  >>> print(reg.coef_)\n",
      "     |  [ 0. -1.11...]\n",
      "     |  \n",
      "     |  See Also\n",
      "     |  --------\n",
      "     |  lars_path, LarsCV\n",
      "     |  sklearn.decomposition.sparse_encode\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      Lars\n",
      "     |      sklearn.base.MultiOutputMixin\n",
      "     |      sklearn.base.RegressorMixin\n",
      "     |      sklearn.linear_model._base.LinearModel\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, *, fit_intercept=True, verbose=False, normalize=True, precompute='auto', n_nonzero_coefs=500, eps=2.220446049250313e-16, copy_X=True, fit_path=True, jitter=None, random_state=None)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  fit(self, X, y, Xy=None)\n",
      "     |      Fit the model using X, y as training data.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          Training data.\n",
      "     |      \n",
      "     |      y : array-like of shape (n_samples,) or (n_samples, n_targets)\n",
      "     |          Target values.\n",
      "     |      \n",
      "     |      Xy : array-like of shape (n_samples,) or (n_samples, n_targets),                 default=None\n",
      "     |          Xy = np.dot(X.T, y) that can be precomputed. It is useful\n",
      "     |          only when the Gram matrix is precomputed.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |          returns an instance of self.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |  \n",
      "     |  method = 'lar'\n",
      "     |  \n",
      "     |  positive = False\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.base.MultiOutputMixin:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.RegressorMixin:\n",
      "     |  \n",
      "     |  score(self, X, y, sample_weight=None)\n",
      "     |      Return the coefficient of determination :math:`R^2` of the\n",
      "     |      prediction.\n",
      "     |      \n",
      "     |      The coefficient :math:`R^2` is defined as :math:`(1 - \\frac{u}{v})`,\n",
      "     |      where :math:`u` is the residual sum of squares ``((y_true - y_pred)\n",
      "     |      ** 2).sum()`` and :math:`v` is the total sum of squares ``((y_true -\n",
      "     |      y_true.mean()) ** 2).sum()``. The best possible score is 1.0 and it\n",
      "     |      can be negative (because the model can be arbitrarily worse). A\n",
      "     |      constant model that always predicts the expected value of `y`,\n",
      "     |      disregarding the input features, would get a :math:`R^2` score of\n",
      "     |      0.0.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          Test samples. For some estimators this may be a precomputed\n",
      "     |          kernel matrix or a list of generic objects instead with shape\n",
      "     |          ``(n_samples, n_samples_fitted)``, where ``n_samples_fitted``\n",
      "     |          is the number of samples used in the fitting for the estimator.\n",
      "     |      \n",
      "     |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      "     |          True values for `X`.\n",
      "     |      \n",
      "     |      sample_weight : array-like of shape (n_samples,), default=None\n",
      "     |          Sample weights.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      score : float\n",
      "     |          :math:`R^2` of ``self.predict(X)`` wrt. `y`.\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      The :math:`R^2` score used when calling ``score`` on a regressor uses\n",
      "     |      ``multioutput='uniform_average'`` from version 0.23 to keep consistent\n",
      "     |      with default value of :func:`~sklearn.metrics.r2_score`.\n",
      "     |      This influences the ``score`` method of all the multioutput\n",
      "     |      regressors (except for\n",
      "     |      :class:`~sklearn.multioutput.MultiOutputRegressor`).\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.linear_model._base.LinearModel:\n",
      "     |  \n",
      "     |  predict(self, X)\n",
      "     |      Predict using the linear model.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like or sparse matrix, shape (n_samples, n_features)\n",
      "     |          Samples.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      C : array, shape (n_samples,)\n",
      "     |          Returns predicted values.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |  \n",
      "     |  __repr__(self, N_CHAR_MAX=700)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  get_params(self, deep=True)\n",
      "     |      Get parameters for this estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      deep : bool, default=True\n",
      "     |          If True, will return the parameters for this estimator and\n",
      "     |          contained subobjects that are estimators.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      params : dict\n",
      "     |          Parameter names mapped to their values.\n",
      "     |  \n",
      "     |  set_params(self, **params)\n",
      "     |      Set the parameters of this estimator.\n",
      "     |      \n",
      "     |      The method works on simple estimators as well as on nested objects\n",
      "     |      (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n",
      "     |      parameters of the form ``<component>__<parameter>`` so that it's\n",
      "     |      possible to update each component of a nested object.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      **params : dict\n",
      "     |          Estimator parameters.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : estimator instance\n",
      "     |          Estimator instance.\n",
      "    \n",
      "    class LarsCV(Lars)\n",
      "     |  LarsCV(*, fit_intercept=True, verbose=False, max_iter=500, normalize=True, precompute='auto', cv=None, max_n_alphas=1000, n_jobs=None, eps=2.220446049250313e-16, copy_X=True)\n",
      "     |  \n",
      "     |  Cross-validated Least Angle Regression model.\n",
      "     |  \n",
      "     |  See glossary entry for :term:`cross-validation estimator`.\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <least_angle_regression>`.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  fit_intercept : bool, default=True\n",
      "     |      whether to calculate the intercept for this model. If set\n",
      "     |      to false, no intercept will be used in calculations\n",
      "     |      (i.e. data is expected to be centered).\n",
      "     |  \n",
      "     |  verbose : bool or int, default=False\n",
      "     |      Sets the verbosity amount.\n",
      "     |  \n",
      "     |  max_iter : int, default=500\n",
      "     |      Maximum number of iterations to perform.\n",
      "     |  \n",
      "     |  normalize : bool, default=True\n",
      "     |      This parameter is ignored when ``fit_intercept`` is set to False.\n",
      "     |      If True, the regressors X will be normalized before regression by\n",
      "     |      subtracting the mean and dividing by the l2-norm.\n",
      "     |      If you wish to standardize, please use\n",
      "     |      :class:`~sklearn.preprocessing.StandardScaler` before calling ``fit``\n",
      "     |      on an estimator with ``normalize=False``.\n",
      "     |  \n",
      "     |  precompute : bool, 'auto' or array-like , default='auto'\n",
      "     |      Whether to use a precomputed Gram matrix to speed up\n",
      "     |      calculations. If set to ``'auto'`` let us decide. The Gram matrix\n",
      "     |      cannot be passed as argument since we will use only subsets of X.\n",
      "     |  \n",
      "     |  cv : int, cross-validation generator or an iterable, default=None\n",
      "     |      Determines the cross-validation splitting strategy.\n",
      "     |      Possible inputs for cv are:\n",
      "     |  \n",
      "     |      - None, to use the default 5-fold cross-validation,\n",
      "     |      - integer, to specify the number of folds.\n",
      "     |      - :term:`CV splitter`,\n",
      "     |      - An iterable yielding (train, test) splits as arrays of indices.\n",
      "     |  \n",
      "     |      For integer/None inputs, :class:`KFold` is used.\n",
      "     |  \n",
      "     |      Refer :ref:`User Guide <cross_validation>` for the various\n",
      "     |      cross-validation strategies that can be used here.\n",
      "     |  \n",
      "     |      .. versionchanged:: 0.22\n",
      "     |          ``cv`` default value if None changed from 3-fold to 5-fold.\n",
      "     |  \n",
      "     |  max_n_alphas : int, default=1000\n",
      "     |      The maximum number of points on the path used to compute the\n",
      "     |      residuals in the cross-validation\n",
      "     |  \n",
      "     |  n_jobs : int or None, default=None\n",
      "     |      Number of CPUs to use during the cross validation.\n",
      "     |      ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n",
      "     |      ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n",
      "     |      for more details.\n",
      "     |  \n",
      "     |  eps : float, default=np.finfo(float).eps\n",
      "     |      The machine-precision regularization in the computation of the\n",
      "     |      Cholesky diagonal factors. Increase this for very ill-conditioned\n",
      "     |      systems. Unlike the ``tol`` parameter in some iterative\n",
      "     |      optimization-based algorithms, this parameter does not control\n",
      "     |      the tolerance of the optimization.\n",
      "     |  \n",
      "     |  copy_X : bool, default=True\n",
      "     |      If ``True``, X will be copied; else, it may be overwritten.\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  active_ : list of length n_alphas or list of such lists\n",
      "     |      Indices of active variables at the end of the path.\n",
      "     |      If this is a list of lists, the outer list length is `n_targets`.\n",
      "     |  \n",
      "     |  coef_ : array-like of shape (n_features,)\n",
      "     |      parameter vector (w in the formulation formula)\n",
      "     |  \n",
      "     |  intercept_ : float\n",
      "     |      independent term in decision function\n",
      "     |  \n",
      "     |  coef_path_ : array-like of shape (n_features, n_alphas)\n",
      "     |      the varying values of the coefficients along the path\n",
      "     |  \n",
      "     |  alpha_ : float\n",
      "     |      the estimated regularization parameter alpha\n",
      "     |  \n",
      "     |  alphas_ : array-like of shape (n_alphas,)\n",
      "     |      the different values of alpha along the path\n",
      "     |  \n",
      "     |  cv_alphas_ : array-like of shape (n_cv_alphas,)\n",
      "     |      all the values of alpha along the path for the different folds\n",
      "     |  \n",
      "     |  mse_path_ : array-like of shape (n_folds, n_cv_alphas)\n",
      "     |      the mean square error on left-out for each fold along the path\n",
      "     |      (alpha values given by ``cv_alphas``)\n",
      "     |  \n",
      "     |  n_iter_ : array-like or int\n",
      "     |      the number of iterations run by Lars with the optimal alpha.\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  >>> from sklearn.linear_model import LarsCV\n",
      "     |  >>> from sklearn.datasets import make_regression\n",
      "     |  >>> X, y = make_regression(n_samples=200, noise=4.0, random_state=0)\n",
      "     |  >>> reg = LarsCV(cv=5).fit(X, y)\n",
      "     |  >>> reg.score(X, y)\n",
      "     |  0.9996...\n",
      "     |  >>> reg.alpha_\n",
      "     |  0.0254...\n",
      "     |  >>> reg.predict(X[:1,])\n",
      "     |  array([154.0842...])\n",
      "     |  \n",
      "     |  See Also\n",
      "     |  --------\n",
      "     |  lars_path, LassoLars, LassoLarsCV\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      LarsCV\n",
      "     |      Lars\n",
      "     |      sklearn.base.MultiOutputMixin\n",
      "     |      sklearn.base.RegressorMixin\n",
      "     |      sklearn.linear_model._base.LinearModel\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, *, fit_intercept=True, verbose=False, max_iter=500, normalize=True, precompute='auto', cv=None, max_n_alphas=1000, n_jobs=None, eps=2.220446049250313e-16, copy_X=True)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  fit(self, X, y)\n",
      "     |      Fit the model using X, y as training data.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          Training data.\n",
      "     |      \n",
      "     |      y : array-like of shape (n_samples,)\n",
      "     |          Target values.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |          returns an instance of self.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |  \n",
      "     |  method = 'lar'\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from Lars:\n",
      "     |  \n",
      "     |  positive = False\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.base.MultiOutputMixin:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.RegressorMixin:\n",
      "     |  \n",
      "     |  score(self, X, y, sample_weight=None)\n",
      "     |      Return the coefficient of determination :math:`R^2` of the\n",
      "     |      prediction.\n",
      "     |      \n",
      "     |      The coefficient :math:`R^2` is defined as :math:`(1 - \\frac{u}{v})`,\n",
      "     |      where :math:`u` is the residual sum of squares ``((y_true - y_pred)\n",
      "     |      ** 2).sum()`` and :math:`v` is the total sum of squares ``((y_true -\n",
      "     |      y_true.mean()) ** 2).sum()``. The best possible score is 1.0 and it\n",
      "     |      can be negative (because the model can be arbitrarily worse). A\n",
      "     |      constant model that always predicts the expected value of `y`,\n",
      "     |      disregarding the input features, would get a :math:`R^2` score of\n",
      "     |      0.0.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          Test samples. For some estimators this may be a precomputed\n",
      "     |          kernel matrix or a list of generic objects instead with shape\n",
      "     |          ``(n_samples, n_samples_fitted)``, where ``n_samples_fitted``\n",
      "     |          is the number of samples used in the fitting for the estimator.\n",
      "     |      \n",
      "     |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      "     |          True values for `X`.\n",
      "     |      \n",
      "     |      sample_weight : array-like of shape (n_samples,), default=None\n",
      "     |          Sample weights.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      score : float\n",
      "     |          :math:`R^2` of ``self.predict(X)`` wrt. `y`.\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      The :math:`R^2` score used when calling ``score`` on a regressor uses\n",
      "     |      ``multioutput='uniform_average'`` from version 0.23 to keep consistent\n",
      "     |      with default value of :func:`~sklearn.metrics.r2_score`.\n",
      "     |      This influences the ``score`` method of all the multioutput\n",
      "     |      regressors (except for\n",
      "     |      :class:`~sklearn.multioutput.MultiOutputRegressor`).\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.linear_model._base.LinearModel:\n",
      "     |  \n",
      "     |  predict(self, X)\n",
      "     |      Predict using the linear model.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like or sparse matrix, shape (n_samples, n_features)\n",
      "     |          Samples.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      C : array, shape (n_samples,)\n",
      "     |          Returns predicted values.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |  \n",
      "     |  __repr__(self, N_CHAR_MAX=700)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  get_params(self, deep=True)\n",
      "     |      Get parameters for this estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      deep : bool, default=True\n",
      "     |          If True, will return the parameters for this estimator and\n",
      "     |          contained subobjects that are estimators.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      params : dict\n",
      "     |          Parameter names mapped to their values.\n",
      "     |  \n",
      "     |  set_params(self, **params)\n",
      "     |      Set the parameters of this estimator.\n",
      "     |      \n",
      "     |      The method works on simple estimators as well as on nested objects\n",
      "     |      (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n",
      "     |      parameters of the form ``<component>__<parameter>`` so that it's\n",
      "     |      possible to update each component of a nested object.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      **params : dict\n",
      "     |          Estimator parameters.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : estimator instance\n",
      "     |          Estimator instance.\n",
      "    \n",
      "    class Lasso(ElasticNet)\n",
      "     |  Lasso(alpha=1.0, *, fit_intercept=True, normalize=False, precompute=False, copy_X=True, max_iter=1000, tol=0.0001, warm_start=False, positive=False, random_state=None, selection='cyclic')\n",
      "     |  \n",
      "     |  Linear Model trained with L1 prior as regularizer (aka the Lasso)\n",
      "     |  \n",
      "     |  The optimization objective for Lasso is::\n",
      "     |  \n",
      "     |      (1 / (2 * n_samples)) * ||y - Xw||^2_2 + alpha * ||w||_1\n",
      "     |  \n",
      "     |  Technically the Lasso model is optimizing the same objective function as\n",
      "     |  the Elastic Net with ``l1_ratio=1.0`` (no L2 penalty).\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <lasso>`.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  alpha : float, default=1.0\n",
      "     |      Constant that multiplies the L1 term. Defaults to 1.0.\n",
      "     |      ``alpha = 0`` is equivalent to an ordinary least square, solved\n",
      "     |      by the :class:`LinearRegression` object. For numerical\n",
      "     |      reasons, using ``alpha = 0`` with the ``Lasso`` object is not advised.\n",
      "     |      Given this, you should use the :class:`LinearRegression` object.\n",
      "     |  \n",
      "     |  fit_intercept : bool, default=True\n",
      "     |      Whether to calculate the intercept for this model. If set\n",
      "     |      to False, no intercept will be used in calculations\n",
      "     |      (i.e. data is expected to be centered).\n",
      "     |  \n",
      "     |  normalize : bool, default=False\n",
      "     |      This parameter is ignored when ``fit_intercept`` is set to False.\n",
      "     |      If True, the regressors X will be normalized before regression by\n",
      "     |      subtracting the mean and dividing by the l2-norm.\n",
      "     |      If you wish to standardize, please use\n",
      "     |      :class:`~sklearn.preprocessing.StandardScaler` before calling ``fit``\n",
      "     |      on an estimator with ``normalize=False``.\n",
      "     |  \n",
      "     |  precompute : bool or array-like of shape (n_features, n_features),                 default=False\n",
      "     |      Whether to use a precomputed Gram matrix to speed up\n",
      "     |      calculations. The Gram matrix can also be passed as argument.\n",
      "     |      For sparse input this option is always ``False`` to preserve sparsity.\n",
      "     |  \n",
      "     |  copy_X : bool, default=True\n",
      "     |      If ``True``, X will be copied; else, it may be overwritten.\n",
      "     |  \n",
      "     |  max_iter : int, default=1000\n",
      "     |      The maximum number of iterations.\n",
      "     |  \n",
      "     |  tol : float, default=1e-4\n",
      "     |      The tolerance for the optimization: if the updates are\n",
      "     |      smaller than ``tol``, the optimization code checks the\n",
      "     |      dual gap for optimality and continues until it is smaller\n",
      "     |      than ``tol``.\n",
      "     |  \n",
      "     |  warm_start : bool, default=False\n",
      "     |      When set to True, reuse the solution of the previous call to fit as\n",
      "     |      initialization, otherwise, just erase the previous solution.\n",
      "     |      See :term:`the Glossary <warm_start>`.\n",
      "     |  \n",
      "     |  positive : bool, default=False\n",
      "     |      When set to ``True``, forces the coefficients to be positive.\n",
      "     |  \n",
      "     |  random_state : int, RandomState instance, default=None\n",
      "     |      The seed of the pseudo random number generator that selects a random\n",
      "     |      feature to update. Used when ``selection`` == 'random'.\n",
      "     |      Pass an int for reproducible output across multiple function calls.\n",
      "     |      See :term:`Glossary <random_state>`.\n",
      "     |  \n",
      "     |  selection : {'cyclic', 'random'}, default='cyclic'\n",
      "     |      If set to 'random', a random coefficient is updated every iteration\n",
      "     |      rather than looping over features sequentially by default. This\n",
      "     |      (setting to 'random') often leads to significantly faster convergence\n",
      "     |      especially when tol is higher than 1e-4.\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  coef_ : ndarray of shape (n_features,) or (n_targets, n_features)\n",
      "     |      Parameter vector (w in the cost function formula).\n",
      "     |  \n",
      "     |  dual_gap_ : float or ndarray of shape (n_targets,)\n",
      "     |      Given param alpha, the dual gaps at the end of the optimization,\n",
      "     |      same shape as each observation of y.\n",
      "     |  \n",
      "     |  sparse_coef_ : sparse matrix of shape (n_features, 1) or             (n_targets, n_features)\n",
      "     |      Readonly property derived from ``coef_``.\n",
      "     |  \n",
      "     |  intercept_ : float or ndarray of shape (n_targets,)\n",
      "     |      Independent term in decision function.\n",
      "     |  \n",
      "     |  n_iter_ : int or list of int\n",
      "     |      Number of iterations run by the coordinate descent solver to reach\n",
      "     |      the specified tolerance.\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  >>> from sklearn import linear_model\n",
      "     |  >>> clf = linear_model.Lasso(alpha=0.1)\n",
      "     |  >>> clf.fit([[0,0], [1, 1], [2, 2]], [0, 1, 2])\n",
      "     |  Lasso(alpha=0.1)\n",
      "     |  >>> print(clf.coef_)\n",
      "     |  [0.85 0.  ]\n",
      "     |  >>> print(clf.intercept_)\n",
      "     |  0.15...\n",
      "     |  \n",
      "     |  See Also\n",
      "     |  --------\n",
      "     |  lars_path\n",
      "     |  lasso_path\n",
      "     |  LassoLars\n",
      "     |  LassoCV\n",
      "     |  LassoLarsCV\n",
      "     |  sklearn.decomposition.sparse_encode\n",
      "     |  \n",
      "     |  Notes\n",
      "     |  -----\n",
      "     |  The algorithm used to fit the model is coordinate descent.\n",
      "     |  \n",
      "     |  To avoid unnecessary memory duplication the X argument of the fit method\n",
      "     |  should be directly passed as a Fortran-contiguous numpy array.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      Lasso\n",
      "     |      ElasticNet\n",
      "     |      sklearn.base.MultiOutputMixin\n",
      "     |      sklearn.base.RegressorMixin\n",
      "     |      sklearn.linear_model._base.LinearModel\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, alpha=1.0, *, fit_intercept=True, normalize=False, precompute=False, copy_X=True, max_iter=1000, tol=0.0001, warm_start=False, positive=False, random_state=None, selection='cyclic')\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods defined here:\n",
      "     |  \n",
      "     |  path = enet_path(X, y, *, l1_ratio=0.5, eps=0.001, n_alphas=100, alphas=None, precompute='auto', Xy=None, copy_X=True, coef_init=None, verbose=False, return_n_iter=False, positive=False, check_input=True, **params)\n",
      "     |      Compute elastic net path with coordinate descent.\n",
      "     |      \n",
      "     |      The elastic net optimization function varies for mono and multi-outputs.\n",
      "     |      \n",
      "     |      For mono-output tasks it is::\n",
      "     |      \n",
      "     |          1 / (2 * n_samples) * ||y - Xw||^2_2\n",
      "     |          + alpha * l1_ratio * ||w||_1\n",
      "     |          + 0.5 * alpha * (1 - l1_ratio) * ||w||^2_2\n",
      "     |      \n",
      "     |      For multi-output tasks it is::\n",
      "     |      \n",
      "     |          (1 / (2 * n_samples)) * ||Y - XW||_Fro^2\n",
      "     |          + alpha * l1_ratio * ||W||_21\n",
      "     |          + 0.5 * alpha * (1 - l1_ratio) * ||W||_Fro^2\n",
      "     |      \n",
      "     |      Where::\n",
      "     |      \n",
      "     |          ||W||_21 = \\sum_i \\sqrt{\\sum_j w_{ij}^2}\n",
      "     |      \n",
      "     |      i.e. the sum of norm of each row.\n",
      "     |      \n",
      "     |      Read more in the :ref:`User Guide <elastic_net>`.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          Training data. Pass directly as Fortran-contiguous data to avoid\n",
      "     |          unnecessary memory duplication. If ``y`` is mono-output then ``X``\n",
      "     |          can be sparse.\n",
      "     |      \n",
      "     |      y : {array-like, sparse matrix} of shape (n_samples,) or         (n_samples, n_outputs)\n",
      "     |          Target values.\n",
      "     |      \n",
      "     |      l1_ratio : float, default=0.5\n",
      "     |          Number between 0 and 1 passed to elastic net (scaling between\n",
      "     |          l1 and l2 penalties). ``l1_ratio=1`` corresponds to the Lasso.\n",
      "     |      \n",
      "     |      eps : float, default=1e-3\n",
      "     |          Length of the path. ``eps=1e-3`` means that\n",
      "     |          ``alpha_min / alpha_max = 1e-3``.\n",
      "     |      \n",
      "     |      n_alphas : int, default=100\n",
      "     |          Number of alphas along the regularization path.\n",
      "     |      \n",
      "     |      alphas : ndarray, default=None\n",
      "     |          List of alphas where to compute the models.\n",
      "     |          If None alphas are set automatically.\n",
      "     |      \n",
      "     |      precompute : 'auto', bool or array-like of shape (n_features, n_features),                 default='auto'\n",
      "     |          Whether to use a precomputed Gram matrix to speed up\n",
      "     |          calculations. If set to ``'auto'`` let us decide. The Gram\n",
      "     |          matrix can also be passed as argument.\n",
      "     |      \n",
      "     |      Xy : array-like of shape (n_features,) or (n_features, n_outputs),         default=None\n",
      "     |          Xy = np.dot(X.T, y) that can be precomputed. It is useful\n",
      "     |          only when the Gram matrix is precomputed.\n",
      "     |      \n",
      "     |      copy_X : bool, default=True\n",
      "     |          If ``True``, X will be copied; else, it may be overwritten.\n",
      "     |      \n",
      "     |      coef_init : ndarray of shape (n_features, ), default=None\n",
      "     |          The initial values of the coefficients.\n",
      "     |      \n",
      "     |      verbose : bool or int, default=False\n",
      "     |          Amount of verbosity.\n",
      "     |      \n",
      "     |      return_n_iter : bool, default=False\n",
      "     |          Whether to return the number of iterations or not.\n",
      "     |      \n",
      "     |      positive : bool, default=False\n",
      "     |          If set to True, forces coefficients to be positive.\n",
      "     |          (Only allowed when ``y.ndim == 1``).\n",
      "     |      \n",
      "     |      check_input : bool, default=True\n",
      "     |          If set to False, the input validation checks are skipped (including the\n",
      "     |          Gram matrix when provided). It is assumed that they are handled\n",
      "     |          by the caller.\n",
      "     |      \n",
      "     |      **params : kwargs\n",
      "     |          Keyword arguments passed to the coordinate descent solver.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      alphas : ndarray of shape (n_alphas,)\n",
      "     |          The alphas along the path where models are computed.\n",
      "     |      \n",
      "     |      coefs : ndarray of shape (n_features, n_alphas) or             (n_outputs, n_features, n_alphas)\n",
      "     |          Coefficients along the path.\n",
      "     |      \n",
      "     |      dual_gaps : ndarray of shape (n_alphas,)\n",
      "     |          The dual gaps at the end of the optimization for each alpha.\n",
      "     |      \n",
      "     |      n_iters : list of int\n",
      "     |          The number of iterations taken by the coordinate descent optimizer to\n",
      "     |          reach the specified tolerance for each alpha.\n",
      "     |          (Is returned when ``return_n_iter`` is set to True).\n",
      "     |      \n",
      "     |      See Also\n",
      "     |      --------\n",
      "     |      MultiTaskElasticNet\n",
      "     |      MultiTaskElasticNetCV\n",
      "     |      ElasticNet\n",
      "     |      ElasticNetCV\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      For an example, see\n",
      "     |      :ref:`examples/linear_model/plot_lasso_coordinate_descent_path.py\n",
      "     |      <sphx_glr_auto_examples_linear_model_plot_lasso_coordinate_descent_path.py>`.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from ElasticNet:\n",
      "     |  \n",
      "     |  fit(self, X, y, sample_weight=None, check_input=True)\n",
      "     |      Fit model with coordinate descent.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {ndarray, sparse matrix} of (n_samples, n_features)\n",
      "     |          Data.\n",
      "     |      \n",
      "     |      y : {ndarray, sparse matrix} of shape (n_samples,) or             (n_samples, n_targets)\n",
      "     |          Target. Will be cast to X's dtype if necessary.\n",
      "     |      \n",
      "     |      sample_weight : float or array-like of shape (n_samples,), default=None\n",
      "     |          Sample weight.\n",
      "     |      \n",
      "     |          .. versionadded:: 0.23\n",
      "     |      \n",
      "     |      check_input : bool, default=True\n",
      "     |          Allow to bypass several input checking.\n",
      "     |          Don't use this parameter unless you know what you do.\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      \n",
      "     |      Coordinate descent is an algorithm that considers each column of\n",
      "     |      data at a time hence it will automatically convert the X input\n",
      "     |      as a Fortran-contiguous numpy array if necessary.\n",
      "     |      \n",
      "     |      To avoid memory re-allocation it is advised to allocate the\n",
      "     |      initial data in memory directly using that format.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties inherited from ElasticNet:\n",
      "     |  \n",
      "     |  sparse_coef_\n",
      "     |      Sparse representation of the fitted `coef_`.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.base.MultiOutputMixin:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.RegressorMixin:\n",
      "     |  \n",
      "     |  score(self, X, y, sample_weight=None)\n",
      "     |      Return the coefficient of determination :math:`R^2` of the\n",
      "     |      prediction.\n",
      "     |      \n",
      "     |      The coefficient :math:`R^2` is defined as :math:`(1 - \\frac{u}{v})`,\n",
      "     |      where :math:`u` is the residual sum of squares ``((y_true - y_pred)\n",
      "     |      ** 2).sum()`` and :math:`v` is the total sum of squares ``((y_true -\n",
      "     |      y_true.mean()) ** 2).sum()``. The best possible score is 1.0 and it\n",
      "     |      can be negative (because the model can be arbitrarily worse). A\n",
      "     |      constant model that always predicts the expected value of `y`,\n",
      "     |      disregarding the input features, would get a :math:`R^2` score of\n",
      "     |      0.0.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          Test samples. For some estimators this may be a precomputed\n",
      "     |          kernel matrix or a list of generic objects instead with shape\n",
      "     |          ``(n_samples, n_samples_fitted)``, where ``n_samples_fitted``\n",
      "     |          is the number of samples used in the fitting for the estimator.\n",
      "     |      \n",
      "     |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      "     |          True values for `X`.\n",
      "     |      \n",
      "     |      sample_weight : array-like of shape (n_samples,), default=None\n",
      "     |          Sample weights.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      score : float\n",
      "     |          :math:`R^2` of ``self.predict(X)`` wrt. `y`.\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      The :math:`R^2` score used when calling ``score`` on a regressor uses\n",
      "     |      ``multioutput='uniform_average'`` from version 0.23 to keep consistent\n",
      "     |      with default value of :func:`~sklearn.metrics.r2_score`.\n",
      "     |      This influences the ``score`` method of all the multioutput\n",
      "     |      regressors (except for\n",
      "     |      :class:`~sklearn.multioutput.MultiOutputRegressor`).\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.linear_model._base.LinearModel:\n",
      "     |  \n",
      "     |  predict(self, X)\n",
      "     |      Predict using the linear model.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like or sparse matrix, shape (n_samples, n_features)\n",
      "     |          Samples.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      C : array, shape (n_samples,)\n",
      "     |          Returns predicted values.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |  \n",
      "     |  __repr__(self, N_CHAR_MAX=700)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  get_params(self, deep=True)\n",
      "     |      Get parameters for this estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      deep : bool, default=True\n",
      "     |          If True, will return the parameters for this estimator and\n",
      "     |          contained subobjects that are estimators.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      params : dict\n",
      "     |          Parameter names mapped to their values.\n",
      "     |  \n",
      "     |  set_params(self, **params)\n",
      "     |      Set the parameters of this estimator.\n",
      "     |      \n",
      "     |      The method works on simple estimators as well as on nested objects\n",
      "     |      (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n",
      "     |      parameters of the form ``<component>__<parameter>`` so that it's\n",
      "     |      possible to update each component of a nested object.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      **params : dict\n",
      "     |          Estimator parameters.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : estimator instance\n",
      "     |          Estimator instance.\n",
      "    \n",
      "    class LassoCV(sklearn.base.RegressorMixin, LinearModelCV)\n",
      "     |  LassoCV(*, eps=0.001, n_alphas=100, alphas=None, fit_intercept=True, normalize=False, precompute='auto', max_iter=1000, tol=0.0001, copy_X=True, cv=None, verbose=False, n_jobs=None, positive=False, random_state=None, selection='cyclic')\n",
      "     |  \n",
      "     |  Lasso linear model with iterative fitting along a regularization path.\n",
      "     |  \n",
      "     |  See glossary entry for :term:`cross-validation estimator`.\n",
      "     |  \n",
      "     |  The best model is selected by cross-validation.\n",
      "     |  \n",
      "     |  The optimization objective for Lasso is::\n",
      "     |  \n",
      "     |      (1 / (2 * n_samples)) * ||y - Xw||^2_2 + alpha * ||w||_1\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <lasso>`.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  eps : float, default=1e-3\n",
      "     |      Length of the path. ``eps=1e-3`` means that\n",
      "     |      ``alpha_min / alpha_max = 1e-3``.\n",
      "     |  \n",
      "     |  n_alphas : int, default=100\n",
      "     |      Number of alphas along the regularization path.\n",
      "     |  \n",
      "     |  alphas : ndarray, default=None\n",
      "     |      List of alphas where to compute the models.\n",
      "     |      If ``None`` alphas are set automatically.\n",
      "     |  \n",
      "     |  fit_intercept : bool, default=True\n",
      "     |      Whether to calculate the intercept for this model. If set\n",
      "     |      to false, no intercept will be used in calculations\n",
      "     |      (i.e. data is expected to be centered).\n",
      "     |  \n",
      "     |  normalize : bool, default=False\n",
      "     |      This parameter is ignored when ``fit_intercept`` is set to False.\n",
      "     |      If True, the regressors X will be normalized before regression by\n",
      "     |      subtracting the mean and dividing by the l2-norm.\n",
      "     |      If you wish to standardize, please use\n",
      "     |      :class:`~sklearn.preprocessing.StandardScaler` before calling ``fit``\n",
      "     |      on an estimator with ``normalize=False``.\n",
      "     |  \n",
      "     |  precompute : 'auto', bool or array-like of shape (n_features, n_features),                 default='auto'\n",
      "     |      Whether to use a precomputed Gram matrix to speed up\n",
      "     |      calculations. If set to ``'auto'`` let us decide. The Gram\n",
      "     |      matrix can also be passed as argument.\n",
      "     |  \n",
      "     |  max_iter : int, default=1000\n",
      "     |      The maximum number of iterations.\n",
      "     |  \n",
      "     |  tol : float, default=1e-4\n",
      "     |      The tolerance for the optimization: if the updates are\n",
      "     |      smaller than ``tol``, the optimization code checks the\n",
      "     |      dual gap for optimality and continues until it is smaller\n",
      "     |      than ``tol``.\n",
      "     |  \n",
      "     |  copy_X : bool, default=True\n",
      "     |      If ``True``, X will be copied; else, it may be overwritten.\n",
      "     |  \n",
      "     |  cv : int, cross-validation generator or iterable, default=None\n",
      "     |      Determines the cross-validation splitting strategy.\n",
      "     |      Possible inputs for cv are:\n",
      "     |  \n",
      "     |      - None, to use the default 5-fold cross-validation,\n",
      "     |      - int, to specify the number of folds.\n",
      "     |      - :term:`CV splitter`,\n",
      "     |      - An iterable yielding (train, test) splits as arrays of indices.\n",
      "     |  \n",
      "     |      For int/None inputs, :class:`KFold` is used.\n",
      "     |  \n",
      "     |      Refer :ref:`User Guide <cross_validation>` for the various\n",
      "     |      cross-validation strategies that can be used here.\n",
      "     |  \n",
      "     |      .. versionchanged:: 0.22\n",
      "     |          ``cv`` default value if None changed from 3-fold to 5-fold.\n",
      "     |  \n",
      "     |  verbose : bool or int, default=False\n",
      "     |      Amount of verbosity.\n",
      "     |  \n",
      "     |  n_jobs : int, default=None\n",
      "     |      Number of CPUs to use during the cross validation.\n",
      "     |      ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n",
      "     |      ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n",
      "     |      for more details.\n",
      "     |  \n",
      "     |  positive : bool, default=False\n",
      "     |      If positive, restrict regression coefficients to be positive.\n",
      "     |  \n",
      "     |  random_state : int, RandomState instance, default=None\n",
      "     |      The seed of the pseudo random number generator that selects a random\n",
      "     |      feature to update. Used when ``selection`` == 'random'.\n",
      "     |      Pass an int for reproducible output across multiple function calls.\n",
      "     |      See :term:`Glossary <random_state>`.\n",
      "     |  \n",
      "     |  selection : {'cyclic', 'random'}, default='cyclic'\n",
      "     |      If set to 'random', a random coefficient is updated every iteration\n",
      "     |      rather than looping over features sequentially by default. This\n",
      "     |      (setting to 'random') often leads to significantly faster convergence\n",
      "     |      especially when tol is higher than 1e-4.\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  alpha_ : float\n",
      "     |      The amount of penalization chosen by cross validation.\n",
      "     |  \n",
      "     |  coef_ : ndarray of shape (n_features,) or (n_targets, n_features)\n",
      "     |      Parameter vector (w in the cost function formula).\n",
      "     |  \n",
      "     |  intercept_ : float or ndarray of shape (n_targets,)\n",
      "     |      Independent term in decision function.\n",
      "     |  \n",
      "     |  mse_path_ : ndarray of shape (n_alphas, n_folds)\n",
      "     |      Mean square error for the test set on each fold, varying alpha.\n",
      "     |  \n",
      "     |  alphas_ : ndarray of shape (n_alphas,)\n",
      "     |      The grid of alphas used for fitting.\n",
      "     |  \n",
      "     |  dual_gap_ : float or ndarray of shape (n_targets,)\n",
      "     |      The dual gap at the end of the optimization for the optimal alpha\n",
      "     |      (``alpha_``).\n",
      "     |  \n",
      "     |  n_iter_ : int\n",
      "     |      Number of iterations run by the coordinate descent solver to reach\n",
      "     |      the specified tolerance for the optimal alpha.\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  >>> from sklearn.linear_model import LassoCV\n",
      "     |  >>> from sklearn.datasets import make_regression\n",
      "     |  >>> X, y = make_regression(noise=4, random_state=0)\n",
      "     |  >>> reg = LassoCV(cv=5, random_state=0).fit(X, y)\n",
      "     |  >>> reg.score(X, y)\n",
      "     |  0.9993...\n",
      "     |  >>> reg.predict(X[:1,])\n",
      "     |  array([-78.4951...])\n",
      "     |  \n",
      "     |  Notes\n",
      "     |  -----\n",
      "     |  For an example, see\n",
      "     |  :ref:`examples/linear_model/plot_lasso_model_selection.py\n",
      "     |  <sphx_glr_auto_examples_linear_model_plot_lasso_model_selection.py>`.\n",
      "     |  \n",
      "     |  To avoid unnecessary memory duplication the X argument of the fit method\n",
      "     |  should be directly passed as a Fortran-contiguous numpy array.\n",
      "     |  \n",
      "     |  See Also\n",
      "     |  --------\n",
      "     |  lars_path\n",
      "     |  lasso_path\n",
      "     |  LassoLars\n",
      "     |  Lasso\n",
      "     |  LassoLarsCV\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      LassoCV\n",
      "     |      sklearn.base.RegressorMixin\n",
      "     |      LinearModelCV\n",
      "     |      sklearn.base.MultiOutputMixin\n",
      "     |      sklearn.linear_model._base.LinearModel\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, *, eps=0.001, n_alphas=100, alphas=None, fit_intercept=True, normalize=False, precompute='auto', max_iter=1000, tol=0.0001, copy_X=True, cv=None, verbose=False, n_jobs=None, positive=False, random_state=None, selection='cyclic')\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods defined here:\n",
      "     |  \n",
      "     |  path = lasso_path(X, y, *, eps=0.001, n_alphas=100, alphas=None, precompute='auto', Xy=None, copy_X=True, coef_init=None, verbose=False, return_n_iter=False, positive=False, **params)\n",
      "     |      Compute Lasso path with coordinate descent\n",
      "     |      \n",
      "     |      The Lasso optimization function varies for mono and multi-outputs.\n",
      "     |      \n",
      "     |      For mono-output tasks it is::\n",
      "     |      \n",
      "     |          (1 / (2 * n_samples)) * ||y - Xw||^2_2 + alpha * ||w||_1\n",
      "     |      \n",
      "     |      For multi-output tasks it is::\n",
      "     |      \n",
      "     |          (1 / (2 * n_samples)) * ||Y - XW||^2_Fro + alpha * ||W||_21\n",
      "     |      \n",
      "     |      Where::\n",
      "     |      \n",
      "     |          ||W||_21 = \\sum_i \\sqrt{\\sum_j w_{ij}^2}\n",
      "     |      \n",
      "     |      i.e. the sum of norm of each row.\n",
      "     |      \n",
      "     |      Read more in the :ref:`User Guide <lasso>`.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          Training data. Pass directly as Fortran-contiguous data to avoid\n",
      "     |          unnecessary memory duplication. If ``y`` is mono-output then ``X``\n",
      "     |          can be sparse.\n",
      "     |      \n",
      "     |      y : {array-like, sparse matrix} of shape (n_samples,) or         (n_samples, n_outputs)\n",
      "     |          Target values\n",
      "     |      \n",
      "     |      eps : float, default=1e-3\n",
      "     |          Length of the path. ``eps=1e-3`` means that\n",
      "     |          ``alpha_min / alpha_max = 1e-3``\n",
      "     |      \n",
      "     |      n_alphas : int, default=100\n",
      "     |          Number of alphas along the regularization path\n",
      "     |      \n",
      "     |      alphas : ndarray, default=None\n",
      "     |          List of alphas where to compute the models.\n",
      "     |          If ``None`` alphas are set automatically\n",
      "     |      \n",
      "     |      precompute : 'auto', bool or array-like of shape (n_features, n_features),                 default='auto'\n",
      "     |          Whether to use a precomputed Gram matrix to speed up\n",
      "     |          calculations. If set to ``'auto'`` let us decide. The Gram\n",
      "     |          matrix can also be passed as argument.\n",
      "     |      \n",
      "     |      Xy : array-like of shape (n_features,) or (n_features, n_outputs),         default=None\n",
      "     |          Xy = np.dot(X.T, y) that can be precomputed. It is useful\n",
      "     |          only when the Gram matrix is precomputed.\n",
      "     |      \n",
      "     |      copy_X : bool, default=True\n",
      "     |          If ``True``, X will be copied; else, it may be overwritten.\n",
      "     |      \n",
      "     |      coef_init : ndarray of shape (n_features, ), default=None\n",
      "     |          The initial values of the coefficients.\n",
      "     |      \n",
      "     |      verbose : bool or int, default=False\n",
      "     |          Amount of verbosity.\n",
      "     |      \n",
      "     |      return_n_iter : bool, default=False\n",
      "     |          whether to return the number of iterations or not.\n",
      "     |      \n",
      "     |      positive : bool, default=False\n",
      "     |          If set to True, forces coefficients to be positive.\n",
      "     |          (Only allowed when ``y.ndim == 1``).\n",
      "     |      \n",
      "     |      **params : kwargs\n",
      "     |          keyword arguments passed to the coordinate descent solver.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      alphas : ndarray of shape (n_alphas,)\n",
      "     |          The alphas along the path where models are computed.\n",
      "     |      \n",
      "     |      coefs : ndarray of shape (n_features, n_alphas) or             (n_outputs, n_features, n_alphas)\n",
      "     |          Coefficients along the path.\n",
      "     |      \n",
      "     |      dual_gaps : ndarray of shape (n_alphas,)\n",
      "     |          The dual gaps at the end of the optimization for each alpha.\n",
      "     |      \n",
      "     |      n_iters : list of int\n",
      "     |          The number of iterations taken by the coordinate descent optimizer to\n",
      "     |          reach the specified tolerance for each alpha.\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      For an example, see\n",
      "     |      :ref:`examples/linear_model/plot_lasso_coordinate_descent_path.py\n",
      "     |      <sphx_glr_auto_examples_linear_model_plot_lasso_coordinate_descent_path.py>`.\n",
      "     |      \n",
      "     |      To avoid unnecessary memory duplication the X argument of the fit method\n",
      "     |      should be directly passed as a Fortran-contiguous numpy array.\n",
      "     |      \n",
      "     |      Note that in certain cases, the Lars solver may be significantly\n",
      "     |      faster to implement this functionality. In particular, linear\n",
      "     |      interpolation can be used to retrieve model coefficients between the\n",
      "     |      values output by lars_path\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      \n",
      "     |      Comparing lasso_path and lars_path with interpolation:\n",
      "     |      \n",
      "     |      >>> X = np.array([[1, 2, 3.1], [2.3, 5.4, 4.3]]).T\n",
      "     |      >>> y = np.array([1, 2, 3.1])\n",
      "     |      >>> # Use lasso_path to compute a coefficient path\n",
      "     |      >>> _, coef_path, _ = lasso_path(X, y, alphas=[5., 1., .5])\n",
      "     |      >>> print(coef_path)\n",
      "     |      [[0.         0.         0.46874778]\n",
      "     |       [0.2159048  0.4425765  0.23689075]]\n",
      "     |      \n",
      "     |      >>> # Now use lars_path and 1D linear interpolation to compute the\n",
      "     |      >>> # same path\n",
      "     |      >>> from sklearn.linear_model import lars_path\n",
      "     |      >>> alphas, active, coef_path_lars = lars_path(X, y, method='lasso')\n",
      "     |      >>> from scipy import interpolate\n",
      "     |      >>> coef_path_continuous = interpolate.interp1d(alphas[::-1],\n",
      "     |      ...                                             coef_path_lars[:, ::-1])\n",
      "     |      >>> print(coef_path_continuous([5., 1., .5]))\n",
      "     |      [[0.         0.         0.46915237]\n",
      "     |       [0.2159048  0.4425765  0.23668876]]\n",
      "     |      \n",
      "     |      See Also\n",
      "     |      --------\n",
      "     |      lars_path\n",
      "     |      Lasso\n",
      "     |      LassoLars\n",
      "     |      LassoCV\n",
      "     |      LassoLarsCV\n",
      "     |      sklearn.decomposition.sparse_encode\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.RegressorMixin:\n",
      "     |  \n",
      "     |  score(self, X, y, sample_weight=None)\n",
      "     |      Return the coefficient of determination :math:`R^2` of the\n",
      "     |      prediction.\n",
      "     |      \n",
      "     |      The coefficient :math:`R^2` is defined as :math:`(1 - \\frac{u}{v})`,\n",
      "     |      where :math:`u` is the residual sum of squares ``((y_true - y_pred)\n",
      "     |      ** 2).sum()`` and :math:`v` is the total sum of squares ``((y_true -\n",
      "     |      y_true.mean()) ** 2).sum()``. The best possible score is 1.0 and it\n",
      "     |      can be negative (because the model can be arbitrarily worse). A\n",
      "     |      constant model that always predicts the expected value of `y`,\n",
      "     |      disregarding the input features, would get a :math:`R^2` score of\n",
      "     |      0.0.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          Test samples. For some estimators this may be a precomputed\n",
      "     |          kernel matrix or a list of generic objects instead with shape\n",
      "     |          ``(n_samples, n_samples_fitted)``, where ``n_samples_fitted``\n",
      "     |          is the number of samples used in the fitting for the estimator.\n",
      "     |      \n",
      "     |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      "     |          True values for `X`.\n",
      "     |      \n",
      "     |      sample_weight : array-like of shape (n_samples,), default=None\n",
      "     |          Sample weights.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      score : float\n",
      "     |          :math:`R^2` of ``self.predict(X)`` wrt. `y`.\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      The :math:`R^2` score used when calling ``score`` on a regressor uses\n",
      "     |      ``multioutput='uniform_average'`` from version 0.23 to keep consistent\n",
      "     |      with default value of :func:`~sklearn.metrics.r2_score`.\n",
      "     |      This influences the ``score`` method of all the multioutput\n",
      "     |      regressors (except for\n",
      "     |      :class:`~sklearn.multioutput.MultiOutputRegressor`).\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.base.RegressorMixin:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from LinearModelCV:\n",
      "     |  \n",
      "     |  fit(self, X, y)\n",
      "     |      Fit linear model with coordinate descent.\n",
      "     |      \n",
      "     |      Fit is on grid of alphas and best alpha estimated by cross-validation.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          Training data. Pass directly as Fortran-contiguous data\n",
      "     |          to avoid unnecessary memory duplication. If y is mono-output,\n",
      "     |          X can be sparse.\n",
      "     |      \n",
      "     |      y : array-like of shape (n_samples,) or (n_samples, n_targets)\n",
      "     |          Target values.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.linear_model._base.LinearModel:\n",
      "     |  \n",
      "     |  predict(self, X)\n",
      "     |      Predict using the linear model.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like or sparse matrix, shape (n_samples, n_features)\n",
      "     |          Samples.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      C : array, shape (n_samples,)\n",
      "     |          Returns predicted values.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |  \n",
      "     |  __repr__(self, N_CHAR_MAX=700)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  get_params(self, deep=True)\n",
      "     |      Get parameters for this estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      deep : bool, default=True\n",
      "     |          If True, will return the parameters for this estimator and\n",
      "     |          contained subobjects that are estimators.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      params : dict\n",
      "     |          Parameter names mapped to their values.\n",
      "     |  \n",
      "     |  set_params(self, **params)\n",
      "     |      Set the parameters of this estimator.\n",
      "     |      \n",
      "     |      The method works on simple estimators as well as on nested objects\n",
      "     |      (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n",
      "     |      parameters of the form ``<component>__<parameter>`` so that it's\n",
      "     |      possible to update each component of a nested object.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      **params : dict\n",
      "     |          Estimator parameters.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : estimator instance\n",
      "     |          Estimator instance.\n",
      "    \n",
      "    class LassoLars(Lars)\n",
      "     |  LassoLars(alpha=1.0, *, fit_intercept=True, verbose=False, normalize=True, precompute='auto', max_iter=500, eps=2.220446049250313e-16, copy_X=True, fit_path=True, positive=False, jitter=None, random_state=None)\n",
      "     |  \n",
      "     |  Lasso model fit with Least Angle Regression a.k.a. Lars\n",
      "     |  \n",
      "     |  It is a Linear Model trained with an L1 prior as regularizer.\n",
      "     |  \n",
      "     |  The optimization objective for Lasso is::\n",
      "     |  \n",
      "     |  (1 / (2 * n_samples)) * ||y - Xw||^2_2 + alpha * ||w||_1\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <least_angle_regression>`.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  alpha : float, default=1.0\n",
      "     |      Constant that multiplies the penalty term. Defaults to 1.0.\n",
      "     |      ``alpha = 0`` is equivalent to an ordinary least square, solved\n",
      "     |      by :class:`LinearRegression`. For numerical reasons, using\n",
      "     |      ``alpha = 0`` with the LassoLars object is not advised and you\n",
      "     |      should prefer the LinearRegression object.\n",
      "     |  \n",
      "     |  fit_intercept : bool, default=True\n",
      "     |      whether to calculate the intercept for this model. If set\n",
      "     |      to false, no intercept will be used in calculations\n",
      "     |      (i.e. data is expected to be centered).\n",
      "     |  \n",
      "     |  verbose : bool or int, default=False\n",
      "     |      Sets the verbosity amount.\n",
      "     |  \n",
      "     |  normalize : bool, default=True\n",
      "     |      This parameter is ignored when ``fit_intercept`` is set to False.\n",
      "     |      If True, the regressors X will be normalized before regression by\n",
      "     |      subtracting the mean and dividing by the l2-norm.\n",
      "     |      If you wish to standardize, please use\n",
      "     |      :class:`~sklearn.preprocessing.StandardScaler` before calling ``fit``\n",
      "     |      on an estimator with ``normalize=False``.\n",
      "     |  \n",
      "     |  precompute : bool, 'auto' or array-like, default='auto'\n",
      "     |      Whether to use a precomputed Gram matrix to speed up\n",
      "     |      calculations. If set to ``'auto'`` let us decide. The Gram\n",
      "     |      matrix can also be passed as argument.\n",
      "     |  \n",
      "     |  max_iter : int, default=500\n",
      "     |      Maximum number of iterations to perform.\n",
      "     |  \n",
      "     |  eps : float, default=np.finfo(float).eps\n",
      "     |      The machine-precision regularization in the computation of the\n",
      "     |      Cholesky diagonal factors. Increase this for very ill-conditioned\n",
      "     |      systems. Unlike the ``tol`` parameter in some iterative\n",
      "     |      optimization-based algorithms, this parameter does not control\n",
      "     |      the tolerance of the optimization.\n",
      "     |  \n",
      "     |  copy_X : bool, default=True\n",
      "     |      If True, X will be copied; else, it may be overwritten.\n",
      "     |  \n",
      "     |  fit_path : bool, default=True\n",
      "     |      If ``True`` the full path is stored in the ``coef_path_`` attribute.\n",
      "     |      If you compute the solution for a large problem or many targets,\n",
      "     |      setting ``fit_path`` to ``False`` will lead to a speedup, especially\n",
      "     |      with a small alpha.\n",
      "     |  \n",
      "     |  positive : bool, default=False\n",
      "     |      Restrict coefficients to be >= 0. Be aware that you might want to\n",
      "     |      remove fit_intercept which is set True by default.\n",
      "     |      Under the positive restriction the model coefficients will not converge\n",
      "     |      to the ordinary-least-squares solution for small values of alpha.\n",
      "     |      Only coefficients up to the smallest alpha value (``alphas_[alphas_ >\n",
      "     |      0.].min()`` when fit_path=True) reached by the stepwise Lars-Lasso\n",
      "     |      algorithm are typically in congruence with the solution of the\n",
      "     |      coordinate descent Lasso estimator.\n",
      "     |  \n",
      "     |  jitter : float, default=None\n",
      "     |      Upper bound on a uniform noise parameter to be added to the\n",
      "     |      `y` values, to satisfy the model's assumption of\n",
      "     |      one-at-a-time computations. Might help with stability.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.23\n",
      "     |  \n",
      "     |  random_state : int, RandomState instance or None, default=None\n",
      "     |      Determines random number generation for jittering. Pass an int\n",
      "     |      for reproducible output across multiple function calls.\n",
      "     |      See :term:`Glossary <random_state>`. Ignored if `jitter` is None.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.23\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  alphas_ : array-like of shape (n_alphas + 1,) or list of such arrays\n",
      "     |      Maximum of covariances (in absolute value) at each iteration.\n",
      "     |      ``n_alphas`` is either ``max_iter``, ``n_features`` or the\n",
      "     |      number of nodes in the path with ``alpha >= alpha_min``, whichever\n",
      "     |      is smaller. If this is a list of array-like, the length of the outer\n",
      "     |      list is `n_targets`.\n",
      "     |  \n",
      "     |  active_ : list of length n_alphas or list of such lists\n",
      "     |      Indices of active variables at the end of the path.\n",
      "     |      If this is a list of list, the length of the outer list is `n_targets`.\n",
      "     |  \n",
      "     |  coef_path_ : array-like of shape (n_features, n_alphas + 1) or list             of such arrays\n",
      "     |      If a list is passed it's expected to be one of n_targets such arrays.\n",
      "     |      The varying values of the coefficients along the path. It is not\n",
      "     |      present if the ``fit_path`` parameter is ``False``. If this is a list\n",
      "     |      of array-like, the length of the outer list is `n_targets`.\n",
      "     |  \n",
      "     |  coef_ : array-like of shape (n_features,) or (n_targets, n_features)\n",
      "     |      Parameter vector (w in the formulation formula).\n",
      "     |  \n",
      "     |  intercept_ : float or array-like of shape (n_targets,)\n",
      "     |      Independent term in decision function.\n",
      "     |  \n",
      "     |  n_iter_ : array-like or int\n",
      "     |      The number of iterations taken by lars_path to find the\n",
      "     |      grid of alphas for each target.\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  >>> from sklearn import linear_model\n",
      "     |  >>> reg = linear_model.LassoLars(alpha=0.01)\n",
      "     |  >>> reg.fit([[-1, 1], [0, 0], [1, 1]], [-1, 0, -1])\n",
      "     |  LassoLars(alpha=0.01)\n",
      "     |  >>> print(reg.coef_)\n",
      "     |  [ 0.         -0.963257...]\n",
      "     |  \n",
      "     |  See Also\n",
      "     |  --------\n",
      "     |  lars_path\n",
      "     |  lasso_path\n",
      "     |  Lasso\n",
      "     |  LassoCV\n",
      "     |  LassoLarsCV\n",
      "     |  LassoLarsIC\n",
      "     |  sklearn.decomposition.sparse_encode\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      LassoLars\n",
      "     |      Lars\n",
      "     |      sklearn.base.MultiOutputMixin\n",
      "     |      sklearn.base.RegressorMixin\n",
      "     |      sklearn.linear_model._base.LinearModel\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, alpha=1.0, *, fit_intercept=True, verbose=False, normalize=True, precompute='auto', max_iter=500, eps=2.220446049250313e-16, copy_X=True, fit_path=True, positive=False, jitter=None, random_state=None)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |  \n",
      "     |  method = 'lasso'\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from Lars:\n",
      "     |  \n",
      "     |  fit(self, X, y, Xy=None)\n",
      "     |      Fit the model using X, y as training data.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          Training data.\n",
      "     |      \n",
      "     |      y : array-like of shape (n_samples,) or (n_samples, n_targets)\n",
      "     |          Target values.\n",
      "     |      \n",
      "     |      Xy : array-like of shape (n_samples,) or (n_samples, n_targets),                 default=None\n",
      "     |          Xy = np.dot(X.T, y) that can be precomputed. It is useful\n",
      "     |          only when the Gram matrix is precomputed.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |          returns an instance of self.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from Lars:\n",
      "     |  \n",
      "     |  positive = False\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.base.MultiOutputMixin:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.RegressorMixin:\n",
      "     |  \n",
      "     |  score(self, X, y, sample_weight=None)\n",
      "     |      Return the coefficient of determination :math:`R^2` of the\n",
      "     |      prediction.\n",
      "     |      \n",
      "     |      The coefficient :math:`R^2` is defined as :math:`(1 - \\frac{u}{v})`,\n",
      "     |      where :math:`u` is the residual sum of squares ``((y_true - y_pred)\n",
      "     |      ** 2).sum()`` and :math:`v` is the total sum of squares ``((y_true -\n",
      "     |      y_true.mean()) ** 2).sum()``. The best possible score is 1.0 and it\n",
      "     |      can be negative (because the model can be arbitrarily worse). A\n",
      "     |      constant model that always predicts the expected value of `y`,\n",
      "     |      disregarding the input features, would get a :math:`R^2` score of\n",
      "     |      0.0.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          Test samples. For some estimators this may be a precomputed\n",
      "     |          kernel matrix or a list of generic objects instead with shape\n",
      "     |          ``(n_samples, n_samples_fitted)``, where ``n_samples_fitted``\n",
      "     |          is the number of samples used in the fitting for the estimator.\n",
      "     |      \n",
      "     |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      "     |          True values for `X`.\n",
      "     |      \n",
      "     |      sample_weight : array-like of shape (n_samples,), default=None\n",
      "     |          Sample weights.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      score : float\n",
      "     |          :math:`R^2` of ``self.predict(X)`` wrt. `y`.\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      The :math:`R^2` score used when calling ``score`` on a regressor uses\n",
      "     |      ``multioutput='uniform_average'`` from version 0.23 to keep consistent\n",
      "     |      with default value of :func:`~sklearn.metrics.r2_score`.\n",
      "     |      This influences the ``score`` method of all the multioutput\n",
      "     |      regressors (except for\n",
      "     |      :class:`~sklearn.multioutput.MultiOutputRegressor`).\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.linear_model._base.LinearModel:\n",
      "     |  \n",
      "     |  predict(self, X)\n",
      "     |      Predict using the linear model.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like or sparse matrix, shape (n_samples, n_features)\n",
      "     |          Samples.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      C : array, shape (n_samples,)\n",
      "     |          Returns predicted values.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |  \n",
      "     |  __repr__(self, N_CHAR_MAX=700)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  get_params(self, deep=True)\n",
      "     |      Get parameters for this estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      deep : bool, default=True\n",
      "     |          If True, will return the parameters for this estimator and\n",
      "     |          contained subobjects that are estimators.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      params : dict\n",
      "     |          Parameter names mapped to their values.\n",
      "     |  \n",
      "     |  set_params(self, **params)\n",
      "     |      Set the parameters of this estimator.\n",
      "     |      \n",
      "     |      The method works on simple estimators as well as on nested objects\n",
      "     |      (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n",
      "     |      parameters of the form ``<component>__<parameter>`` so that it's\n",
      "     |      possible to update each component of a nested object.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      **params : dict\n",
      "     |          Estimator parameters.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : estimator instance\n",
      "     |          Estimator instance.\n",
      "    \n",
      "    class LassoLarsCV(LarsCV)\n",
      "     |  LassoLarsCV(*, fit_intercept=True, verbose=False, max_iter=500, normalize=True, precompute='auto', cv=None, max_n_alphas=1000, n_jobs=None, eps=2.220446049250313e-16, copy_X=True, positive=False)\n",
      "     |  \n",
      "     |  Cross-validated Lasso, using the LARS algorithm.\n",
      "     |  \n",
      "     |  See glossary entry for :term:`cross-validation estimator`.\n",
      "     |  \n",
      "     |  The optimization objective for Lasso is::\n",
      "     |  \n",
      "     |  (1 / (2 * n_samples)) * ||y - Xw||^2_2 + alpha * ||w||_1\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <least_angle_regression>`.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  fit_intercept : bool, default=True\n",
      "     |      whether to calculate the intercept for this model. If set\n",
      "     |      to false, no intercept will be used in calculations\n",
      "     |      (i.e. data is expected to be centered).\n",
      "     |  \n",
      "     |  verbose : bool or int, default=False\n",
      "     |      Sets the verbosity amount.\n",
      "     |  \n",
      "     |  max_iter : int, default=500\n",
      "     |      Maximum number of iterations to perform.\n",
      "     |  \n",
      "     |  normalize : bool, default=True\n",
      "     |      This parameter is ignored when ``fit_intercept`` is set to False.\n",
      "     |      If True, the regressors X will be normalized before regression by\n",
      "     |      subtracting the mean and dividing by the l2-norm.\n",
      "     |      If you wish to standardize, please use\n",
      "     |      :class:`~sklearn.preprocessing.StandardScaler` before calling ``fit``\n",
      "     |      on an estimator with ``normalize=False``.\n",
      "     |  \n",
      "     |  precompute : bool or 'auto' , default='auto'\n",
      "     |      Whether to use a precomputed Gram matrix to speed up\n",
      "     |      calculations. If set to ``'auto'`` let us decide. The Gram matrix\n",
      "     |      cannot be passed as argument since we will use only subsets of X.\n",
      "     |  \n",
      "     |  cv : int, cross-validation generator or an iterable, default=None\n",
      "     |      Determines the cross-validation splitting strategy.\n",
      "     |      Possible inputs for cv are:\n",
      "     |  \n",
      "     |      - None, to use the default 5-fold cross-validation,\n",
      "     |      - integer, to specify the number of folds.\n",
      "     |      - :term:`CV splitter`,\n",
      "     |      - An iterable yielding (train, test) splits as arrays of indices.\n",
      "     |  \n",
      "     |      For integer/None inputs, :class:`KFold` is used.\n",
      "     |  \n",
      "     |      Refer :ref:`User Guide <cross_validation>` for the various\n",
      "     |      cross-validation strategies that can be used here.\n",
      "     |  \n",
      "     |      .. versionchanged:: 0.22\n",
      "     |          ``cv`` default value if None changed from 3-fold to 5-fold.\n",
      "     |  \n",
      "     |  max_n_alphas : int, default=1000\n",
      "     |      The maximum number of points on the path used to compute the\n",
      "     |      residuals in the cross-validation\n",
      "     |  \n",
      "     |  n_jobs : int or None, default=None\n",
      "     |      Number of CPUs to use during the cross validation.\n",
      "     |      ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n",
      "     |      ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n",
      "     |      for more details.\n",
      "     |  \n",
      "     |  eps : float, default=np.finfo(float).eps\n",
      "     |      The machine-precision regularization in the computation of the\n",
      "     |      Cholesky diagonal factors. Increase this for very ill-conditioned\n",
      "     |      systems. Unlike the ``tol`` parameter in some iterative\n",
      "     |      optimization-based algorithms, this parameter does not control\n",
      "     |      the tolerance of the optimization.\n",
      "     |  \n",
      "     |  copy_X : bool, default=True\n",
      "     |      If True, X will be copied; else, it may be overwritten.\n",
      "     |  \n",
      "     |  positive : bool, default=False\n",
      "     |      Restrict coefficients to be >= 0. Be aware that you might want to\n",
      "     |      remove fit_intercept which is set True by default.\n",
      "     |      Under the positive restriction the model coefficients do not converge\n",
      "     |      to the ordinary-least-squares solution for small values of alpha.\n",
      "     |      Only coefficients up to the smallest alpha value (``alphas_[alphas_ >\n",
      "     |      0.].min()`` when fit_path=True) reached by the stepwise Lars-Lasso\n",
      "     |      algorithm are typically in congruence with the solution of the\n",
      "     |      coordinate descent Lasso estimator.\n",
      "     |      As a consequence using LassoLarsCV only makes sense for problems where\n",
      "     |      a sparse solution is expected and/or reached.\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  coef_ : array-like of shape (n_features,)\n",
      "     |      parameter vector (w in the formulation formula)\n",
      "     |  \n",
      "     |  intercept_ : float\n",
      "     |      independent term in decision function.\n",
      "     |  \n",
      "     |  coef_path_ : array-like of shape (n_features, n_alphas)\n",
      "     |      the varying values of the coefficients along the path\n",
      "     |  \n",
      "     |  alpha_ : float\n",
      "     |      the estimated regularization parameter alpha\n",
      "     |  \n",
      "     |  alphas_ : array-like of shape (n_alphas,)\n",
      "     |      the different values of alpha along the path\n",
      "     |  \n",
      "     |  cv_alphas_ : array-like of shape (n_cv_alphas,)\n",
      "     |      all the values of alpha along the path for the different folds\n",
      "     |  \n",
      "     |  mse_path_ : array-like of shape (n_folds, n_cv_alphas)\n",
      "     |      the mean square error on left-out for each fold along the path\n",
      "     |      (alpha values given by ``cv_alphas``)\n",
      "     |  \n",
      "     |  n_iter_ : array-like or int\n",
      "     |      the number of iterations run by Lars with the optimal alpha.\n",
      "     |  \n",
      "     |  active_ : list of int\n",
      "     |      Indices of active variables at the end of the path.\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  >>> from sklearn.linear_model import LassoLarsCV\n",
      "     |  >>> from sklearn.datasets import make_regression\n",
      "     |  >>> X, y = make_regression(noise=4.0, random_state=0)\n",
      "     |  >>> reg = LassoLarsCV(cv=5).fit(X, y)\n",
      "     |  >>> reg.score(X, y)\n",
      "     |  0.9992...\n",
      "     |  >>> reg.alpha_\n",
      "     |  0.0484...\n",
      "     |  >>> reg.predict(X[:1,])\n",
      "     |  array([-77.8723...])\n",
      "     |  \n",
      "     |  Notes\n",
      "     |  -----\n",
      "     |  \n",
      "     |  The object solves the same problem as the LassoCV object. However,\n",
      "     |  unlike the LassoCV, it find the relevant alphas values by itself.\n",
      "     |  In general, because of this property, it will be more stable.\n",
      "     |  However, it is more fragile to heavily multicollinear datasets.\n",
      "     |  \n",
      "     |  It is more efficient than the LassoCV if only a small number of\n",
      "     |  features are selected compared to the total number, for instance if\n",
      "     |  there are very few samples compared to the number of features.\n",
      "     |  \n",
      "     |  See Also\n",
      "     |  --------\n",
      "     |  lars_path, LassoLars, LarsCV, LassoCV\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      LassoLarsCV\n",
      "     |      LarsCV\n",
      "     |      Lars\n",
      "     |      sklearn.base.MultiOutputMixin\n",
      "     |      sklearn.base.RegressorMixin\n",
      "     |      sklearn.linear_model._base.LinearModel\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, *, fit_intercept=True, verbose=False, max_iter=500, normalize=True, precompute='auto', cv=None, max_n_alphas=1000, n_jobs=None, eps=2.220446049250313e-16, copy_X=True, positive=False)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |  \n",
      "     |  method = 'lasso'\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from LarsCV:\n",
      "     |  \n",
      "     |  fit(self, X, y)\n",
      "     |      Fit the model using X, y as training data.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          Training data.\n",
      "     |      \n",
      "     |      y : array-like of shape (n_samples,)\n",
      "     |          Target values.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |          returns an instance of self.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from Lars:\n",
      "     |  \n",
      "     |  positive = False\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.base.MultiOutputMixin:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.RegressorMixin:\n",
      "     |  \n",
      "     |  score(self, X, y, sample_weight=None)\n",
      "     |      Return the coefficient of determination :math:`R^2` of the\n",
      "     |      prediction.\n",
      "     |      \n",
      "     |      The coefficient :math:`R^2` is defined as :math:`(1 - \\frac{u}{v})`,\n",
      "     |      where :math:`u` is the residual sum of squares ``((y_true - y_pred)\n",
      "     |      ** 2).sum()`` and :math:`v` is the total sum of squares ``((y_true -\n",
      "     |      y_true.mean()) ** 2).sum()``. The best possible score is 1.0 and it\n",
      "     |      can be negative (because the model can be arbitrarily worse). A\n",
      "     |      constant model that always predicts the expected value of `y`,\n",
      "     |      disregarding the input features, would get a :math:`R^2` score of\n",
      "     |      0.0.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          Test samples. For some estimators this may be a precomputed\n",
      "     |          kernel matrix or a list of generic objects instead with shape\n",
      "     |          ``(n_samples, n_samples_fitted)``, where ``n_samples_fitted``\n",
      "     |          is the number of samples used in the fitting for the estimator.\n",
      "     |      \n",
      "     |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      "     |          True values for `X`.\n",
      "     |      \n",
      "     |      sample_weight : array-like of shape (n_samples,), default=None\n",
      "     |          Sample weights.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      score : float\n",
      "     |          :math:`R^2` of ``self.predict(X)`` wrt. `y`.\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      The :math:`R^2` score used when calling ``score`` on a regressor uses\n",
      "     |      ``multioutput='uniform_average'`` from version 0.23 to keep consistent\n",
      "     |      with default value of :func:`~sklearn.metrics.r2_score`.\n",
      "     |      This influences the ``score`` method of all the multioutput\n",
      "     |      regressors (except for\n",
      "     |      :class:`~sklearn.multioutput.MultiOutputRegressor`).\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.linear_model._base.LinearModel:\n",
      "     |  \n",
      "     |  predict(self, X)\n",
      "     |      Predict using the linear model.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like or sparse matrix, shape (n_samples, n_features)\n",
      "     |          Samples.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      C : array, shape (n_samples,)\n",
      "     |          Returns predicted values.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |  \n",
      "     |  __repr__(self, N_CHAR_MAX=700)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  get_params(self, deep=True)\n",
      "     |      Get parameters for this estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      deep : bool, default=True\n",
      "     |          If True, will return the parameters for this estimator and\n",
      "     |          contained subobjects that are estimators.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      params : dict\n",
      "     |          Parameter names mapped to their values.\n",
      "     |  \n",
      "     |  set_params(self, **params)\n",
      "     |      Set the parameters of this estimator.\n",
      "     |      \n",
      "     |      The method works on simple estimators as well as on nested objects\n",
      "     |      (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n",
      "     |      parameters of the form ``<component>__<parameter>`` so that it's\n",
      "     |      possible to update each component of a nested object.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      **params : dict\n",
      "     |          Estimator parameters.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : estimator instance\n",
      "     |          Estimator instance.\n",
      "    \n",
      "    class LassoLarsIC(LassoLars)\n",
      "     |  LassoLarsIC(criterion='aic', *, fit_intercept=True, verbose=False, normalize=True, precompute='auto', max_iter=500, eps=2.220446049250313e-16, copy_X=True, positive=False)\n",
      "     |  \n",
      "     |  Lasso model fit with Lars using BIC or AIC for model selection\n",
      "     |  \n",
      "     |  The optimization objective for Lasso is::\n",
      "     |  \n",
      "     |  (1 / (2 * n_samples)) * ||y - Xw||^2_2 + alpha * ||w||_1\n",
      "     |  \n",
      "     |  AIC is the Akaike information criterion and BIC is the Bayes\n",
      "     |  Information criterion. Such criteria are useful to select the value\n",
      "     |  of the regularization parameter by making a trade-off between the\n",
      "     |  goodness of fit and the complexity of the model. A good model should\n",
      "     |  explain well the data while being simple.\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <least_angle_regression>`.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  criterion : {'bic' , 'aic'}, default='aic'\n",
      "     |      The type of criterion to use.\n",
      "     |  \n",
      "     |  fit_intercept : bool, default=True\n",
      "     |      whether to calculate the intercept for this model. If set\n",
      "     |      to false, no intercept will be used in calculations\n",
      "     |      (i.e. data is expected to be centered).\n",
      "     |  \n",
      "     |  verbose : bool or int, default=False\n",
      "     |      Sets the verbosity amount.\n",
      "     |  \n",
      "     |  normalize : bool, default=True\n",
      "     |      This parameter is ignored when ``fit_intercept`` is set to False.\n",
      "     |      If True, the regressors X will be normalized before regression by\n",
      "     |      subtracting the mean and dividing by the l2-norm.\n",
      "     |      If you wish to standardize, please use\n",
      "     |      :class:`~sklearn.preprocessing.StandardScaler` before calling ``fit``\n",
      "     |      on an estimator with ``normalize=False``.\n",
      "     |  \n",
      "     |  precompute : bool, 'auto' or array-like, default='auto'\n",
      "     |      Whether to use a precomputed Gram matrix to speed up\n",
      "     |      calculations. If set to ``'auto'`` let us decide. The Gram\n",
      "     |      matrix can also be passed as argument.\n",
      "     |  \n",
      "     |  max_iter : int, default=500\n",
      "     |      Maximum number of iterations to perform. Can be used for\n",
      "     |      early stopping.\n",
      "     |  \n",
      "     |  eps : float, default=np.finfo(float).eps\n",
      "     |      The machine-precision regularization in the computation of the\n",
      "     |      Cholesky diagonal factors. Increase this for very ill-conditioned\n",
      "     |      systems. Unlike the ``tol`` parameter in some iterative\n",
      "     |      optimization-based algorithms, this parameter does not control\n",
      "     |      the tolerance of the optimization.\n",
      "     |  \n",
      "     |  copy_X : bool, default=True\n",
      "     |      If True, X will be copied; else, it may be overwritten.\n",
      "     |  \n",
      "     |  positive : bool, default=False\n",
      "     |      Restrict coefficients to be >= 0. Be aware that you might want to\n",
      "     |      remove fit_intercept which is set True by default.\n",
      "     |      Under the positive restriction the model coefficients do not converge\n",
      "     |      to the ordinary-least-squares solution for small values of alpha.\n",
      "     |      Only coefficients up to the smallest alpha value (``alphas_[alphas_ >\n",
      "     |      0.].min()`` when fit_path=True) reached by the stepwise Lars-Lasso\n",
      "     |      algorithm are typically in congruence with the solution of the\n",
      "     |      coordinate descent Lasso estimator.\n",
      "     |      As a consequence using LassoLarsIC only makes sense for problems where\n",
      "     |      a sparse solution is expected and/or reached.\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  coef_ : array-like of shape (n_features,)\n",
      "     |      parameter vector (w in the formulation formula)\n",
      "     |  \n",
      "     |  intercept_ : float\n",
      "     |      independent term in decision function.\n",
      "     |  \n",
      "     |  alpha_ : float\n",
      "     |      the alpha parameter chosen by the information criterion\n",
      "     |  \n",
      "     |  alphas_ : array-like of shape (n_alphas + 1,) or list of such arrays\n",
      "     |      Maximum of covariances (in absolute value) at each iteration.\n",
      "     |      ``n_alphas`` is either ``max_iter``, ``n_features`` or the\n",
      "     |      number of nodes in the path with ``alpha >= alpha_min``, whichever\n",
      "     |      is smaller. If a list, it will be of length `n_targets`.\n",
      "     |  \n",
      "     |  n_iter_ : int\n",
      "     |      number of iterations run by lars_path to find the grid of\n",
      "     |      alphas.\n",
      "     |  \n",
      "     |  criterion_ : array-like of shape (n_alphas,)\n",
      "     |      The value of the information criteria ('aic', 'bic') across all\n",
      "     |      alphas. The alpha which has the smallest information criterion is\n",
      "     |      chosen. This value is larger by a factor of ``n_samples`` compared to\n",
      "     |      Eqns. 2.15 and 2.16 in (Zou et al, 2007).\n",
      "     |  \n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  >>> from sklearn import linear_model\n",
      "     |  >>> reg = linear_model.LassoLarsIC(criterion='bic')\n",
      "     |  >>> reg.fit([[-1, 1], [0, 0], [1, 1]], [-1.1111, 0, -1.1111])\n",
      "     |  LassoLarsIC(criterion='bic')\n",
      "     |  >>> print(reg.coef_)\n",
      "     |  [ 0.  -1.11...]\n",
      "     |  \n",
      "     |  Notes\n",
      "     |  -----\n",
      "     |  The estimation of the number of degrees of freedom is given by:\n",
      "     |  \n",
      "     |  \"On the degrees of freedom of the lasso\"\n",
      "     |  Hui Zou, Trevor Hastie, and Robert Tibshirani\n",
      "     |  Ann. Statist. Volume 35, Number 5 (2007), 2173-2192.\n",
      "     |  \n",
      "     |  https://en.wikipedia.org/wiki/Akaike_information_criterion\n",
      "     |  https://en.wikipedia.org/wiki/Bayesian_information_criterion\n",
      "     |  \n",
      "     |  See Also\n",
      "     |  --------\n",
      "     |  lars_path, LassoLars, LassoLarsCV\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      LassoLarsIC\n",
      "     |      LassoLars\n",
      "     |      Lars\n",
      "     |      sklearn.base.MultiOutputMixin\n",
      "     |      sklearn.base.RegressorMixin\n",
      "     |      sklearn.linear_model._base.LinearModel\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, criterion='aic', *, fit_intercept=True, verbose=False, normalize=True, precompute='auto', max_iter=500, eps=2.220446049250313e-16, copy_X=True, positive=False)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  fit(self, X, y, copy_X=None)\n",
      "     |      Fit the model using X, y as training data.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          training data.\n",
      "     |      \n",
      "     |      y : array-like of shape (n_samples,)\n",
      "     |          target values. Will be cast to X's dtype if necessary\n",
      "     |      \n",
      "     |      copy_X : bool, default=None\n",
      "     |          If provided, this parameter will override the choice\n",
      "     |          of copy_X made at instance creation.\n",
      "     |          If ``True``, X will be copied; else, it may be overwritten.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |          returns an instance of self.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from LassoLars:\n",
      "     |  \n",
      "     |  method = 'lasso'\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from Lars:\n",
      "     |  \n",
      "     |  positive = False\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.base.MultiOutputMixin:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.RegressorMixin:\n",
      "     |  \n",
      "     |  score(self, X, y, sample_weight=None)\n",
      "     |      Return the coefficient of determination :math:`R^2` of the\n",
      "     |      prediction.\n",
      "     |      \n",
      "     |      The coefficient :math:`R^2` is defined as :math:`(1 - \\frac{u}{v})`,\n",
      "     |      where :math:`u` is the residual sum of squares ``((y_true - y_pred)\n",
      "     |      ** 2).sum()`` and :math:`v` is the total sum of squares ``((y_true -\n",
      "     |      y_true.mean()) ** 2).sum()``. The best possible score is 1.0 and it\n",
      "     |      can be negative (because the model can be arbitrarily worse). A\n",
      "     |      constant model that always predicts the expected value of `y`,\n",
      "     |      disregarding the input features, would get a :math:`R^2` score of\n",
      "     |      0.0.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          Test samples. For some estimators this may be a precomputed\n",
      "     |          kernel matrix or a list of generic objects instead with shape\n",
      "     |          ``(n_samples, n_samples_fitted)``, where ``n_samples_fitted``\n",
      "     |          is the number of samples used in the fitting for the estimator.\n",
      "     |      \n",
      "     |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      "     |          True values for `X`.\n",
      "     |      \n",
      "     |      sample_weight : array-like of shape (n_samples,), default=None\n",
      "     |          Sample weights.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      score : float\n",
      "     |          :math:`R^2` of ``self.predict(X)`` wrt. `y`.\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      The :math:`R^2` score used when calling ``score`` on a regressor uses\n",
      "     |      ``multioutput='uniform_average'`` from version 0.23 to keep consistent\n",
      "     |      with default value of :func:`~sklearn.metrics.r2_score`.\n",
      "     |      This influences the ``score`` method of all the multioutput\n",
      "     |      regressors (except for\n",
      "     |      :class:`~sklearn.multioutput.MultiOutputRegressor`).\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.linear_model._base.LinearModel:\n",
      "     |  \n",
      "     |  predict(self, X)\n",
      "     |      Predict using the linear model.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like or sparse matrix, shape (n_samples, n_features)\n",
      "     |          Samples.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      C : array, shape (n_samples,)\n",
      "     |          Returns predicted values.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |  \n",
      "     |  __repr__(self, N_CHAR_MAX=700)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  get_params(self, deep=True)\n",
      "     |      Get parameters for this estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      deep : bool, default=True\n",
      "     |          If True, will return the parameters for this estimator and\n",
      "     |          contained subobjects that are estimators.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      params : dict\n",
      "     |          Parameter names mapped to their values.\n",
      "     |  \n",
      "     |  set_params(self, **params)\n",
      "     |      Set the parameters of this estimator.\n",
      "     |      \n",
      "     |      The method works on simple estimators as well as on nested objects\n",
      "     |      (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n",
      "     |      parameters of the form ``<component>__<parameter>`` so that it's\n",
      "     |      possible to update each component of a nested object.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      **params : dict\n",
      "     |          Estimator parameters.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : estimator instance\n",
      "     |          Estimator instance.\n",
      "    \n",
      "    class LinearRegression(sklearn.base.MultiOutputMixin, sklearn.base.RegressorMixin, LinearModel)\n",
      "     |  LinearRegression(*, fit_intercept=True, normalize=False, copy_X=True, n_jobs=None, positive=False)\n",
      "     |  \n",
      "     |  Ordinary least squares Linear Regression.\n",
      "     |  \n",
      "     |  LinearRegression fits a linear model with coefficients w = (w1, ..., wp)\n",
      "     |  to minimize the residual sum of squares between the observed targets in\n",
      "     |  the dataset, and the targets predicted by the linear approximation.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  fit_intercept : bool, default=True\n",
      "     |      Whether to calculate the intercept for this model. If set\n",
      "     |      to False, no intercept will be used in calculations\n",
      "     |      (i.e. data is expected to be centered).\n",
      "     |  \n",
      "     |  normalize : bool, default=False\n",
      "     |      This parameter is ignored when ``fit_intercept`` is set to False.\n",
      "     |      If True, the regressors X will be normalized before regression by\n",
      "     |      subtracting the mean and dividing by the l2-norm.\n",
      "     |      If you wish to standardize, please use\n",
      "     |      :class:`~sklearn.preprocessing.StandardScaler` before calling ``fit``\n",
      "     |      on an estimator with ``normalize=False``.\n",
      "     |  \n",
      "     |  copy_X : bool, default=True\n",
      "     |      If True, X will be copied; else, it may be overwritten.\n",
      "     |  \n",
      "     |  n_jobs : int, default=None\n",
      "     |      The number of jobs to use for the computation. This will only provide\n",
      "     |      speedup for n_targets > 1 and sufficient large problems.\n",
      "     |      ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n",
      "     |      ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n",
      "     |      for more details.\n",
      "     |  \n",
      "     |  positive : bool, default=False\n",
      "     |      When set to ``True``, forces the coefficients to be positive. This\n",
      "     |      option is only supported for dense arrays.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.24\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  coef_ : array of shape (n_features, ) or (n_targets, n_features)\n",
      "     |      Estimated coefficients for the linear regression problem.\n",
      "     |      If multiple targets are passed during the fit (y 2D), this\n",
      "     |      is a 2D array of shape (n_targets, n_features), while if only\n",
      "     |      one target is passed, this is a 1D array of length n_features.\n",
      "     |  \n",
      "     |  rank_ : int\n",
      "     |      Rank of matrix `X`. Only available when `X` is dense.\n",
      "     |  \n",
      "     |  singular_ : array of shape (min(X, y),)\n",
      "     |      Singular values of `X`. Only available when `X` is dense.\n",
      "     |  \n",
      "     |  intercept_ : float or array of shape (n_targets,)\n",
      "     |      Independent term in the linear model. Set to 0.0 if\n",
      "     |      `fit_intercept = False`.\n",
      "     |  \n",
      "     |  See Also\n",
      "     |  --------\n",
      "     |  Ridge : Ridge regression addresses some of the\n",
      "     |      problems of Ordinary Least Squares by imposing a penalty on the\n",
      "     |      size of the coefficients with l2 regularization.\n",
      "     |  Lasso : The Lasso is a linear model that estimates\n",
      "     |      sparse coefficients with l1 regularization.\n",
      "     |  ElasticNet : Elastic-Net is a linear regression\n",
      "     |      model trained with both l1 and l2 -norm regularization of the\n",
      "     |      coefficients.\n",
      "     |  \n",
      "     |  Notes\n",
      "     |  -----\n",
      "     |  From the implementation point of view, this is just plain Ordinary\n",
      "     |  Least Squares (scipy.linalg.lstsq) or Non Negative Least Squares\n",
      "     |  (scipy.optimize.nnls) wrapped as a predictor object.\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  >>> import numpy as np\n",
      "     |  >>> from sklearn.linear_model import LinearRegression\n",
      "     |  >>> X = np.array([[1, 1], [1, 2], [2, 2], [2, 3]])\n",
      "     |  >>> # y = 1 * x_0 + 2 * x_1 + 3\n",
      "     |  >>> y = np.dot(X, np.array([1, 2])) + 3\n",
      "     |  >>> reg = LinearRegression().fit(X, y)\n",
      "     |  >>> reg.score(X, y)\n",
      "     |  1.0\n",
      "     |  >>> reg.coef_\n",
      "     |  array([1., 2.])\n",
      "     |  >>> reg.intercept_\n",
      "     |  3.0...\n",
      "     |  >>> reg.predict(np.array([[3, 5]]))\n",
      "     |  array([16.])\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      LinearRegression\n",
      "     |      sklearn.base.MultiOutputMixin\n",
      "     |      sklearn.base.RegressorMixin\n",
      "     |      LinearModel\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, *, fit_intercept=True, normalize=False, copy_X=True, n_jobs=None, positive=False)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  fit(self, X, y, sample_weight=None)\n",
      "     |      Fit linear model.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          Training data\n",
      "     |      \n",
      "     |      y : array-like of shape (n_samples,) or (n_samples, n_targets)\n",
      "     |          Target values. Will be cast to X's dtype if necessary\n",
      "     |      \n",
      "     |      sample_weight : array-like of shape (n_samples,), default=None\n",
      "     |          Individual weights for each sample\n",
      "     |      \n",
      "     |          .. versionadded:: 0.17\n",
      "     |             parameter *sample_weight* support to LinearRegression.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : returns an instance of self.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.base.MultiOutputMixin:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.RegressorMixin:\n",
      "     |  \n",
      "     |  score(self, X, y, sample_weight=None)\n",
      "     |      Return the coefficient of determination :math:`R^2` of the\n",
      "     |      prediction.\n",
      "     |      \n",
      "     |      The coefficient :math:`R^2` is defined as :math:`(1 - \\frac{u}{v})`,\n",
      "     |      where :math:`u` is the residual sum of squares ``((y_true - y_pred)\n",
      "     |      ** 2).sum()`` and :math:`v` is the total sum of squares ``((y_true -\n",
      "     |      y_true.mean()) ** 2).sum()``. The best possible score is 1.0 and it\n",
      "     |      can be negative (because the model can be arbitrarily worse). A\n",
      "     |      constant model that always predicts the expected value of `y`,\n",
      "     |      disregarding the input features, would get a :math:`R^2` score of\n",
      "     |      0.0.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          Test samples. For some estimators this may be a precomputed\n",
      "     |          kernel matrix or a list of generic objects instead with shape\n",
      "     |          ``(n_samples, n_samples_fitted)``, where ``n_samples_fitted``\n",
      "     |          is the number of samples used in the fitting for the estimator.\n",
      "     |      \n",
      "     |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      "     |          True values for `X`.\n",
      "     |      \n",
      "     |      sample_weight : array-like of shape (n_samples,), default=None\n",
      "     |          Sample weights.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      score : float\n",
      "     |          :math:`R^2` of ``self.predict(X)`` wrt. `y`.\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      The :math:`R^2` score used when calling ``score`` on a regressor uses\n",
      "     |      ``multioutput='uniform_average'`` from version 0.23 to keep consistent\n",
      "     |      with default value of :func:`~sklearn.metrics.r2_score`.\n",
      "     |      This influences the ``score`` method of all the multioutput\n",
      "     |      regressors (except for\n",
      "     |      :class:`~sklearn.multioutput.MultiOutputRegressor`).\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from LinearModel:\n",
      "     |  \n",
      "     |  predict(self, X)\n",
      "     |      Predict using the linear model.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like or sparse matrix, shape (n_samples, n_features)\n",
      "     |          Samples.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      C : array, shape (n_samples,)\n",
      "     |          Returns predicted values.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |  \n",
      "     |  __repr__(self, N_CHAR_MAX=700)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  get_params(self, deep=True)\n",
      "     |      Get parameters for this estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      deep : bool, default=True\n",
      "     |          If True, will return the parameters for this estimator and\n",
      "     |          contained subobjects that are estimators.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      params : dict\n",
      "     |          Parameter names mapped to their values.\n",
      "     |  \n",
      "     |  set_params(self, **params)\n",
      "     |      Set the parameters of this estimator.\n",
      "     |      \n",
      "     |      The method works on simple estimators as well as on nested objects\n",
      "     |      (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n",
      "     |      parameters of the form ``<component>__<parameter>`` so that it's\n",
      "     |      possible to update each component of a nested object.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      **params : dict\n",
      "     |          Estimator parameters.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : estimator instance\n",
      "     |          Estimator instance.\n",
      "    \n",
      "    class Log(Classification)\n",
      "     |  Logistic regression loss for binary classification with y in {-1, 1}\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      Log\n",
      "     |      Classification\n",
      "     |      LossFunction\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __reduce__(...)\n",
      "     |      Helper for pickle.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods defined here:\n",
      "     |  \n",
      "     |  __new__(*args, **kwargs) from builtins.type\n",
      "     |      Create and return a new object.  See help(type) for accurate signature.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __pyx_vtable__ = <capsule object NULL>\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from Classification:\n",
      "     |  \n",
      "     |  __setstate__ = __setstate_cython__(...)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from LossFunction:\n",
      "     |  \n",
      "     |  py_dloss(...)\n",
      "     |      Python version of `dloss` for testing.\n",
      "     |      \n",
      "     |      Pytest needs a python function and can't use cdef functions.\n",
      "    \n",
      "    class LogisticRegression(sklearn.linear_model._base.LinearClassifierMixin, sklearn.linear_model._base.SparseCoefMixin, sklearn.base.BaseEstimator)\n",
      "     |  LogisticRegression(penalty='l2', *, dual=False, tol=0.0001, C=1.0, fit_intercept=True, intercept_scaling=1, class_weight=None, random_state=None, solver='lbfgs', max_iter=100, multi_class='auto', verbose=0, warm_start=False, n_jobs=None, l1_ratio=None)\n",
      "     |  \n",
      "     |  Logistic Regression (aka logit, MaxEnt) classifier.\n",
      "     |  \n",
      "     |  In the multiclass case, the training algorithm uses the one-vs-rest (OvR)\n",
      "     |  scheme if the 'multi_class' option is set to 'ovr', and uses the\n",
      "     |  cross-entropy loss if the 'multi_class' option is set to 'multinomial'.\n",
      "     |  (Currently the 'multinomial' option is supported only by the 'lbfgs',\n",
      "     |  'sag', 'saga' and 'newton-cg' solvers.)\n",
      "     |  \n",
      "     |  This class implements regularized logistic regression using the\n",
      "     |  'liblinear' library, 'newton-cg', 'sag', 'saga' and 'lbfgs' solvers. **Note\n",
      "     |  that regularization is applied by default**. It can handle both dense\n",
      "     |  and sparse input. Use C-ordered arrays or CSR matrices containing 64-bit\n",
      "     |  floats for optimal performance; any other input format will be converted\n",
      "     |  (and copied).\n",
      "     |  \n",
      "     |  The 'newton-cg', 'sag', and 'lbfgs' solvers support only L2 regularization\n",
      "     |  with primal formulation, or no regularization. The 'liblinear' solver\n",
      "     |  supports both L1 and L2 regularization, with a dual formulation only for\n",
      "     |  the L2 penalty. The Elastic-Net regularization is only supported by the\n",
      "     |  'saga' solver.\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <logistic_regression>`.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  penalty : {'l1', 'l2', 'elasticnet', 'none'}, default='l2'\n",
      "     |      Used to specify the norm used in the penalization. The 'newton-cg',\n",
      "     |      'sag' and 'lbfgs' solvers support only l2 penalties. 'elasticnet' is\n",
      "     |      only supported by the 'saga' solver. If 'none' (not supported by the\n",
      "     |      liblinear solver), no regularization is applied.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.19\n",
      "     |         l1 penalty with SAGA solver (allowing 'multinomial' + L1)\n",
      "     |  \n",
      "     |  dual : bool, default=False\n",
      "     |      Dual or primal formulation. Dual formulation is only implemented for\n",
      "     |      l2 penalty with liblinear solver. Prefer dual=False when\n",
      "     |      n_samples > n_features.\n",
      "     |  \n",
      "     |  tol : float, default=1e-4\n",
      "     |      Tolerance for stopping criteria.\n",
      "     |  \n",
      "     |  C : float, default=1.0\n",
      "     |      Inverse of regularization strength; must be a positive float.\n",
      "     |      Like in support vector machines, smaller values specify stronger\n",
      "     |      regularization.\n",
      "     |  \n",
      "     |  fit_intercept : bool, default=True\n",
      "     |      Specifies if a constant (a.k.a. bias or intercept) should be\n",
      "     |      added to the decision function.\n",
      "     |  \n",
      "     |  intercept_scaling : float, default=1\n",
      "     |      Useful only when the solver 'liblinear' is used\n",
      "     |      and self.fit_intercept is set to True. In this case, x becomes\n",
      "     |      [x, self.intercept_scaling],\n",
      "     |      i.e. a \"synthetic\" feature with constant value equal to\n",
      "     |      intercept_scaling is appended to the instance vector.\n",
      "     |      The intercept becomes ``intercept_scaling * synthetic_feature_weight``.\n",
      "     |  \n",
      "     |      Note! the synthetic feature weight is subject to l1/l2 regularization\n",
      "     |      as all other features.\n",
      "     |      To lessen the effect of regularization on synthetic feature weight\n",
      "     |      (and therefore on the intercept) intercept_scaling has to be increased.\n",
      "     |  \n",
      "     |  class_weight : dict or 'balanced', default=None\n",
      "     |      Weights associated with classes in the form ``{class_label: weight}``.\n",
      "     |      If not given, all classes are supposed to have weight one.\n",
      "     |  \n",
      "     |      The \"balanced\" mode uses the values of y to automatically adjust\n",
      "     |      weights inversely proportional to class frequencies in the input data\n",
      "     |      as ``n_samples / (n_classes * np.bincount(y))``.\n",
      "     |  \n",
      "     |      Note that these weights will be multiplied with sample_weight (passed\n",
      "     |      through the fit method) if sample_weight is specified.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.17\n",
      "     |         *class_weight='balanced'*\n",
      "     |  \n",
      "     |  random_state : int, RandomState instance, default=None\n",
      "     |      Used when ``solver`` == 'sag', 'saga' or 'liblinear' to shuffle the\n",
      "     |      data. See :term:`Glossary <random_state>` for details.\n",
      "     |  \n",
      "     |  solver : {'newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'},             default='lbfgs'\n",
      "     |  \n",
      "     |      Algorithm to use in the optimization problem.\n",
      "     |  \n",
      "     |      - For small datasets, 'liblinear' is a good choice, whereas 'sag' and\n",
      "     |        'saga' are faster for large ones.\n",
      "     |      - For multiclass problems, only 'newton-cg', 'sag', 'saga' and 'lbfgs'\n",
      "     |        handle multinomial loss; 'liblinear' is limited to one-versus-rest\n",
      "     |        schemes.\n",
      "     |      - 'newton-cg', 'lbfgs', 'sag' and 'saga' handle L2 or no penalty\n",
      "     |      - 'liblinear' and 'saga' also handle L1 penalty\n",
      "     |      - 'saga' also supports 'elasticnet' penalty\n",
      "     |      - 'liblinear' does not support setting ``penalty='none'``\n",
      "     |  \n",
      "     |      Note that 'sag' and 'saga' fast convergence is only guaranteed on\n",
      "     |      features with approximately the same scale. You can\n",
      "     |      preprocess the data with a scaler from sklearn.preprocessing.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.17\n",
      "     |         Stochastic Average Gradient descent solver.\n",
      "     |      .. versionadded:: 0.19\n",
      "     |         SAGA solver.\n",
      "     |      .. versionchanged:: 0.22\n",
      "     |          The default solver changed from 'liblinear' to 'lbfgs' in 0.22.\n",
      "     |  \n",
      "     |  max_iter : int, default=100\n",
      "     |      Maximum number of iterations taken for the solvers to converge.\n",
      "     |  \n",
      "     |  multi_class : {'auto', 'ovr', 'multinomial'}, default='auto'\n",
      "     |      If the option chosen is 'ovr', then a binary problem is fit for each\n",
      "     |      label. For 'multinomial' the loss minimised is the multinomial loss fit\n",
      "     |      across the entire probability distribution, *even when the data is\n",
      "     |      binary*. 'multinomial' is unavailable when solver='liblinear'.\n",
      "     |      'auto' selects 'ovr' if the data is binary, or if solver='liblinear',\n",
      "     |      and otherwise selects 'multinomial'.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.18\n",
      "     |         Stochastic Average Gradient descent solver for 'multinomial' case.\n",
      "     |      .. versionchanged:: 0.22\n",
      "     |          Default changed from 'ovr' to 'auto' in 0.22.\n",
      "     |  \n",
      "     |  verbose : int, default=0\n",
      "     |      For the liblinear and lbfgs solvers set verbose to any positive\n",
      "     |      number for verbosity.\n",
      "     |  \n",
      "     |  warm_start : bool, default=False\n",
      "     |      When set to True, reuse the solution of the previous call to fit as\n",
      "     |      initialization, otherwise, just erase the previous solution.\n",
      "     |      Useless for liblinear solver. See :term:`the Glossary <warm_start>`.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.17\n",
      "     |         *warm_start* to support *lbfgs*, *newton-cg*, *sag*, *saga* solvers.\n",
      "     |  \n",
      "     |  n_jobs : int, default=None\n",
      "     |      Number of CPU cores used when parallelizing over classes if\n",
      "     |      multi_class='ovr'\". This parameter is ignored when the ``solver`` is\n",
      "     |      set to 'liblinear' regardless of whether 'multi_class' is specified or\n",
      "     |      not. ``None`` means 1 unless in a :obj:`joblib.parallel_backend`\n",
      "     |      context. ``-1`` means using all processors.\n",
      "     |      See :term:`Glossary <n_jobs>` for more details.\n",
      "     |  \n",
      "     |  l1_ratio : float, default=None\n",
      "     |      The Elastic-Net mixing parameter, with ``0 <= l1_ratio <= 1``. Only\n",
      "     |      used if ``penalty='elasticnet'``. Setting ``l1_ratio=0`` is equivalent\n",
      "     |      to using ``penalty='l2'``, while setting ``l1_ratio=1`` is equivalent\n",
      "     |      to using ``penalty='l1'``. For ``0 < l1_ratio <1``, the penalty is a\n",
      "     |      combination of L1 and L2.\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  \n",
      "     |  classes_ : ndarray of shape (n_classes, )\n",
      "     |      A list of class labels known to the classifier.\n",
      "     |  \n",
      "     |  coef_ : ndarray of shape (1, n_features) or (n_classes, n_features)\n",
      "     |      Coefficient of the features in the decision function.\n",
      "     |  \n",
      "     |      `coef_` is of shape (1, n_features) when the given problem is binary.\n",
      "     |      In particular, when `multi_class='multinomial'`, `coef_` corresponds\n",
      "     |      to outcome 1 (True) and `-coef_` corresponds to outcome 0 (False).\n",
      "     |  \n",
      "     |  intercept_ : ndarray of shape (1,) or (n_classes,)\n",
      "     |      Intercept (a.k.a. bias) added to the decision function.\n",
      "     |  \n",
      "     |      If `fit_intercept` is set to False, the intercept is set to zero.\n",
      "     |      `intercept_` is of shape (1,) when the given problem is binary.\n",
      "     |      In particular, when `multi_class='multinomial'`, `intercept_`\n",
      "     |      corresponds to outcome 1 (True) and `-intercept_` corresponds to\n",
      "     |      outcome 0 (False).\n",
      "     |  \n",
      "     |  n_iter_ : ndarray of shape (n_classes,) or (1, )\n",
      "     |      Actual number of iterations for all classes. If binary or multinomial,\n",
      "     |      it returns only 1 element. For liblinear solver, only the maximum\n",
      "     |      number of iteration across all classes is given.\n",
      "     |  \n",
      "     |      .. versionchanged:: 0.20\n",
      "     |  \n",
      "     |          In SciPy <= 1.0.0 the number of lbfgs iterations may exceed\n",
      "     |          ``max_iter``. ``n_iter_`` will now report at most ``max_iter``.\n",
      "     |  \n",
      "     |  See Also\n",
      "     |  --------\n",
      "     |  SGDClassifier : Incrementally trained logistic regression (when given\n",
      "     |      the parameter ``loss=\"log\"``).\n",
      "     |  LogisticRegressionCV : Logistic regression with built-in cross validation.\n",
      "     |  \n",
      "     |  Notes\n",
      "     |  -----\n",
      "     |  The underlying C implementation uses a random number generator to\n",
      "     |  select features when fitting the model. It is thus not uncommon,\n",
      "     |  to have slightly different results for the same input data. If\n",
      "     |  that happens, try with a smaller tol parameter.\n",
      "     |  \n",
      "     |  Predict output may not match that of standalone liblinear in certain\n",
      "     |  cases. See :ref:`differences from liblinear <liblinear_differences>`\n",
      "     |  in the narrative documentation.\n",
      "     |  \n",
      "     |  References\n",
      "     |  ----------\n",
      "     |  \n",
      "     |  L-BFGS-B -- Software for Large-scale Bound-constrained Optimization\n",
      "     |      Ciyou Zhu, Richard Byrd, Jorge Nocedal and Jose Luis Morales.\n",
      "     |      http://users.iems.northwestern.edu/~nocedal/lbfgsb.html\n",
      "     |  \n",
      "     |  LIBLINEAR -- A Library for Large Linear Classification\n",
      "     |      https://www.csie.ntu.edu.tw/~cjlin/liblinear/\n",
      "     |  \n",
      "     |  SAG -- Mark Schmidt, Nicolas Le Roux, and Francis Bach\n",
      "     |      Minimizing Finite Sums with the Stochastic Average Gradient\n",
      "     |      https://hal.inria.fr/hal-00860051/document\n",
      "     |  \n",
      "     |  SAGA -- Defazio, A., Bach F. & Lacoste-Julien S. (2014).\n",
      "     |      SAGA: A Fast Incremental Gradient Method With Support\n",
      "     |      for Non-Strongly Convex Composite Objectives\n",
      "     |      https://arxiv.org/abs/1407.0202\n",
      "     |  \n",
      "     |  Hsiang-Fu Yu, Fang-Lan Huang, Chih-Jen Lin (2011). Dual coordinate descent\n",
      "     |      methods for logistic regression and maximum entropy models.\n",
      "     |      Machine Learning 85(1-2):41-75.\n",
      "     |      https://www.csie.ntu.edu.tw/~cjlin/papers/maxent_dual.pdf\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  >>> from sklearn.datasets import load_iris\n",
      "     |  >>> from sklearn.linear_model import LogisticRegression\n",
      "     |  >>> X, y = load_iris(return_X_y=True)\n",
      "     |  >>> clf = LogisticRegression(random_state=0).fit(X, y)\n",
      "     |  >>> clf.predict(X[:2, :])\n",
      "     |  array([0, 0])\n",
      "     |  >>> clf.predict_proba(X[:2, :])\n",
      "     |  array([[9.8...e-01, 1.8...e-02, 1.4...e-08],\n",
      "     |         [9.7...e-01, 2.8...e-02, ...e-08]])\n",
      "     |  >>> clf.score(X, y)\n",
      "     |  0.97...\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      LogisticRegression\n",
      "     |      sklearn.linear_model._base.LinearClassifierMixin\n",
      "     |      sklearn.base.ClassifierMixin\n",
      "     |      sklearn.linear_model._base.SparseCoefMixin\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, penalty='l2', *, dual=False, tol=0.0001, C=1.0, fit_intercept=True, intercept_scaling=1, class_weight=None, random_state=None, solver='lbfgs', max_iter=100, multi_class='auto', verbose=0, warm_start=False, n_jobs=None, l1_ratio=None)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  fit(self, X, y, sample_weight=None)\n",
      "     |      Fit the model according to the given training data.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          Training vector, where n_samples is the number of samples and\n",
      "     |          n_features is the number of features.\n",
      "     |      \n",
      "     |      y : array-like of shape (n_samples,)\n",
      "     |          Target vector relative to X.\n",
      "     |      \n",
      "     |      sample_weight : array-like of shape (n_samples,) default=None\n",
      "     |          Array of weights that are assigned to individual samples.\n",
      "     |          If not provided, then each sample is given unit weight.\n",
      "     |      \n",
      "     |          .. versionadded:: 0.17\n",
      "     |             *sample_weight* support to LogisticRegression.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self\n",
      "     |          Fitted estimator.\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      The SAGA solver supports both float64 and float32 bit arrays.\n",
      "     |  \n",
      "     |  predict_log_proba(self, X)\n",
      "     |      Predict logarithm of probability estimates.\n",
      "     |      \n",
      "     |      The returned estimates for all classes are ordered by the\n",
      "     |      label of classes.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          Vector to be scored, where `n_samples` is the number of samples and\n",
      "     |          `n_features` is the number of features.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      T : array-like of shape (n_samples, n_classes)\n",
      "     |          Returns the log-probability of the sample for each class in the\n",
      "     |          model, where classes are ordered as they are in ``self.classes_``.\n",
      "     |  \n",
      "     |  predict_proba(self, X)\n",
      "     |      Probability estimates.\n",
      "     |      \n",
      "     |      The returned estimates for all classes are ordered by the\n",
      "     |      label of classes.\n",
      "     |      \n",
      "     |      For a multi_class problem, if multi_class is set to be \"multinomial\"\n",
      "     |      the softmax function is used to find the predicted probability of\n",
      "     |      each class.\n",
      "     |      Else use a one-vs-rest approach, i.e calculate the probability\n",
      "     |      of each class assuming it to be positive using the logistic function.\n",
      "     |      and normalize these values across all the classes.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          Vector to be scored, where `n_samples` is the number of samples and\n",
      "     |          `n_features` is the number of features.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      T : array-like of shape (n_samples, n_classes)\n",
      "     |          Returns the probability of the sample for each class in the model,\n",
      "     |          where classes are ordered as they are in ``self.classes_``.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.linear_model._base.LinearClassifierMixin:\n",
      "     |  \n",
      "     |  decision_function(self, X)\n",
      "     |      Predict confidence scores for samples.\n",
      "     |      \n",
      "     |      The confidence score for a sample is proportional to the signed\n",
      "     |      distance of that sample to the hyperplane.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like or sparse matrix, shape (n_samples, n_features)\n",
      "     |          Samples.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      array, shape=(n_samples,) if n_classes == 2 else (n_samples, n_classes)\n",
      "     |          Confidence scores per (sample, class) combination. In the binary\n",
      "     |          case, confidence score for self.classes_[1] where >0 means this\n",
      "     |          class would be predicted.\n",
      "     |  \n",
      "     |  predict(self, X)\n",
      "     |      Predict class labels for samples in X.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like or sparse matrix, shape (n_samples, n_features)\n",
      "     |          Samples.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      C : array, shape [n_samples]\n",
      "     |          Predicted class label per sample.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.ClassifierMixin:\n",
      "     |  \n",
      "     |  score(self, X, y, sample_weight=None)\n",
      "     |      Return the mean accuracy on the given test data and labels.\n",
      "     |      \n",
      "     |      In multi-label classification, this is the subset accuracy\n",
      "     |      which is a harsh metric since you require for each sample that\n",
      "     |      each label set be correctly predicted.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          Test samples.\n",
      "     |      \n",
      "     |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      "     |          True labels for `X`.\n",
      "     |      \n",
      "     |      sample_weight : array-like of shape (n_samples,), default=None\n",
      "     |          Sample weights.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      score : float\n",
      "     |          Mean accuracy of ``self.predict(X)`` wrt. `y`.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.base.ClassifierMixin:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.linear_model._base.SparseCoefMixin:\n",
      "     |  \n",
      "     |  densify(self)\n",
      "     |      Convert coefficient matrix to dense array format.\n",
      "     |      \n",
      "     |      Converts the ``coef_`` member (back) to a numpy.ndarray. This is the\n",
      "     |      default format of ``coef_`` and is required for fitting, so calling\n",
      "     |      this method is only required on models that have previously been\n",
      "     |      sparsified; otherwise, it is a no-op.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self\n",
      "     |          Fitted estimator.\n",
      "     |  \n",
      "     |  sparsify(self)\n",
      "     |      Convert coefficient matrix to sparse format.\n",
      "     |      \n",
      "     |      Converts the ``coef_`` member to a scipy.sparse matrix, which for\n",
      "     |      L1-regularized models can be much more memory- and storage-efficient\n",
      "     |      than the usual numpy.ndarray representation.\n",
      "     |      \n",
      "     |      The ``intercept_`` member is not converted.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self\n",
      "     |          Fitted estimator.\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      For non-sparse models, i.e. when there are not many zeros in ``coef_``,\n",
      "     |      this may actually *increase* memory usage, so use this method with\n",
      "     |      care. A rule of thumb is that the number of zero elements, which can\n",
      "     |      be computed with ``(coef_ == 0).sum()``, must be more than 50% for this\n",
      "     |      to provide significant benefits.\n",
      "     |      \n",
      "     |      After calling this method, further fitting with the partial_fit\n",
      "     |      method (if any) will not work until you call densify.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |  \n",
      "     |  __repr__(self, N_CHAR_MAX=700)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  get_params(self, deep=True)\n",
      "     |      Get parameters for this estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      deep : bool, default=True\n",
      "     |          If True, will return the parameters for this estimator and\n",
      "     |          contained subobjects that are estimators.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      params : dict\n",
      "     |          Parameter names mapped to their values.\n",
      "     |  \n",
      "     |  set_params(self, **params)\n",
      "     |      Set the parameters of this estimator.\n",
      "     |      \n",
      "     |      The method works on simple estimators as well as on nested objects\n",
      "     |      (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n",
      "     |      parameters of the form ``<component>__<parameter>`` so that it's\n",
      "     |      possible to update each component of a nested object.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      **params : dict\n",
      "     |          Estimator parameters.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : estimator instance\n",
      "     |          Estimator instance.\n",
      "    \n",
      "    class LogisticRegressionCV(LogisticRegression, sklearn.linear_model._base.LinearClassifierMixin, sklearn.base.BaseEstimator)\n",
      "     |  LogisticRegressionCV(*, Cs=10, fit_intercept=True, cv=None, dual=False, penalty='l2', scoring=None, solver='lbfgs', tol=0.0001, max_iter=100, class_weight=None, n_jobs=None, verbose=0, refit=True, intercept_scaling=1.0, multi_class='auto', random_state=None, l1_ratios=None)\n",
      "     |  \n",
      "     |  Logistic Regression CV (aka logit, MaxEnt) classifier.\n",
      "     |  \n",
      "     |  See glossary entry for :term:`cross-validation estimator`.\n",
      "     |  \n",
      "     |  This class implements logistic regression using liblinear, newton-cg, sag\n",
      "     |  of lbfgs optimizer. The newton-cg, sag and lbfgs solvers support only L2\n",
      "     |  regularization with primal formulation. The liblinear solver supports both\n",
      "     |  L1 and L2 regularization, with a dual formulation only for the L2 penalty.\n",
      "     |  Elastic-Net penalty is only supported by the saga solver.\n",
      "     |  \n",
      "     |  For the grid of `Cs` values and `l1_ratios` values, the best hyperparameter\n",
      "     |  is selected by the cross-validator\n",
      "     |  :class:`~sklearn.model_selection.StratifiedKFold`, but it can be changed\n",
      "     |  using the :term:`cv` parameter. The 'newton-cg', 'sag', 'saga' and 'lbfgs'\n",
      "     |  solvers can warm-start the coefficients (see :term:`Glossary<warm_start>`).\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <logistic_regression>`.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  Cs : int or list of floats, default=10\n",
      "     |      Each of the values in Cs describes the inverse of regularization\n",
      "     |      strength. If Cs is as an int, then a grid of Cs values are chosen\n",
      "     |      in a logarithmic scale between 1e-4 and 1e4.\n",
      "     |      Like in support vector machines, smaller values specify stronger\n",
      "     |      regularization.\n",
      "     |  \n",
      "     |  fit_intercept : bool, default=True\n",
      "     |      Specifies if a constant (a.k.a. bias or intercept) should be\n",
      "     |      added to the decision function.\n",
      "     |  \n",
      "     |  cv : int or cross-validation generator, default=None\n",
      "     |      The default cross-validation generator used is Stratified K-Folds.\n",
      "     |      If an integer is provided, then it is the number of folds used.\n",
      "     |      See the module :mod:`sklearn.model_selection` module for the\n",
      "     |      list of possible cross-validation objects.\n",
      "     |  \n",
      "     |      .. versionchanged:: 0.22\n",
      "     |          ``cv`` default value if None changed from 3-fold to 5-fold.\n",
      "     |  \n",
      "     |  dual : bool, default=False\n",
      "     |      Dual or primal formulation. Dual formulation is only implemented for\n",
      "     |      l2 penalty with liblinear solver. Prefer dual=False when\n",
      "     |      n_samples > n_features.\n",
      "     |  \n",
      "     |  penalty : {'l1', 'l2', 'elasticnet'}, default='l2'\n",
      "     |      Used to specify the norm used in the penalization. The 'newton-cg',\n",
      "     |      'sag' and 'lbfgs' solvers support only l2 penalties. 'elasticnet' is\n",
      "     |      only supported by the 'saga' solver.\n",
      "     |  \n",
      "     |  scoring : str or callable, default=None\n",
      "     |      A string (see model evaluation documentation) or\n",
      "     |      a scorer callable object / function with signature\n",
      "     |      ``scorer(estimator, X, y)``. For a list of scoring functions\n",
      "     |      that can be used, look at :mod:`sklearn.metrics`. The\n",
      "     |      default scoring option used is 'accuracy'.\n",
      "     |  \n",
      "     |  solver : {'newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'},             default='lbfgs'\n",
      "     |  \n",
      "     |      Algorithm to use in the optimization problem.\n",
      "     |  \n",
      "     |      - For small datasets, 'liblinear' is a good choice, whereas 'sag' and\n",
      "     |        'saga' are faster for large ones.\n",
      "     |      - For multiclass problems, only 'newton-cg', 'sag', 'saga' and 'lbfgs'\n",
      "     |        handle multinomial loss; 'liblinear' is limited to one-versus-rest\n",
      "     |        schemes.\n",
      "     |      - 'newton-cg', 'lbfgs' and 'sag' only handle L2 penalty, whereas\n",
      "     |        'liblinear' and 'saga' handle L1 penalty.\n",
      "     |      - 'liblinear' might be slower in LogisticRegressionCV because it does\n",
      "     |        not handle warm-starting.\n",
      "     |  \n",
      "     |      Note that 'sag' and 'saga' fast convergence is only guaranteed on\n",
      "     |      features with approximately the same scale. You can preprocess the data\n",
      "     |      with a scaler from sklearn.preprocessing.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.17\n",
      "     |         Stochastic Average Gradient descent solver.\n",
      "     |      .. versionadded:: 0.19\n",
      "     |         SAGA solver.\n",
      "     |  \n",
      "     |  tol : float, default=1e-4\n",
      "     |      Tolerance for stopping criteria.\n",
      "     |  \n",
      "     |  max_iter : int, default=100\n",
      "     |      Maximum number of iterations of the optimization algorithm.\n",
      "     |  \n",
      "     |  class_weight : dict or 'balanced', default=None\n",
      "     |      Weights associated with classes in the form ``{class_label: weight}``.\n",
      "     |      If not given, all classes are supposed to have weight one.\n",
      "     |  \n",
      "     |      The \"balanced\" mode uses the values of y to automatically adjust\n",
      "     |      weights inversely proportional to class frequencies in the input data\n",
      "     |      as ``n_samples / (n_classes * np.bincount(y))``.\n",
      "     |  \n",
      "     |      Note that these weights will be multiplied with sample_weight (passed\n",
      "     |      through the fit method) if sample_weight is specified.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.17\n",
      "     |         class_weight == 'balanced'\n",
      "     |  \n",
      "     |  n_jobs : int, default=None\n",
      "     |      Number of CPU cores used during the cross-validation loop.\n",
      "     |      ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n",
      "     |      ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n",
      "     |      for more details.\n",
      "     |  \n",
      "     |  verbose : int, default=0\n",
      "     |      For the 'liblinear', 'sag' and 'lbfgs' solvers set verbose to any\n",
      "     |      positive number for verbosity.\n",
      "     |  \n",
      "     |  refit : bool, default=True\n",
      "     |      If set to True, the scores are averaged across all folds, and the\n",
      "     |      coefs and the C that corresponds to the best score is taken, and a\n",
      "     |      final refit is done using these parameters.\n",
      "     |      Otherwise the coefs, intercepts and C that correspond to the\n",
      "     |      best scores across folds are averaged.\n",
      "     |  \n",
      "     |  intercept_scaling : float, default=1\n",
      "     |      Useful only when the solver 'liblinear' is used\n",
      "     |      and self.fit_intercept is set to True. In this case, x becomes\n",
      "     |      [x, self.intercept_scaling],\n",
      "     |      i.e. a \"synthetic\" feature with constant value equal to\n",
      "     |      intercept_scaling is appended to the instance vector.\n",
      "     |      The intercept becomes ``intercept_scaling * synthetic_feature_weight``.\n",
      "     |  \n",
      "     |      Note! the synthetic feature weight is subject to l1/l2 regularization\n",
      "     |      as all other features.\n",
      "     |      To lessen the effect of regularization on synthetic feature weight\n",
      "     |      (and therefore on the intercept) intercept_scaling has to be increased.\n",
      "     |  \n",
      "     |  multi_class : {'auto, 'ovr', 'multinomial'}, default='auto'\n",
      "     |      If the option chosen is 'ovr', then a binary problem is fit for each\n",
      "     |      label. For 'multinomial' the loss minimised is the multinomial loss fit\n",
      "     |      across the entire probability distribution, *even when the data is\n",
      "     |      binary*. 'multinomial' is unavailable when solver='liblinear'.\n",
      "     |      'auto' selects 'ovr' if the data is binary, or if solver='liblinear',\n",
      "     |      and otherwise selects 'multinomial'.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.18\n",
      "     |         Stochastic Average Gradient descent solver for 'multinomial' case.\n",
      "     |      .. versionchanged:: 0.22\n",
      "     |          Default changed from 'ovr' to 'auto' in 0.22.\n",
      "     |  \n",
      "     |  random_state : int, RandomState instance, default=None\n",
      "     |      Used when `solver='sag'`, 'saga' or 'liblinear' to shuffle the data.\n",
      "     |      Note that this only applies to the solver and not the cross-validation\n",
      "     |      generator. See :term:`Glossary <random_state>` for details.\n",
      "     |  \n",
      "     |  l1_ratios : list of float, default=None\n",
      "     |      The list of Elastic-Net mixing parameter, with ``0 <= l1_ratio <= 1``.\n",
      "     |      Only used if ``penalty='elasticnet'``. A value of 0 is equivalent to\n",
      "     |      using ``penalty='l2'``, while 1 is equivalent to using\n",
      "     |      ``penalty='l1'``. For ``0 < l1_ratio <1``, the penalty is a combination\n",
      "     |      of L1 and L2.\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  classes_ : ndarray of shape (n_classes, )\n",
      "     |      A list of class labels known to the classifier.\n",
      "     |  \n",
      "     |  coef_ : ndarray of shape (1, n_features) or (n_classes, n_features)\n",
      "     |      Coefficient of the features in the decision function.\n",
      "     |  \n",
      "     |      `coef_` is of shape (1, n_features) when the given problem\n",
      "     |      is binary.\n",
      "     |  \n",
      "     |  intercept_ : ndarray of shape (1,) or (n_classes,)\n",
      "     |      Intercept (a.k.a. bias) added to the decision function.\n",
      "     |  \n",
      "     |      If `fit_intercept` is set to False, the intercept is set to zero.\n",
      "     |      `intercept_` is of shape(1,) when the problem is binary.\n",
      "     |  \n",
      "     |  Cs_ : ndarray of shape (n_cs)\n",
      "     |      Array of C i.e. inverse of regularization parameter values used\n",
      "     |      for cross-validation.\n",
      "     |  \n",
      "     |  l1_ratios_ : ndarray of shape (n_l1_ratios)\n",
      "     |      Array of l1_ratios used for cross-validation. If no l1_ratio is used\n",
      "     |      (i.e. penalty is not 'elasticnet'), this is set to ``[None]``\n",
      "     |  \n",
      "     |  coefs_paths_ : ndarray of shape (n_folds, n_cs, n_features) or                    (n_folds, n_cs, n_features + 1)\n",
      "     |      dict with classes as the keys, and the path of coefficients obtained\n",
      "     |      during cross-validating across each fold and then across each Cs\n",
      "     |      after doing an OvR for the corresponding class as values.\n",
      "     |      If the 'multi_class' option is set to 'multinomial', then\n",
      "     |      the coefs_paths are the coefficients corresponding to each class.\n",
      "     |      Each dict value has shape ``(n_folds, n_cs, n_features)`` or\n",
      "     |      ``(n_folds, n_cs, n_features + 1)`` depending on whether the\n",
      "     |      intercept is fit or not. If ``penalty='elasticnet'``, the shape is\n",
      "     |      ``(n_folds, n_cs, n_l1_ratios_, n_features)`` or\n",
      "     |      ``(n_folds, n_cs, n_l1_ratios_, n_features + 1)``.\n",
      "     |  \n",
      "     |  scores_ : dict\n",
      "     |      dict with classes as the keys, and the values as the\n",
      "     |      grid of scores obtained during cross-validating each fold, after doing\n",
      "     |      an OvR for the corresponding class. If the 'multi_class' option\n",
      "     |      given is 'multinomial' then the same scores are repeated across\n",
      "     |      all classes, since this is the multinomial class. Each dict value\n",
      "     |      has shape ``(n_folds, n_cs`` or ``(n_folds, n_cs, n_l1_ratios)`` if\n",
      "     |      ``penalty='elasticnet'``.\n",
      "     |  \n",
      "     |  C_ : ndarray of shape (n_classes,) or (n_classes - 1,)\n",
      "     |      Array of C that maps to the best scores across every class. If refit is\n",
      "     |      set to False, then for each class, the best C is the average of the\n",
      "     |      C's that correspond to the best scores for each fold.\n",
      "     |      `C_` is of shape(n_classes,) when the problem is binary.\n",
      "     |  \n",
      "     |  l1_ratio_ : ndarray of shape (n_classes,) or (n_classes - 1,)\n",
      "     |      Array of l1_ratio that maps to the best scores across every class. If\n",
      "     |      refit is set to False, then for each class, the best l1_ratio is the\n",
      "     |      average of the l1_ratio's that correspond to the best scores for each\n",
      "     |      fold.  `l1_ratio_` is of shape(n_classes,) when the problem is binary.\n",
      "     |  \n",
      "     |  n_iter_ : ndarray of shape (n_classes, n_folds, n_cs) or (1, n_folds, n_cs)\n",
      "     |      Actual number of iterations for all classes, folds and Cs.\n",
      "     |      In the binary or multinomial cases, the first dimension is equal to 1.\n",
      "     |      If ``penalty='elasticnet'``, the shape is ``(n_classes, n_folds,\n",
      "     |      n_cs, n_l1_ratios)`` or ``(1, n_folds, n_cs, n_l1_ratios)``.\n",
      "     |  \n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  >>> from sklearn.datasets import load_iris\n",
      "     |  >>> from sklearn.linear_model import LogisticRegressionCV\n",
      "     |  >>> X, y = load_iris(return_X_y=True)\n",
      "     |  >>> clf = LogisticRegressionCV(cv=5, random_state=0).fit(X, y)\n",
      "     |  >>> clf.predict(X[:2, :])\n",
      "     |  array([0, 0])\n",
      "     |  >>> clf.predict_proba(X[:2, :]).shape\n",
      "     |  (2, 3)\n",
      "     |  >>> clf.score(X, y)\n",
      "     |  0.98...\n",
      "     |  \n",
      "     |  See Also\n",
      "     |  --------\n",
      "     |  LogisticRegression\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      LogisticRegressionCV\n",
      "     |      LogisticRegression\n",
      "     |      sklearn.linear_model._base.LinearClassifierMixin\n",
      "     |      sklearn.base.ClassifierMixin\n",
      "     |      sklearn.linear_model._base.SparseCoefMixin\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, *, Cs=10, fit_intercept=True, cv=None, dual=False, penalty='l2', scoring=None, solver='lbfgs', tol=0.0001, max_iter=100, class_weight=None, n_jobs=None, verbose=0, refit=True, intercept_scaling=1.0, multi_class='auto', random_state=None, l1_ratios=None)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  fit(self, X, y, sample_weight=None)\n",
      "     |      Fit the model according to the given training data.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          Training vector, where n_samples is the number of samples and\n",
      "     |          n_features is the number of features.\n",
      "     |      \n",
      "     |      y : array-like of shape (n_samples,)\n",
      "     |          Target vector relative to X.\n",
      "     |      \n",
      "     |      sample_weight : array-like of shape (n_samples,) default=None\n",
      "     |          Array of weights that are assigned to individual samples.\n",
      "     |          If not provided, then each sample is given unit weight.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |  \n",
      "     |  score(self, X, y, sample_weight=None)\n",
      "     |      Returns the score using the `scoring` option on the given\n",
      "     |      test data and labels.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          Test samples.\n",
      "     |      \n",
      "     |      y : array-like of shape (n_samples,)\n",
      "     |          True labels for X.\n",
      "     |      \n",
      "     |      sample_weight : array-like of shape (n_samples,), default=None\n",
      "     |          Sample weights.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      score : float\n",
      "     |          Score of self.predict(X) wrt. y.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from LogisticRegression:\n",
      "     |  \n",
      "     |  predict_log_proba(self, X)\n",
      "     |      Predict logarithm of probability estimates.\n",
      "     |      \n",
      "     |      The returned estimates for all classes are ordered by the\n",
      "     |      label of classes.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          Vector to be scored, where `n_samples` is the number of samples and\n",
      "     |          `n_features` is the number of features.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      T : array-like of shape (n_samples, n_classes)\n",
      "     |          Returns the log-probability of the sample for each class in the\n",
      "     |          model, where classes are ordered as they are in ``self.classes_``.\n",
      "     |  \n",
      "     |  predict_proba(self, X)\n",
      "     |      Probability estimates.\n",
      "     |      \n",
      "     |      The returned estimates for all classes are ordered by the\n",
      "     |      label of classes.\n",
      "     |      \n",
      "     |      For a multi_class problem, if multi_class is set to be \"multinomial\"\n",
      "     |      the softmax function is used to find the predicted probability of\n",
      "     |      each class.\n",
      "     |      Else use a one-vs-rest approach, i.e calculate the probability\n",
      "     |      of each class assuming it to be positive using the logistic function.\n",
      "     |      and normalize these values across all the classes.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          Vector to be scored, where `n_samples` is the number of samples and\n",
      "     |          `n_features` is the number of features.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      T : array-like of shape (n_samples, n_classes)\n",
      "     |          Returns the probability of the sample for each class in the model,\n",
      "     |          where classes are ordered as they are in ``self.classes_``.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.linear_model._base.LinearClassifierMixin:\n",
      "     |  \n",
      "     |  decision_function(self, X)\n",
      "     |      Predict confidence scores for samples.\n",
      "     |      \n",
      "     |      The confidence score for a sample is proportional to the signed\n",
      "     |      distance of that sample to the hyperplane.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like or sparse matrix, shape (n_samples, n_features)\n",
      "     |          Samples.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      array, shape=(n_samples,) if n_classes == 2 else (n_samples, n_classes)\n",
      "     |          Confidence scores per (sample, class) combination. In the binary\n",
      "     |          case, confidence score for self.classes_[1] where >0 means this\n",
      "     |          class would be predicted.\n",
      "     |  \n",
      "     |  predict(self, X)\n",
      "     |      Predict class labels for samples in X.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like or sparse matrix, shape (n_samples, n_features)\n",
      "     |          Samples.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      C : array, shape [n_samples]\n",
      "     |          Predicted class label per sample.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.base.ClassifierMixin:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.linear_model._base.SparseCoefMixin:\n",
      "     |  \n",
      "     |  densify(self)\n",
      "     |      Convert coefficient matrix to dense array format.\n",
      "     |      \n",
      "     |      Converts the ``coef_`` member (back) to a numpy.ndarray. This is the\n",
      "     |      default format of ``coef_`` and is required for fitting, so calling\n",
      "     |      this method is only required on models that have previously been\n",
      "     |      sparsified; otherwise, it is a no-op.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self\n",
      "     |          Fitted estimator.\n",
      "     |  \n",
      "     |  sparsify(self)\n",
      "     |      Convert coefficient matrix to sparse format.\n",
      "     |      \n",
      "     |      Converts the ``coef_`` member to a scipy.sparse matrix, which for\n",
      "     |      L1-regularized models can be much more memory- and storage-efficient\n",
      "     |      than the usual numpy.ndarray representation.\n",
      "     |      \n",
      "     |      The ``intercept_`` member is not converted.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self\n",
      "     |          Fitted estimator.\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      For non-sparse models, i.e. when there are not many zeros in ``coef_``,\n",
      "     |      this may actually *increase* memory usage, so use this method with\n",
      "     |      care. A rule of thumb is that the number of zero elements, which can\n",
      "     |      be computed with ``(coef_ == 0).sum()``, must be more than 50% for this\n",
      "     |      to provide significant benefits.\n",
      "     |      \n",
      "     |      After calling this method, further fitting with the partial_fit\n",
      "     |      method (if any) will not work until you call densify.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |  \n",
      "     |  __repr__(self, N_CHAR_MAX=700)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  get_params(self, deep=True)\n",
      "     |      Get parameters for this estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      deep : bool, default=True\n",
      "     |          If True, will return the parameters for this estimator and\n",
      "     |          contained subobjects that are estimators.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      params : dict\n",
      "     |          Parameter names mapped to their values.\n",
      "     |  \n",
      "     |  set_params(self, **params)\n",
      "     |      Set the parameters of this estimator.\n",
      "     |      \n",
      "     |      The method works on simple estimators as well as on nested objects\n",
      "     |      (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n",
      "     |      parameters of the form ``<component>__<parameter>`` so that it's\n",
      "     |      possible to update each component of a nested object.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      **params : dict\n",
      "     |          Estimator parameters.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : estimator instance\n",
      "     |          Estimator instance.\n",
      "    \n",
      "    class ModifiedHuber(Classification)\n",
      "     |  Modified Huber loss for binary classification with y in {-1, 1}\n",
      "     |  \n",
      "     |  This is equivalent to quadratically smoothed SVM with gamma = 2.\n",
      "     |  \n",
      "     |  See T. Zhang 'Solving Large Scale Linear Prediction Problems Using\n",
      "     |  Stochastic Gradient Descent', ICML'04.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      ModifiedHuber\n",
      "     |      Classification\n",
      "     |      LossFunction\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __reduce__(...)\n",
      "     |      Helper for pickle.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods defined here:\n",
      "     |  \n",
      "     |  __new__(*args, **kwargs) from builtins.type\n",
      "     |      Create and return a new object.  See help(type) for accurate signature.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __pyx_vtable__ = <capsule object NULL>\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from Classification:\n",
      "     |  \n",
      "     |  __setstate__ = __setstate_cython__(...)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from LossFunction:\n",
      "     |  \n",
      "     |  py_dloss(...)\n",
      "     |      Python version of `dloss` for testing.\n",
      "     |      \n",
      "     |      Pytest needs a python function and can't use cdef functions.\n",
      "    \n",
      "    class MultiTaskElasticNet(Lasso)\n",
      "     |  MultiTaskElasticNet(alpha=1.0, *, l1_ratio=0.5, fit_intercept=True, normalize=False, copy_X=True, max_iter=1000, tol=0.0001, warm_start=False, random_state=None, selection='cyclic')\n",
      "     |  \n",
      "     |  Multi-task ElasticNet model trained with L1/L2 mixed-norm as\n",
      "     |  regularizer.\n",
      "     |  \n",
      "     |  The optimization objective for MultiTaskElasticNet is::\n",
      "     |  \n",
      "     |      (1 / (2 * n_samples)) * ||Y - XW||_Fro^2\n",
      "     |      + alpha * l1_ratio * ||W||_21\n",
      "     |      + 0.5 * alpha * (1 - l1_ratio) * ||W||_Fro^2\n",
      "     |  \n",
      "     |  Where::\n",
      "     |  \n",
      "     |      ||W||_21 = sum_i sqrt(sum_j W_ij ^ 2)\n",
      "     |  \n",
      "     |  i.e. the sum of norms of each row.\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <multi_task_elastic_net>`.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  alpha : float, default=1.0\n",
      "     |      Constant that multiplies the L1/L2 term. Defaults to 1.0.\n",
      "     |  \n",
      "     |  l1_ratio : float, default=0.5\n",
      "     |      The ElasticNet mixing parameter, with 0 < l1_ratio <= 1.\n",
      "     |      For l1_ratio = 1 the penalty is an L1/L2 penalty. For l1_ratio = 0 it\n",
      "     |      is an L2 penalty.\n",
      "     |      For ``0 < l1_ratio < 1``, the penalty is a combination of L1/L2 and L2.\n",
      "     |  \n",
      "     |  fit_intercept : bool, default=True\n",
      "     |      Whether to calculate the intercept for this model. If set\n",
      "     |      to false, no intercept will be used in calculations\n",
      "     |      (i.e. data is expected to be centered).\n",
      "     |  \n",
      "     |  normalize : bool, default=False\n",
      "     |      This parameter is ignored when ``fit_intercept`` is set to False.\n",
      "     |      If True, the regressors X will be normalized before regression by\n",
      "     |      subtracting the mean and dividing by the l2-norm.\n",
      "     |      If you wish to standardize, please use\n",
      "     |      :class:`~sklearn.preprocessing.StandardScaler` before calling ``fit``\n",
      "     |      on an estimator with ``normalize=False``.\n",
      "     |  \n",
      "     |  copy_X : bool, default=True\n",
      "     |      If ``True``, X will be copied; else, it may be overwritten.\n",
      "     |  \n",
      "     |  max_iter : int, default=1000\n",
      "     |      The maximum number of iterations.\n",
      "     |  \n",
      "     |  tol : float, default=1e-4\n",
      "     |      The tolerance for the optimization: if the updates are\n",
      "     |      smaller than ``tol``, the optimization code checks the\n",
      "     |      dual gap for optimality and continues until it is smaller\n",
      "     |      than ``tol``.\n",
      "     |  \n",
      "     |  warm_start : bool, default=False\n",
      "     |      When set to ``True``, reuse the solution of the previous call to fit as\n",
      "     |      initialization, otherwise, just erase the previous solution.\n",
      "     |      See :term:`the Glossary <warm_start>`.\n",
      "     |  \n",
      "     |  random_state : int, RandomState instance, default=None\n",
      "     |      The seed of the pseudo random number generator that selects a random\n",
      "     |      feature to update. Used when ``selection`` == 'random'.\n",
      "     |      Pass an int for reproducible output across multiple function calls.\n",
      "     |      See :term:`Glossary <random_state>`.\n",
      "     |  \n",
      "     |  selection : {'cyclic', 'random'}, default='cyclic'\n",
      "     |      If set to 'random', a random coefficient is updated every iteration\n",
      "     |      rather than looping over features sequentially by default. This\n",
      "     |      (setting to 'random') often leads to significantly faster convergence\n",
      "     |      especially when tol is higher than 1e-4.\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  intercept_ : ndarray of shape (n_tasks,)\n",
      "     |      Independent term in decision function.\n",
      "     |  \n",
      "     |  coef_ : ndarray of shape (n_tasks, n_features)\n",
      "     |      Parameter vector (W in the cost function formula). If a 1D y is\n",
      "     |      passed in at fit (non multi-task usage), ``coef_`` is then a 1D array.\n",
      "     |      Note that ``coef_`` stores the transpose of ``W``, ``W.T``.\n",
      "     |  \n",
      "     |  n_iter_ : int\n",
      "     |      Number of iterations run by the coordinate descent solver to reach\n",
      "     |      the specified tolerance.\n",
      "     |  \n",
      "     |  dual_gap_ : float\n",
      "     |      The dual gaps at the end of the optimization.\n",
      "     |  \n",
      "     |  eps_ : float\n",
      "     |      The tolerance scaled scaled by the variance of the target `y`.\n",
      "     |  \n",
      "     |  sparse_coef_ : sparse matrix of shape (n_features,) or             (n_tasks, n_features)\n",
      "     |      Sparse representation of the `coef_`.\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  >>> from sklearn import linear_model\n",
      "     |  >>> clf = linear_model.MultiTaskElasticNet(alpha=0.1)\n",
      "     |  >>> clf.fit([[0,0], [1, 1], [2, 2]], [[0, 0], [1, 1], [2, 2]])\n",
      "     |  MultiTaskElasticNet(alpha=0.1)\n",
      "     |  >>> print(clf.coef_)\n",
      "     |  [[0.45663524 0.45612256]\n",
      "     |   [0.45663524 0.45612256]]\n",
      "     |  >>> print(clf.intercept_)\n",
      "     |  [0.0872422 0.0872422]\n",
      "     |  \n",
      "     |  See Also\n",
      "     |  --------\n",
      "     |  MultiTaskElasticNetCV : Multi-task L1/L2 ElasticNet with built-in\n",
      "     |      cross-validation.\n",
      "     |  ElasticNet\n",
      "     |  MultiTaskLasso\n",
      "     |  \n",
      "     |  Notes\n",
      "     |  -----\n",
      "     |  The algorithm used to fit the model is coordinate descent.\n",
      "     |  \n",
      "     |  To avoid unnecessary memory duplication the X and y arguments of the fit\n",
      "     |  method should be directly passed as Fortran-contiguous numpy arrays.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      MultiTaskElasticNet\n",
      "     |      Lasso\n",
      "     |      ElasticNet\n",
      "     |      sklearn.base.MultiOutputMixin\n",
      "     |      sklearn.base.RegressorMixin\n",
      "     |      sklearn.linear_model._base.LinearModel\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, alpha=1.0, *, l1_ratio=0.5, fit_intercept=True, normalize=False, copy_X=True, max_iter=1000, tol=0.0001, warm_start=False, random_state=None, selection='cyclic')\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  fit(self, X, y)\n",
      "     |      Fit MultiTaskElasticNet model with coordinate descent\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : ndarray of shape (n_samples, n_features)\n",
      "     |          Data.\n",
      "     |      y : ndarray of shape (n_samples, n_tasks)\n",
      "     |          Target. Will be cast to X's dtype if necessary.\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      \n",
      "     |      Coordinate descent is an algorithm that considers each column of\n",
      "     |      data at a time hence it will automatically convert the X input\n",
      "     |      as a Fortran-contiguous numpy array if necessary.\n",
      "     |      \n",
      "     |      To avoid memory re-allocation it is advised to allocate the\n",
      "     |      initial data in memory directly using that format.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods inherited from Lasso:\n",
      "     |  \n",
      "     |  path = enet_path(X, y, *, l1_ratio=0.5, eps=0.001, n_alphas=100, alphas=None, precompute='auto', Xy=None, copy_X=True, coef_init=None, verbose=False, return_n_iter=False, positive=False, check_input=True, **params)\n",
      "     |      Compute elastic net path with coordinate descent.\n",
      "     |      \n",
      "     |      The elastic net optimization function varies for mono and multi-outputs.\n",
      "     |      \n",
      "     |      For mono-output tasks it is::\n",
      "     |      \n",
      "     |          1 / (2 * n_samples) * ||y - Xw||^2_2\n",
      "     |          + alpha * l1_ratio * ||w||_1\n",
      "     |          + 0.5 * alpha * (1 - l1_ratio) * ||w||^2_2\n",
      "     |      \n",
      "     |      For multi-output tasks it is::\n",
      "     |      \n",
      "     |          (1 / (2 * n_samples)) * ||Y - XW||_Fro^2\n",
      "     |          + alpha * l1_ratio * ||W||_21\n",
      "     |          + 0.5 * alpha * (1 - l1_ratio) * ||W||_Fro^2\n",
      "     |      \n",
      "     |      Where::\n",
      "     |      \n",
      "     |          ||W||_21 = \\sum_i \\sqrt{\\sum_j w_{ij}^2}\n",
      "     |      \n",
      "     |      i.e. the sum of norm of each row.\n",
      "     |      \n",
      "     |      Read more in the :ref:`User Guide <elastic_net>`.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          Training data. Pass directly as Fortran-contiguous data to avoid\n",
      "     |          unnecessary memory duplication. If ``y`` is mono-output then ``X``\n",
      "     |          can be sparse.\n",
      "     |      \n",
      "     |      y : {array-like, sparse matrix} of shape (n_samples,) or         (n_samples, n_outputs)\n",
      "     |          Target values.\n",
      "     |      \n",
      "     |      l1_ratio : float, default=0.5\n",
      "     |          Number between 0 and 1 passed to elastic net (scaling between\n",
      "     |          l1 and l2 penalties). ``l1_ratio=1`` corresponds to the Lasso.\n",
      "     |      \n",
      "     |      eps : float, default=1e-3\n",
      "     |          Length of the path. ``eps=1e-3`` means that\n",
      "     |          ``alpha_min / alpha_max = 1e-3``.\n",
      "     |      \n",
      "     |      n_alphas : int, default=100\n",
      "     |          Number of alphas along the regularization path.\n",
      "     |      \n",
      "     |      alphas : ndarray, default=None\n",
      "     |          List of alphas where to compute the models.\n",
      "     |          If None alphas are set automatically.\n",
      "     |      \n",
      "     |      precompute : 'auto', bool or array-like of shape (n_features, n_features),                 default='auto'\n",
      "     |          Whether to use a precomputed Gram matrix to speed up\n",
      "     |          calculations. If set to ``'auto'`` let us decide. The Gram\n",
      "     |          matrix can also be passed as argument.\n",
      "     |      \n",
      "     |      Xy : array-like of shape (n_features,) or (n_features, n_outputs),         default=None\n",
      "     |          Xy = np.dot(X.T, y) that can be precomputed. It is useful\n",
      "     |          only when the Gram matrix is precomputed.\n",
      "     |      \n",
      "     |      copy_X : bool, default=True\n",
      "     |          If ``True``, X will be copied; else, it may be overwritten.\n",
      "     |      \n",
      "     |      coef_init : ndarray of shape (n_features, ), default=None\n",
      "     |          The initial values of the coefficients.\n",
      "     |      \n",
      "     |      verbose : bool or int, default=False\n",
      "     |          Amount of verbosity.\n",
      "     |      \n",
      "     |      return_n_iter : bool, default=False\n",
      "     |          Whether to return the number of iterations or not.\n",
      "     |      \n",
      "     |      positive : bool, default=False\n",
      "     |          If set to True, forces coefficients to be positive.\n",
      "     |          (Only allowed when ``y.ndim == 1``).\n",
      "     |      \n",
      "     |      check_input : bool, default=True\n",
      "     |          If set to False, the input validation checks are skipped (including the\n",
      "     |          Gram matrix when provided). It is assumed that they are handled\n",
      "     |          by the caller.\n",
      "     |      \n",
      "     |      **params : kwargs\n",
      "     |          Keyword arguments passed to the coordinate descent solver.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      alphas : ndarray of shape (n_alphas,)\n",
      "     |          The alphas along the path where models are computed.\n",
      "     |      \n",
      "     |      coefs : ndarray of shape (n_features, n_alphas) or             (n_outputs, n_features, n_alphas)\n",
      "     |          Coefficients along the path.\n",
      "     |      \n",
      "     |      dual_gaps : ndarray of shape (n_alphas,)\n",
      "     |          The dual gaps at the end of the optimization for each alpha.\n",
      "     |      \n",
      "     |      n_iters : list of int\n",
      "     |          The number of iterations taken by the coordinate descent optimizer to\n",
      "     |          reach the specified tolerance for each alpha.\n",
      "     |          (Is returned when ``return_n_iter`` is set to True).\n",
      "     |      \n",
      "     |      See Also\n",
      "     |      --------\n",
      "     |      MultiTaskElasticNet\n",
      "     |      MultiTaskElasticNetCV\n",
      "     |      ElasticNet\n",
      "     |      ElasticNetCV\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      For an example, see\n",
      "     |      :ref:`examples/linear_model/plot_lasso_coordinate_descent_path.py\n",
      "     |      <sphx_glr_auto_examples_linear_model_plot_lasso_coordinate_descent_path.py>`.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties inherited from ElasticNet:\n",
      "     |  \n",
      "     |  sparse_coef_\n",
      "     |      Sparse representation of the fitted `coef_`.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.base.MultiOutputMixin:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.RegressorMixin:\n",
      "     |  \n",
      "     |  score(self, X, y, sample_weight=None)\n",
      "     |      Return the coefficient of determination :math:`R^2` of the\n",
      "     |      prediction.\n",
      "     |      \n",
      "     |      The coefficient :math:`R^2` is defined as :math:`(1 - \\frac{u}{v})`,\n",
      "     |      where :math:`u` is the residual sum of squares ``((y_true - y_pred)\n",
      "     |      ** 2).sum()`` and :math:`v` is the total sum of squares ``((y_true -\n",
      "     |      y_true.mean()) ** 2).sum()``. The best possible score is 1.0 and it\n",
      "     |      can be negative (because the model can be arbitrarily worse). A\n",
      "     |      constant model that always predicts the expected value of `y`,\n",
      "     |      disregarding the input features, would get a :math:`R^2` score of\n",
      "     |      0.0.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          Test samples. For some estimators this may be a precomputed\n",
      "     |          kernel matrix or a list of generic objects instead with shape\n",
      "     |          ``(n_samples, n_samples_fitted)``, where ``n_samples_fitted``\n",
      "     |          is the number of samples used in the fitting for the estimator.\n",
      "     |      \n",
      "     |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      "     |          True values for `X`.\n",
      "     |      \n",
      "     |      sample_weight : array-like of shape (n_samples,), default=None\n",
      "     |          Sample weights.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      score : float\n",
      "     |          :math:`R^2` of ``self.predict(X)`` wrt. `y`.\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      The :math:`R^2` score used when calling ``score`` on a regressor uses\n",
      "     |      ``multioutput='uniform_average'`` from version 0.23 to keep consistent\n",
      "     |      with default value of :func:`~sklearn.metrics.r2_score`.\n",
      "     |      This influences the ``score`` method of all the multioutput\n",
      "     |      regressors (except for\n",
      "     |      :class:`~sklearn.multioutput.MultiOutputRegressor`).\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.linear_model._base.LinearModel:\n",
      "     |  \n",
      "     |  predict(self, X)\n",
      "     |      Predict using the linear model.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like or sparse matrix, shape (n_samples, n_features)\n",
      "     |          Samples.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      C : array, shape (n_samples,)\n",
      "     |          Returns predicted values.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |  \n",
      "     |  __repr__(self, N_CHAR_MAX=700)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  get_params(self, deep=True)\n",
      "     |      Get parameters for this estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      deep : bool, default=True\n",
      "     |          If True, will return the parameters for this estimator and\n",
      "     |          contained subobjects that are estimators.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      params : dict\n",
      "     |          Parameter names mapped to their values.\n",
      "     |  \n",
      "     |  set_params(self, **params)\n",
      "     |      Set the parameters of this estimator.\n",
      "     |      \n",
      "     |      The method works on simple estimators as well as on nested objects\n",
      "     |      (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n",
      "     |      parameters of the form ``<component>__<parameter>`` so that it's\n",
      "     |      possible to update each component of a nested object.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      **params : dict\n",
      "     |          Estimator parameters.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : estimator instance\n",
      "     |          Estimator instance.\n",
      "    \n",
      "    class MultiTaskElasticNetCV(sklearn.base.RegressorMixin, LinearModelCV)\n",
      "     |  MultiTaskElasticNetCV(*, l1_ratio=0.5, eps=0.001, n_alphas=100, alphas=None, fit_intercept=True, normalize=False, max_iter=1000, tol=0.0001, cv=None, copy_X=True, verbose=0, n_jobs=None, random_state=None, selection='cyclic')\n",
      "     |  \n",
      "     |  Multi-task L1/L2 ElasticNet with built-in cross-validation.\n",
      "     |  \n",
      "     |  See glossary entry for :term:`cross-validation estimator`.\n",
      "     |  \n",
      "     |  The optimization objective for MultiTaskElasticNet is::\n",
      "     |  \n",
      "     |      (1 / (2 * n_samples)) * ||Y - XW||^Fro_2\n",
      "     |      + alpha * l1_ratio * ||W||_21\n",
      "     |      + 0.5 * alpha * (1 - l1_ratio) * ||W||_Fro^2\n",
      "     |  \n",
      "     |  Where::\n",
      "     |  \n",
      "     |      ||W||_21 = \\sum_i \\sqrt{\\sum_j w_{ij}^2}\n",
      "     |  \n",
      "     |  i.e. the sum of norm of each row.\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <multi_task_elastic_net>`.\n",
      "     |  \n",
      "     |  .. versionadded:: 0.15\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  l1_ratio : float or list of float, default=0.5\n",
      "     |      The ElasticNet mixing parameter, with 0 < l1_ratio <= 1.\n",
      "     |      For l1_ratio = 1 the penalty is an L1/L2 penalty. For l1_ratio = 0 it\n",
      "     |      is an L2 penalty.\n",
      "     |      For ``0 < l1_ratio < 1``, the penalty is a combination of L1/L2 and L2.\n",
      "     |      This parameter can be a list, in which case the different\n",
      "     |      values are tested by cross-validation and the one giving the best\n",
      "     |      prediction score is used. Note that a good choice of list of\n",
      "     |      values for l1_ratio is often to put more values close to 1\n",
      "     |      (i.e. Lasso) and less close to 0 (i.e. Ridge), as in ``[.1, .5, .7,\n",
      "     |      .9, .95, .99, 1]``\n",
      "     |  \n",
      "     |  eps : float, default=1e-3\n",
      "     |      Length of the path. ``eps=1e-3`` means that\n",
      "     |      ``alpha_min / alpha_max = 1e-3``.\n",
      "     |  \n",
      "     |  n_alphas : int, default=100\n",
      "     |      Number of alphas along the regularization path.\n",
      "     |  \n",
      "     |  alphas : array-like, default=None\n",
      "     |      List of alphas where to compute the models.\n",
      "     |      If not provided, set automatically.\n",
      "     |  \n",
      "     |  fit_intercept : bool, default=True\n",
      "     |      Whether to calculate the intercept for this model. If set\n",
      "     |      to false, no intercept will be used in calculations\n",
      "     |      (i.e. data is expected to be centered).\n",
      "     |  \n",
      "     |  normalize : bool, default=False\n",
      "     |      This parameter is ignored when ``fit_intercept`` is set to False.\n",
      "     |      If True, the regressors X will be normalized before regression by\n",
      "     |      subtracting the mean and dividing by the l2-norm.\n",
      "     |      If you wish to standardize, please use\n",
      "     |      :class:`~sklearn.preprocessing.StandardScaler` before calling ``fit``\n",
      "     |      on an estimator with ``normalize=False``.\n",
      "     |  \n",
      "     |  max_iter : int, default=1000\n",
      "     |      The maximum number of iterations.\n",
      "     |  \n",
      "     |  tol : float, default=1e-4\n",
      "     |      The tolerance for the optimization: if the updates are\n",
      "     |      smaller than ``tol``, the optimization code checks the\n",
      "     |      dual gap for optimality and continues until it is smaller\n",
      "     |      than ``tol``.\n",
      "     |  \n",
      "     |  cv : int, cross-validation generator or iterable, default=None\n",
      "     |      Determines the cross-validation splitting strategy.\n",
      "     |      Possible inputs for cv are:\n",
      "     |  \n",
      "     |      - None, to use the default 5-fold cross-validation,\n",
      "     |      - int, to specify the number of folds.\n",
      "     |      - :term:`CV splitter`,\n",
      "     |      - An iterable yielding (train, test) splits as arrays of indices.\n",
      "     |  \n",
      "     |      For int/None inputs, :class:`KFold` is used.\n",
      "     |  \n",
      "     |      Refer :ref:`User Guide <cross_validation>` for the various\n",
      "     |      cross-validation strategies that can be used here.\n",
      "     |  \n",
      "     |      .. versionchanged:: 0.22\n",
      "     |          ``cv`` default value if None changed from 3-fold to 5-fold.\n",
      "     |  \n",
      "     |  copy_X : bool, default=True\n",
      "     |      If ``True``, X will be copied; else, it may be overwritten.\n",
      "     |  \n",
      "     |  verbose : bool or int, default=0\n",
      "     |      Amount of verbosity.\n",
      "     |  \n",
      "     |  n_jobs : int, default=None\n",
      "     |      Number of CPUs to use during the cross validation. Note that this is\n",
      "     |      used only if multiple values for l1_ratio are given.\n",
      "     |      ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n",
      "     |      ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n",
      "     |      for more details.\n",
      "     |  \n",
      "     |  random_state : int, RandomState instance, default=None\n",
      "     |      The seed of the pseudo random number generator that selects a random\n",
      "     |      feature to update. Used when ``selection`` == 'random'.\n",
      "     |      Pass an int for reproducible output across multiple function calls.\n",
      "     |      See :term:`Glossary <random_state>`.\n",
      "     |  \n",
      "     |  selection : {'cyclic', 'random'}, default='cyclic'\n",
      "     |      If set to 'random', a random coefficient is updated every iteration\n",
      "     |      rather than looping over features sequentially by default. This\n",
      "     |      (setting to 'random') often leads to significantly faster convergence\n",
      "     |      especially when tol is higher than 1e-4.\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  intercept_ : ndarray of shape (n_tasks,)\n",
      "     |      Independent term in decision function.\n",
      "     |  \n",
      "     |  coef_ : ndarray of shape (n_tasks, n_features)\n",
      "     |      Parameter vector (W in the cost function formula).\n",
      "     |      Note that ``coef_`` stores the transpose of ``W``, ``W.T``.\n",
      "     |  \n",
      "     |  alpha_ : float\n",
      "     |      The amount of penalization chosen by cross validation.\n",
      "     |  \n",
      "     |  mse_path_ : ndarray of shape (n_alphas, n_folds) or                 (n_l1_ratio, n_alphas, n_folds)\n",
      "     |      Mean square error for the test set on each fold, varying alpha.\n",
      "     |  \n",
      "     |  alphas_ : ndarray of shape (n_alphas,) or (n_l1_ratio, n_alphas)\n",
      "     |      The grid of alphas used for fitting, for each l1_ratio.\n",
      "     |  \n",
      "     |  l1_ratio_ : float\n",
      "     |      Best l1_ratio obtained by cross-validation.\n",
      "     |  \n",
      "     |  n_iter_ : int\n",
      "     |      Number of iterations run by the coordinate descent solver to reach\n",
      "     |      the specified tolerance for the optimal alpha.\n",
      "     |  \n",
      "     |  dual_gap_ : float\n",
      "     |      The dual gap at the end of the optimization for the optimal alpha.\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  >>> from sklearn import linear_model\n",
      "     |  >>> clf = linear_model.MultiTaskElasticNetCV(cv=3)\n",
      "     |  >>> clf.fit([[0,0], [1, 1], [2, 2]],\n",
      "     |  ...         [[0, 0], [1, 1], [2, 2]])\n",
      "     |  MultiTaskElasticNetCV(cv=3)\n",
      "     |  >>> print(clf.coef_)\n",
      "     |  [[0.52875032 0.46958558]\n",
      "     |   [0.52875032 0.46958558]]\n",
      "     |  >>> print(clf.intercept_)\n",
      "     |  [0.00166409 0.00166409]\n",
      "     |  \n",
      "     |  See Also\n",
      "     |  --------\n",
      "     |  MultiTaskElasticNet\n",
      "     |  ElasticNetCV\n",
      "     |  MultiTaskLassoCV\n",
      "     |  \n",
      "     |  Notes\n",
      "     |  -----\n",
      "     |  The algorithm used to fit the model is coordinate descent.\n",
      "     |  \n",
      "     |  To avoid unnecessary memory duplication the X and y arguments of the fit\n",
      "     |  method should be directly passed as Fortran-contiguous numpy arrays.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      MultiTaskElasticNetCV\n",
      "     |      sklearn.base.RegressorMixin\n",
      "     |      LinearModelCV\n",
      "     |      sklearn.base.MultiOutputMixin\n",
      "     |      sklearn.linear_model._base.LinearModel\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, *, l1_ratio=0.5, eps=0.001, n_alphas=100, alphas=None, fit_intercept=True, normalize=False, max_iter=1000, tol=0.0001, cv=None, copy_X=True, verbose=0, n_jobs=None, random_state=None, selection='cyclic')\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods defined here:\n",
      "     |  \n",
      "     |  path = enet_path(X, y, *, l1_ratio=0.5, eps=0.001, n_alphas=100, alphas=None, precompute='auto', Xy=None, copy_X=True, coef_init=None, verbose=False, return_n_iter=False, positive=False, check_input=True, **params)\n",
      "     |      Compute elastic net path with coordinate descent.\n",
      "     |      \n",
      "     |      The elastic net optimization function varies for mono and multi-outputs.\n",
      "     |      \n",
      "     |      For mono-output tasks it is::\n",
      "     |      \n",
      "     |          1 / (2 * n_samples) * ||y - Xw||^2_2\n",
      "     |          + alpha * l1_ratio * ||w||_1\n",
      "     |          + 0.5 * alpha * (1 - l1_ratio) * ||w||^2_2\n",
      "     |      \n",
      "     |      For multi-output tasks it is::\n",
      "     |      \n",
      "     |          (1 / (2 * n_samples)) * ||Y - XW||_Fro^2\n",
      "     |          + alpha * l1_ratio * ||W||_21\n",
      "     |          + 0.5 * alpha * (1 - l1_ratio) * ||W||_Fro^2\n",
      "     |      \n",
      "     |      Where::\n",
      "     |      \n",
      "     |          ||W||_21 = \\sum_i \\sqrt{\\sum_j w_{ij}^2}\n",
      "     |      \n",
      "     |      i.e. the sum of norm of each row.\n",
      "     |      \n",
      "     |      Read more in the :ref:`User Guide <elastic_net>`.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          Training data. Pass directly as Fortran-contiguous data to avoid\n",
      "     |          unnecessary memory duplication. If ``y`` is mono-output then ``X``\n",
      "     |          can be sparse.\n",
      "     |      \n",
      "     |      y : {array-like, sparse matrix} of shape (n_samples,) or         (n_samples, n_outputs)\n",
      "     |          Target values.\n",
      "     |      \n",
      "     |      l1_ratio : float, default=0.5\n",
      "     |          Number between 0 and 1 passed to elastic net (scaling between\n",
      "     |          l1 and l2 penalties). ``l1_ratio=1`` corresponds to the Lasso.\n",
      "     |      \n",
      "     |      eps : float, default=1e-3\n",
      "     |          Length of the path. ``eps=1e-3`` means that\n",
      "     |          ``alpha_min / alpha_max = 1e-3``.\n",
      "     |      \n",
      "     |      n_alphas : int, default=100\n",
      "     |          Number of alphas along the regularization path.\n",
      "     |      \n",
      "     |      alphas : ndarray, default=None\n",
      "     |          List of alphas where to compute the models.\n",
      "     |          If None alphas are set automatically.\n",
      "     |      \n",
      "     |      precompute : 'auto', bool or array-like of shape (n_features, n_features),                 default='auto'\n",
      "     |          Whether to use a precomputed Gram matrix to speed up\n",
      "     |          calculations. If set to ``'auto'`` let us decide. The Gram\n",
      "     |          matrix can also be passed as argument.\n",
      "     |      \n",
      "     |      Xy : array-like of shape (n_features,) or (n_features, n_outputs),         default=None\n",
      "     |          Xy = np.dot(X.T, y) that can be precomputed. It is useful\n",
      "     |          only when the Gram matrix is precomputed.\n",
      "     |      \n",
      "     |      copy_X : bool, default=True\n",
      "     |          If ``True``, X will be copied; else, it may be overwritten.\n",
      "     |      \n",
      "     |      coef_init : ndarray of shape (n_features, ), default=None\n",
      "     |          The initial values of the coefficients.\n",
      "     |      \n",
      "     |      verbose : bool or int, default=False\n",
      "     |          Amount of verbosity.\n",
      "     |      \n",
      "     |      return_n_iter : bool, default=False\n",
      "     |          Whether to return the number of iterations or not.\n",
      "     |      \n",
      "     |      positive : bool, default=False\n",
      "     |          If set to True, forces coefficients to be positive.\n",
      "     |          (Only allowed when ``y.ndim == 1``).\n",
      "     |      \n",
      "     |      check_input : bool, default=True\n",
      "     |          If set to False, the input validation checks are skipped (including the\n",
      "     |          Gram matrix when provided). It is assumed that they are handled\n",
      "     |          by the caller.\n",
      "     |      \n",
      "     |      **params : kwargs\n",
      "     |          Keyword arguments passed to the coordinate descent solver.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      alphas : ndarray of shape (n_alphas,)\n",
      "     |          The alphas along the path where models are computed.\n",
      "     |      \n",
      "     |      coefs : ndarray of shape (n_features, n_alphas) or             (n_outputs, n_features, n_alphas)\n",
      "     |          Coefficients along the path.\n",
      "     |      \n",
      "     |      dual_gaps : ndarray of shape (n_alphas,)\n",
      "     |          The dual gaps at the end of the optimization for each alpha.\n",
      "     |      \n",
      "     |      n_iters : list of int\n",
      "     |          The number of iterations taken by the coordinate descent optimizer to\n",
      "     |          reach the specified tolerance for each alpha.\n",
      "     |          (Is returned when ``return_n_iter`` is set to True).\n",
      "     |      \n",
      "     |      See Also\n",
      "     |      --------\n",
      "     |      MultiTaskElasticNet\n",
      "     |      MultiTaskElasticNetCV\n",
      "     |      ElasticNet\n",
      "     |      ElasticNetCV\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      For an example, see\n",
      "     |      :ref:`examples/linear_model/plot_lasso_coordinate_descent_path.py\n",
      "     |      <sphx_glr_auto_examples_linear_model_plot_lasso_coordinate_descent_path.py>`.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.RegressorMixin:\n",
      "     |  \n",
      "     |  score(self, X, y, sample_weight=None)\n",
      "     |      Return the coefficient of determination :math:`R^2` of the\n",
      "     |      prediction.\n",
      "     |      \n",
      "     |      The coefficient :math:`R^2` is defined as :math:`(1 - \\frac{u}{v})`,\n",
      "     |      where :math:`u` is the residual sum of squares ``((y_true - y_pred)\n",
      "     |      ** 2).sum()`` and :math:`v` is the total sum of squares ``((y_true -\n",
      "     |      y_true.mean()) ** 2).sum()``. The best possible score is 1.0 and it\n",
      "     |      can be negative (because the model can be arbitrarily worse). A\n",
      "     |      constant model that always predicts the expected value of `y`,\n",
      "     |      disregarding the input features, would get a :math:`R^2` score of\n",
      "     |      0.0.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          Test samples. For some estimators this may be a precomputed\n",
      "     |          kernel matrix or a list of generic objects instead with shape\n",
      "     |          ``(n_samples, n_samples_fitted)``, where ``n_samples_fitted``\n",
      "     |          is the number of samples used in the fitting for the estimator.\n",
      "     |      \n",
      "     |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      "     |          True values for `X`.\n",
      "     |      \n",
      "     |      sample_weight : array-like of shape (n_samples,), default=None\n",
      "     |          Sample weights.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      score : float\n",
      "     |          :math:`R^2` of ``self.predict(X)`` wrt. `y`.\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      The :math:`R^2` score used when calling ``score`` on a regressor uses\n",
      "     |      ``multioutput='uniform_average'`` from version 0.23 to keep consistent\n",
      "     |      with default value of :func:`~sklearn.metrics.r2_score`.\n",
      "     |      This influences the ``score`` method of all the multioutput\n",
      "     |      regressors (except for\n",
      "     |      :class:`~sklearn.multioutput.MultiOutputRegressor`).\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.base.RegressorMixin:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from LinearModelCV:\n",
      "     |  \n",
      "     |  fit(self, X, y)\n",
      "     |      Fit linear model with coordinate descent.\n",
      "     |      \n",
      "     |      Fit is on grid of alphas and best alpha estimated by cross-validation.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          Training data. Pass directly as Fortran-contiguous data\n",
      "     |          to avoid unnecessary memory duplication. If y is mono-output,\n",
      "     |          X can be sparse.\n",
      "     |      \n",
      "     |      y : array-like of shape (n_samples,) or (n_samples, n_targets)\n",
      "     |          Target values.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.linear_model._base.LinearModel:\n",
      "     |  \n",
      "     |  predict(self, X)\n",
      "     |      Predict using the linear model.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like or sparse matrix, shape (n_samples, n_features)\n",
      "     |          Samples.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      C : array, shape (n_samples,)\n",
      "     |          Returns predicted values.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |  \n",
      "     |  __repr__(self, N_CHAR_MAX=700)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  get_params(self, deep=True)\n",
      "     |      Get parameters for this estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      deep : bool, default=True\n",
      "     |          If True, will return the parameters for this estimator and\n",
      "     |          contained subobjects that are estimators.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      params : dict\n",
      "     |          Parameter names mapped to their values.\n",
      "     |  \n",
      "     |  set_params(self, **params)\n",
      "     |      Set the parameters of this estimator.\n",
      "     |      \n",
      "     |      The method works on simple estimators as well as on nested objects\n",
      "     |      (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n",
      "     |      parameters of the form ``<component>__<parameter>`` so that it's\n",
      "     |      possible to update each component of a nested object.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      **params : dict\n",
      "     |          Estimator parameters.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : estimator instance\n",
      "     |          Estimator instance.\n",
      "    \n",
      "    class MultiTaskLasso(MultiTaskElasticNet)\n",
      "     |  MultiTaskLasso(alpha=1.0, *, fit_intercept=True, normalize=False, copy_X=True, max_iter=1000, tol=0.0001, warm_start=False, random_state=None, selection='cyclic')\n",
      "     |  \n",
      "     |  Multi-task Lasso model trained with L1/L2 mixed-norm as regularizer.\n",
      "     |  \n",
      "     |  The optimization objective for Lasso is::\n",
      "     |  \n",
      "     |      (1 / (2 * n_samples)) * ||Y - XW||^2_Fro + alpha * ||W||_21\n",
      "     |  \n",
      "     |  Where::\n",
      "     |  \n",
      "     |      ||W||_21 = \\sum_i \\sqrt{\\sum_j w_{ij}^2}\n",
      "     |  \n",
      "     |  i.e. the sum of norm of each row.\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <multi_task_lasso>`.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  alpha : float, default=1.0\n",
      "     |      Constant that multiplies the L1/L2 term. Defaults to 1.0.\n",
      "     |  \n",
      "     |  fit_intercept : bool, default=True\n",
      "     |      Whether to calculate the intercept for this model. If set\n",
      "     |      to false, no intercept will be used in calculations\n",
      "     |      (i.e. data is expected to be centered).\n",
      "     |  \n",
      "     |  normalize : bool, default=False\n",
      "     |      This parameter is ignored when ``fit_intercept`` is set to False.\n",
      "     |      If True, the regressors X will be normalized before regression by\n",
      "     |      subtracting the mean and dividing by the l2-norm.\n",
      "     |      If you wish to standardize, please use\n",
      "     |      :class:`~sklearn.preprocessing.StandardScaler` before calling ``fit``\n",
      "     |      on an estimator with ``normalize=False``.\n",
      "     |  \n",
      "     |  copy_X : bool, default=True\n",
      "     |      If ``True``, X will be copied; else, it may be overwritten.\n",
      "     |  \n",
      "     |  max_iter : int, default=1000\n",
      "     |      The maximum number of iterations.\n",
      "     |  \n",
      "     |  tol : float, default=1e-4\n",
      "     |      The tolerance for the optimization: if the updates are\n",
      "     |      smaller than ``tol``, the optimization code checks the\n",
      "     |      dual gap for optimality and continues until it is smaller\n",
      "     |      than ``tol``.\n",
      "     |  \n",
      "     |  warm_start : bool, default=False\n",
      "     |      When set to ``True``, reuse the solution of the previous call to fit as\n",
      "     |      initialization, otherwise, just erase the previous solution.\n",
      "     |      See :term:`the Glossary <warm_start>`.\n",
      "     |  \n",
      "     |  random_state : int, RandomState instance, default=None\n",
      "     |      The seed of the pseudo random number generator that selects a random\n",
      "     |      feature to update. Used when ``selection`` == 'random'.\n",
      "     |      Pass an int for reproducible output across multiple function calls.\n",
      "     |      See :term:`Glossary <random_state>`.\n",
      "     |  \n",
      "     |  selection : {'cyclic', 'random'}, default='cyclic'\n",
      "     |      If set to 'random', a random coefficient is updated every iteration\n",
      "     |      rather than looping over features sequentially by default. This\n",
      "     |      (setting to 'random') often leads to significantly faster convergence\n",
      "     |      especially when tol is higher than 1e-4\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  coef_ : ndarray of shape (n_tasks, n_features)\n",
      "     |      Parameter vector (W in the cost function formula).\n",
      "     |      Note that ``coef_`` stores the transpose of ``W``, ``W.T``.\n",
      "     |  \n",
      "     |  intercept_ : ndarray of shape (n_tasks,)\n",
      "     |      Independent term in decision function.\n",
      "     |  \n",
      "     |  n_iter_ : int\n",
      "     |      Number of iterations run by the coordinate descent solver to reach\n",
      "     |      the specified tolerance.\n",
      "     |  \n",
      "     |  dual_gap_ : ndarray of shape (n_alphas,)\n",
      "     |      The dual gaps at the end of the optimization for each alpha.\n",
      "     |  \n",
      "     |  eps_ : float\n",
      "     |      The tolerance scaled scaled by the variance of the target `y`.\n",
      "     |  \n",
      "     |  sparse_coef_ : sparse matrix of shape (n_features,) or             (n_tasks, n_features)\n",
      "     |      Sparse representation of the `coef_`.\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  >>> from sklearn import linear_model\n",
      "     |  >>> clf = linear_model.MultiTaskLasso(alpha=0.1)\n",
      "     |  >>> clf.fit([[0, 1], [1, 2], [2, 4]], [[0, 0], [1, 1], [2, 3]])\n",
      "     |  MultiTaskLasso(alpha=0.1)\n",
      "     |  >>> print(clf.coef_)\n",
      "     |  [[0.         0.60809415]\n",
      "     |  [0.         0.94592424]]\n",
      "     |  >>> print(clf.intercept_)\n",
      "     |  [-0.41888636 -0.87382323]\n",
      "     |  \n",
      "     |  See Also\n",
      "     |  --------\n",
      "     |  MultiTaskLasso : Multi-task L1/L2 Lasso with built-in cross-validation.\n",
      "     |  Lasso\n",
      "     |  MultiTaskElasticNet\n",
      "     |  \n",
      "     |  Notes\n",
      "     |  -----\n",
      "     |  The algorithm used to fit the model is coordinate descent.\n",
      "     |  \n",
      "     |  To avoid unnecessary memory duplication the X and y arguments of the fit\n",
      "     |  method should be directly passed as Fortran-contiguous numpy arrays.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      MultiTaskLasso\n",
      "     |      MultiTaskElasticNet\n",
      "     |      Lasso\n",
      "     |      ElasticNet\n",
      "     |      sklearn.base.MultiOutputMixin\n",
      "     |      sklearn.base.RegressorMixin\n",
      "     |      sklearn.linear_model._base.LinearModel\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, alpha=1.0, *, fit_intercept=True, normalize=False, copy_X=True, max_iter=1000, tol=0.0001, warm_start=False, random_state=None, selection='cyclic')\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from MultiTaskElasticNet:\n",
      "     |  \n",
      "     |  fit(self, X, y)\n",
      "     |      Fit MultiTaskElasticNet model with coordinate descent\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : ndarray of shape (n_samples, n_features)\n",
      "     |          Data.\n",
      "     |      y : ndarray of shape (n_samples, n_tasks)\n",
      "     |          Target. Will be cast to X's dtype if necessary.\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      \n",
      "     |      Coordinate descent is an algorithm that considers each column of\n",
      "     |      data at a time hence it will automatically convert the X input\n",
      "     |      as a Fortran-contiguous numpy array if necessary.\n",
      "     |      \n",
      "     |      To avoid memory re-allocation it is advised to allocate the\n",
      "     |      initial data in memory directly using that format.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods inherited from Lasso:\n",
      "     |  \n",
      "     |  path = enet_path(X, y, *, l1_ratio=0.5, eps=0.001, n_alphas=100, alphas=None, precompute='auto', Xy=None, copy_X=True, coef_init=None, verbose=False, return_n_iter=False, positive=False, check_input=True, **params)\n",
      "     |      Compute elastic net path with coordinate descent.\n",
      "     |      \n",
      "     |      The elastic net optimization function varies for mono and multi-outputs.\n",
      "     |      \n",
      "     |      For mono-output tasks it is::\n",
      "     |      \n",
      "     |          1 / (2 * n_samples) * ||y - Xw||^2_2\n",
      "     |          + alpha * l1_ratio * ||w||_1\n",
      "     |          + 0.5 * alpha * (1 - l1_ratio) * ||w||^2_2\n",
      "     |      \n",
      "     |      For multi-output tasks it is::\n",
      "     |      \n",
      "     |          (1 / (2 * n_samples)) * ||Y - XW||_Fro^2\n",
      "     |          + alpha * l1_ratio * ||W||_21\n",
      "     |          + 0.5 * alpha * (1 - l1_ratio) * ||W||_Fro^2\n",
      "     |      \n",
      "     |      Where::\n",
      "     |      \n",
      "     |          ||W||_21 = \\sum_i \\sqrt{\\sum_j w_{ij}^2}\n",
      "     |      \n",
      "     |      i.e. the sum of norm of each row.\n",
      "     |      \n",
      "     |      Read more in the :ref:`User Guide <elastic_net>`.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          Training data. Pass directly as Fortran-contiguous data to avoid\n",
      "     |          unnecessary memory duplication. If ``y`` is mono-output then ``X``\n",
      "     |          can be sparse.\n",
      "     |      \n",
      "     |      y : {array-like, sparse matrix} of shape (n_samples,) or         (n_samples, n_outputs)\n",
      "     |          Target values.\n",
      "     |      \n",
      "     |      l1_ratio : float, default=0.5\n",
      "     |          Number between 0 and 1 passed to elastic net (scaling between\n",
      "     |          l1 and l2 penalties). ``l1_ratio=1`` corresponds to the Lasso.\n",
      "     |      \n",
      "     |      eps : float, default=1e-3\n",
      "     |          Length of the path. ``eps=1e-3`` means that\n",
      "     |          ``alpha_min / alpha_max = 1e-3``.\n",
      "     |      \n",
      "     |      n_alphas : int, default=100\n",
      "     |          Number of alphas along the regularization path.\n",
      "     |      \n",
      "     |      alphas : ndarray, default=None\n",
      "     |          List of alphas where to compute the models.\n",
      "     |          If None alphas are set automatically.\n",
      "     |      \n",
      "     |      precompute : 'auto', bool or array-like of shape (n_features, n_features),                 default='auto'\n",
      "     |          Whether to use a precomputed Gram matrix to speed up\n",
      "     |          calculations. If set to ``'auto'`` let us decide. The Gram\n",
      "     |          matrix can also be passed as argument.\n",
      "     |      \n",
      "     |      Xy : array-like of shape (n_features,) or (n_features, n_outputs),         default=None\n",
      "     |          Xy = np.dot(X.T, y) that can be precomputed. It is useful\n",
      "     |          only when the Gram matrix is precomputed.\n",
      "     |      \n",
      "     |      copy_X : bool, default=True\n",
      "     |          If ``True``, X will be copied; else, it may be overwritten.\n",
      "     |      \n",
      "     |      coef_init : ndarray of shape (n_features, ), default=None\n",
      "     |          The initial values of the coefficients.\n",
      "     |      \n",
      "     |      verbose : bool or int, default=False\n",
      "     |          Amount of verbosity.\n",
      "     |      \n",
      "     |      return_n_iter : bool, default=False\n",
      "     |          Whether to return the number of iterations or not.\n",
      "     |      \n",
      "     |      positive : bool, default=False\n",
      "     |          If set to True, forces coefficients to be positive.\n",
      "     |          (Only allowed when ``y.ndim == 1``).\n",
      "     |      \n",
      "     |      check_input : bool, default=True\n",
      "     |          If set to False, the input validation checks are skipped (including the\n",
      "     |          Gram matrix when provided). It is assumed that they are handled\n",
      "     |          by the caller.\n",
      "     |      \n",
      "     |      **params : kwargs\n",
      "     |          Keyword arguments passed to the coordinate descent solver.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      alphas : ndarray of shape (n_alphas,)\n",
      "     |          The alphas along the path where models are computed.\n",
      "     |      \n",
      "     |      coefs : ndarray of shape (n_features, n_alphas) or             (n_outputs, n_features, n_alphas)\n",
      "     |          Coefficients along the path.\n",
      "     |      \n",
      "     |      dual_gaps : ndarray of shape (n_alphas,)\n",
      "     |          The dual gaps at the end of the optimization for each alpha.\n",
      "     |      \n",
      "     |      n_iters : list of int\n",
      "     |          The number of iterations taken by the coordinate descent optimizer to\n",
      "     |          reach the specified tolerance for each alpha.\n",
      "     |          (Is returned when ``return_n_iter`` is set to True).\n",
      "     |      \n",
      "     |      See Also\n",
      "     |      --------\n",
      "     |      MultiTaskElasticNet\n",
      "     |      MultiTaskElasticNetCV\n",
      "     |      ElasticNet\n",
      "     |      ElasticNetCV\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      For an example, see\n",
      "     |      :ref:`examples/linear_model/plot_lasso_coordinate_descent_path.py\n",
      "     |      <sphx_glr_auto_examples_linear_model_plot_lasso_coordinate_descent_path.py>`.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties inherited from ElasticNet:\n",
      "     |  \n",
      "     |  sparse_coef_\n",
      "     |      Sparse representation of the fitted `coef_`.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.base.MultiOutputMixin:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.RegressorMixin:\n",
      "     |  \n",
      "     |  score(self, X, y, sample_weight=None)\n",
      "     |      Return the coefficient of determination :math:`R^2` of the\n",
      "     |      prediction.\n",
      "     |      \n",
      "     |      The coefficient :math:`R^2` is defined as :math:`(1 - \\frac{u}{v})`,\n",
      "     |      where :math:`u` is the residual sum of squares ``((y_true - y_pred)\n",
      "     |      ** 2).sum()`` and :math:`v` is the total sum of squares ``((y_true -\n",
      "     |      y_true.mean()) ** 2).sum()``. The best possible score is 1.0 and it\n",
      "     |      can be negative (because the model can be arbitrarily worse). A\n",
      "     |      constant model that always predicts the expected value of `y`,\n",
      "     |      disregarding the input features, would get a :math:`R^2` score of\n",
      "     |      0.0.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          Test samples. For some estimators this may be a precomputed\n",
      "     |          kernel matrix or a list of generic objects instead with shape\n",
      "     |          ``(n_samples, n_samples_fitted)``, where ``n_samples_fitted``\n",
      "     |          is the number of samples used in the fitting for the estimator.\n",
      "     |      \n",
      "     |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      "     |          True values for `X`.\n",
      "     |      \n",
      "     |      sample_weight : array-like of shape (n_samples,), default=None\n",
      "     |          Sample weights.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      score : float\n",
      "     |          :math:`R^2` of ``self.predict(X)`` wrt. `y`.\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      The :math:`R^2` score used when calling ``score`` on a regressor uses\n",
      "     |      ``multioutput='uniform_average'`` from version 0.23 to keep consistent\n",
      "     |      with default value of :func:`~sklearn.metrics.r2_score`.\n",
      "     |      This influences the ``score`` method of all the multioutput\n",
      "     |      regressors (except for\n",
      "     |      :class:`~sklearn.multioutput.MultiOutputRegressor`).\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.linear_model._base.LinearModel:\n",
      "     |  \n",
      "     |  predict(self, X)\n",
      "     |      Predict using the linear model.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like or sparse matrix, shape (n_samples, n_features)\n",
      "     |          Samples.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      C : array, shape (n_samples,)\n",
      "     |          Returns predicted values.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |  \n",
      "     |  __repr__(self, N_CHAR_MAX=700)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  get_params(self, deep=True)\n",
      "     |      Get parameters for this estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      deep : bool, default=True\n",
      "     |          If True, will return the parameters for this estimator and\n",
      "     |          contained subobjects that are estimators.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      params : dict\n",
      "     |          Parameter names mapped to their values.\n",
      "     |  \n",
      "     |  set_params(self, **params)\n",
      "     |      Set the parameters of this estimator.\n",
      "     |      \n",
      "     |      The method works on simple estimators as well as on nested objects\n",
      "     |      (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n",
      "     |      parameters of the form ``<component>__<parameter>`` so that it's\n",
      "     |      possible to update each component of a nested object.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      **params : dict\n",
      "     |          Estimator parameters.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : estimator instance\n",
      "     |          Estimator instance.\n",
      "    \n",
      "    class MultiTaskLassoCV(sklearn.base.RegressorMixin, LinearModelCV)\n",
      "     |  MultiTaskLassoCV(*, eps=0.001, n_alphas=100, alphas=None, fit_intercept=True, normalize=False, max_iter=1000, tol=0.0001, copy_X=True, cv=None, verbose=False, n_jobs=None, random_state=None, selection='cyclic')\n",
      "     |  \n",
      "     |  Multi-task Lasso model trained with L1/L2 mixed-norm as regularizer.\n",
      "     |  \n",
      "     |  See glossary entry for :term:`cross-validation estimator`.\n",
      "     |  \n",
      "     |  The optimization objective for MultiTaskLasso is::\n",
      "     |  \n",
      "     |      (1 / (2 * n_samples)) * ||Y - XW||^Fro_2 + alpha * ||W||_21\n",
      "     |  \n",
      "     |  Where::\n",
      "     |  \n",
      "     |      ||W||_21 = \\sum_i \\sqrt{\\sum_j w_{ij}^2}\n",
      "     |  \n",
      "     |  i.e. the sum of norm of each row.\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <multi_task_lasso>`.\n",
      "     |  \n",
      "     |  .. versionadded:: 0.15\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  eps : float, default=1e-3\n",
      "     |      Length of the path. ``eps=1e-3`` means that\n",
      "     |      ``alpha_min / alpha_max = 1e-3``.\n",
      "     |  \n",
      "     |  n_alphas : int, default=100\n",
      "     |      Number of alphas along the regularization path.\n",
      "     |  \n",
      "     |  alphas : array-like, default=None\n",
      "     |      List of alphas where to compute the models.\n",
      "     |      If not provided, set automatically.\n",
      "     |  \n",
      "     |  fit_intercept : bool, default=True\n",
      "     |      Whether to calculate the intercept for this model. If set\n",
      "     |      to false, no intercept will be used in calculations\n",
      "     |      (i.e. data is expected to be centered).\n",
      "     |  \n",
      "     |  normalize : bool, default=False\n",
      "     |      This parameter is ignored when ``fit_intercept`` is set to False.\n",
      "     |      If True, the regressors X will be normalized before regression by\n",
      "     |      subtracting the mean and dividing by the l2-norm.\n",
      "     |      If you wish to standardize, please use\n",
      "     |      :class:`~sklearn.preprocessing.StandardScaler` before calling ``fit``\n",
      "     |      on an estimator with ``normalize=False``.\n",
      "     |  \n",
      "     |  max_iter : int, default=1000\n",
      "     |      The maximum number of iterations.\n",
      "     |  \n",
      "     |  tol : float, default=1e-4\n",
      "     |      The tolerance for the optimization: if the updates are\n",
      "     |      smaller than ``tol``, the optimization code checks the\n",
      "     |      dual gap for optimality and continues until it is smaller\n",
      "     |      than ``tol``.\n",
      "     |  \n",
      "     |  copy_X : bool, default=True\n",
      "     |      If ``True``, X will be copied; else, it may be overwritten.\n",
      "     |  \n",
      "     |  cv : int, cross-validation generator or iterable, default=None\n",
      "     |      Determines the cross-validation splitting strategy.\n",
      "     |      Possible inputs for cv are:\n",
      "     |  \n",
      "     |      - None, to use the default 5-fold cross-validation,\n",
      "     |      - int, to specify the number of folds.\n",
      "     |      - :term:`CV splitter`,\n",
      "     |      - An iterable yielding (train, test) splits as arrays of indices.\n",
      "     |  \n",
      "     |      For int/None inputs, :class:`KFold` is used.\n",
      "     |  \n",
      "     |      Refer :ref:`User Guide <cross_validation>` for the various\n",
      "     |      cross-validation strategies that can be used here.\n",
      "     |  \n",
      "     |      .. versionchanged:: 0.22\n",
      "     |          ``cv`` default value if None changed from 3-fold to 5-fold.\n",
      "     |  \n",
      "     |  verbose : bool or int, default=False\n",
      "     |      Amount of verbosity.\n",
      "     |  \n",
      "     |  n_jobs : int, default=None\n",
      "     |      Number of CPUs to use during the cross validation. Note that this is\n",
      "     |      used only if multiple values for l1_ratio are given.\n",
      "     |      ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n",
      "     |      ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n",
      "     |      for more details.\n",
      "     |  \n",
      "     |  random_state : int, RandomState instance, default=None\n",
      "     |      The seed of the pseudo random number generator that selects a random\n",
      "     |      feature to update. Used when ``selection`` == 'random'.\n",
      "     |      Pass an int for reproducible output across multiple function calls.\n",
      "     |      See :term:`Glossary <random_state>`.\n",
      "     |  \n",
      "     |  selection : {'cyclic', 'random'}, default='cyclic'\n",
      "     |      If set to 'random', a random coefficient is updated every iteration\n",
      "     |      rather than looping over features sequentially by default. This\n",
      "     |      (setting to 'random') often leads to significantly faster convergence\n",
      "     |      especially when tol is higher than 1e-4.\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  intercept_ : ndarray of shape (n_tasks,)\n",
      "     |      Independent term in decision function.\n",
      "     |  \n",
      "     |  coef_ : ndarray of shape (n_tasks, n_features)\n",
      "     |      Parameter vector (W in the cost function formula).\n",
      "     |      Note that ``coef_`` stores the transpose of ``W``, ``W.T``.\n",
      "     |  \n",
      "     |  alpha_ : float\n",
      "     |      The amount of penalization chosen by cross validation.\n",
      "     |  \n",
      "     |  mse_path_ : ndarray of shape (n_alphas, n_folds)\n",
      "     |      Mean square error for the test set on each fold, varying alpha.\n",
      "     |  \n",
      "     |  alphas_ : ndarray of shape (n_alphas,)\n",
      "     |      The grid of alphas used for fitting.\n",
      "     |  \n",
      "     |  n_iter_ : int\n",
      "     |      Number of iterations run by the coordinate descent solver to reach\n",
      "     |      the specified tolerance for the optimal alpha.\n",
      "     |  \n",
      "     |  dual_gap_ : float\n",
      "     |      The dual gap at the end of the optimization for the optimal alpha.\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  >>> from sklearn.linear_model import MultiTaskLassoCV\n",
      "     |  >>> from sklearn.datasets import make_regression\n",
      "     |  >>> from sklearn.metrics import r2_score\n",
      "     |  >>> X, y = make_regression(n_targets=2, noise=4, random_state=0)\n",
      "     |  >>> reg = MultiTaskLassoCV(cv=5, random_state=0).fit(X, y)\n",
      "     |  >>> r2_score(y, reg.predict(X))\n",
      "     |  0.9994...\n",
      "     |  >>> reg.alpha_\n",
      "     |  0.5713...\n",
      "     |  >>> reg.predict(X[:1,])\n",
      "     |  array([[153.7971...,  94.9015...]])\n",
      "     |  \n",
      "     |  See Also\n",
      "     |  --------\n",
      "     |  MultiTaskElasticNet\n",
      "     |  ElasticNetCV\n",
      "     |  MultiTaskElasticNetCV\n",
      "     |  \n",
      "     |  Notes\n",
      "     |  -----\n",
      "     |  The algorithm used to fit the model is coordinate descent.\n",
      "     |  \n",
      "     |  To avoid unnecessary memory duplication the X and y arguments of the fit\n",
      "     |  method should be directly passed as Fortran-contiguous numpy arrays.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      MultiTaskLassoCV\n",
      "     |      sklearn.base.RegressorMixin\n",
      "     |      LinearModelCV\n",
      "     |      sklearn.base.MultiOutputMixin\n",
      "     |      sklearn.linear_model._base.LinearModel\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, *, eps=0.001, n_alphas=100, alphas=None, fit_intercept=True, normalize=False, max_iter=1000, tol=0.0001, copy_X=True, cv=None, verbose=False, n_jobs=None, random_state=None, selection='cyclic')\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods defined here:\n",
      "     |  \n",
      "     |  path = lasso_path(X, y, *, eps=0.001, n_alphas=100, alphas=None, precompute='auto', Xy=None, copy_X=True, coef_init=None, verbose=False, return_n_iter=False, positive=False, **params)\n",
      "     |      Compute Lasso path with coordinate descent\n",
      "     |      \n",
      "     |      The Lasso optimization function varies for mono and multi-outputs.\n",
      "     |      \n",
      "     |      For mono-output tasks it is::\n",
      "     |      \n",
      "     |          (1 / (2 * n_samples)) * ||y - Xw||^2_2 + alpha * ||w||_1\n",
      "     |      \n",
      "     |      For multi-output tasks it is::\n",
      "     |      \n",
      "     |          (1 / (2 * n_samples)) * ||Y - XW||^2_Fro + alpha * ||W||_21\n",
      "     |      \n",
      "     |      Where::\n",
      "     |      \n",
      "     |          ||W||_21 = \\sum_i \\sqrt{\\sum_j w_{ij}^2}\n",
      "     |      \n",
      "     |      i.e. the sum of norm of each row.\n",
      "     |      \n",
      "     |      Read more in the :ref:`User Guide <lasso>`.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          Training data. Pass directly as Fortran-contiguous data to avoid\n",
      "     |          unnecessary memory duplication. If ``y`` is mono-output then ``X``\n",
      "     |          can be sparse.\n",
      "     |      \n",
      "     |      y : {array-like, sparse matrix} of shape (n_samples,) or         (n_samples, n_outputs)\n",
      "     |          Target values\n",
      "     |      \n",
      "     |      eps : float, default=1e-3\n",
      "     |          Length of the path. ``eps=1e-3`` means that\n",
      "     |          ``alpha_min / alpha_max = 1e-3``\n",
      "     |      \n",
      "     |      n_alphas : int, default=100\n",
      "     |          Number of alphas along the regularization path\n",
      "     |      \n",
      "     |      alphas : ndarray, default=None\n",
      "     |          List of alphas where to compute the models.\n",
      "     |          If ``None`` alphas are set automatically\n",
      "     |      \n",
      "     |      precompute : 'auto', bool or array-like of shape (n_features, n_features),                 default='auto'\n",
      "     |          Whether to use a precomputed Gram matrix to speed up\n",
      "     |          calculations. If set to ``'auto'`` let us decide. The Gram\n",
      "     |          matrix can also be passed as argument.\n",
      "     |      \n",
      "     |      Xy : array-like of shape (n_features,) or (n_features, n_outputs),         default=None\n",
      "     |          Xy = np.dot(X.T, y) that can be precomputed. It is useful\n",
      "     |          only when the Gram matrix is precomputed.\n",
      "     |      \n",
      "     |      copy_X : bool, default=True\n",
      "     |          If ``True``, X will be copied; else, it may be overwritten.\n",
      "     |      \n",
      "     |      coef_init : ndarray of shape (n_features, ), default=None\n",
      "     |          The initial values of the coefficients.\n",
      "     |      \n",
      "     |      verbose : bool or int, default=False\n",
      "     |          Amount of verbosity.\n",
      "     |      \n",
      "     |      return_n_iter : bool, default=False\n",
      "     |          whether to return the number of iterations or not.\n",
      "     |      \n",
      "     |      positive : bool, default=False\n",
      "     |          If set to True, forces coefficients to be positive.\n",
      "     |          (Only allowed when ``y.ndim == 1``).\n",
      "     |      \n",
      "     |      **params : kwargs\n",
      "     |          keyword arguments passed to the coordinate descent solver.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      alphas : ndarray of shape (n_alphas,)\n",
      "     |          The alphas along the path where models are computed.\n",
      "     |      \n",
      "     |      coefs : ndarray of shape (n_features, n_alphas) or             (n_outputs, n_features, n_alphas)\n",
      "     |          Coefficients along the path.\n",
      "     |      \n",
      "     |      dual_gaps : ndarray of shape (n_alphas,)\n",
      "     |          The dual gaps at the end of the optimization for each alpha.\n",
      "     |      \n",
      "     |      n_iters : list of int\n",
      "     |          The number of iterations taken by the coordinate descent optimizer to\n",
      "     |          reach the specified tolerance for each alpha.\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      For an example, see\n",
      "     |      :ref:`examples/linear_model/plot_lasso_coordinate_descent_path.py\n",
      "     |      <sphx_glr_auto_examples_linear_model_plot_lasso_coordinate_descent_path.py>`.\n",
      "     |      \n",
      "     |      To avoid unnecessary memory duplication the X argument of the fit method\n",
      "     |      should be directly passed as a Fortran-contiguous numpy array.\n",
      "     |      \n",
      "     |      Note that in certain cases, the Lars solver may be significantly\n",
      "     |      faster to implement this functionality. In particular, linear\n",
      "     |      interpolation can be used to retrieve model coefficients between the\n",
      "     |      values output by lars_path\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      \n",
      "     |      Comparing lasso_path and lars_path with interpolation:\n",
      "     |      \n",
      "     |      >>> X = np.array([[1, 2, 3.1], [2.3, 5.4, 4.3]]).T\n",
      "     |      >>> y = np.array([1, 2, 3.1])\n",
      "     |      >>> # Use lasso_path to compute a coefficient path\n",
      "     |      >>> _, coef_path, _ = lasso_path(X, y, alphas=[5., 1., .5])\n",
      "     |      >>> print(coef_path)\n",
      "     |      [[0.         0.         0.46874778]\n",
      "     |       [0.2159048  0.4425765  0.23689075]]\n",
      "     |      \n",
      "     |      >>> # Now use lars_path and 1D linear interpolation to compute the\n",
      "     |      >>> # same path\n",
      "     |      >>> from sklearn.linear_model import lars_path\n",
      "     |      >>> alphas, active, coef_path_lars = lars_path(X, y, method='lasso')\n",
      "     |      >>> from scipy import interpolate\n",
      "     |      >>> coef_path_continuous = interpolate.interp1d(alphas[::-1],\n",
      "     |      ...                                             coef_path_lars[:, ::-1])\n",
      "     |      >>> print(coef_path_continuous([5., 1., .5]))\n",
      "     |      [[0.         0.         0.46915237]\n",
      "     |       [0.2159048  0.4425765  0.23668876]]\n",
      "     |      \n",
      "     |      See Also\n",
      "     |      --------\n",
      "     |      lars_path\n",
      "     |      Lasso\n",
      "     |      LassoLars\n",
      "     |      LassoCV\n",
      "     |      LassoLarsCV\n",
      "     |      sklearn.decomposition.sparse_encode\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.RegressorMixin:\n",
      "     |  \n",
      "     |  score(self, X, y, sample_weight=None)\n",
      "     |      Return the coefficient of determination :math:`R^2` of the\n",
      "     |      prediction.\n",
      "     |      \n",
      "     |      The coefficient :math:`R^2` is defined as :math:`(1 - \\frac{u}{v})`,\n",
      "     |      where :math:`u` is the residual sum of squares ``((y_true - y_pred)\n",
      "     |      ** 2).sum()`` and :math:`v` is the total sum of squares ``((y_true -\n",
      "     |      y_true.mean()) ** 2).sum()``. The best possible score is 1.0 and it\n",
      "     |      can be negative (because the model can be arbitrarily worse). A\n",
      "     |      constant model that always predicts the expected value of `y`,\n",
      "     |      disregarding the input features, would get a :math:`R^2` score of\n",
      "     |      0.0.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          Test samples. For some estimators this may be a precomputed\n",
      "     |          kernel matrix or a list of generic objects instead with shape\n",
      "     |          ``(n_samples, n_samples_fitted)``, where ``n_samples_fitted``\n",
      "     |          is the number of samples used in the fitting for the estimator.\n",
      "     |      \n",
      "     |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      "     |          True values for `X`.\n",
      "     |      \n",
      "     |      sample_weight : array-like of shape (n_samples,), default=None\n",
      "     |          Sample weights.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      score : float\n",
      "     |          :math:`R^2` of ``self.predict(X)`` wrt. `y`.\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      The :math:`R^2` score used when calling ``score`` on a regressor uses\n",
      "     |      ``multioutput='uniform_average'`` from version 0.23 to keep consistent\n",
      "     |      with default value of :func:`~sklearn.metrics.r2_score`.\n",
      "     |      This influences the ``score`` method of all the multioutput\n",
      "     |      regressors (except for\n",
      "     |      :class:`~sklearn.multioutput.MultiOutputRegressor`).\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.base.RegressorMixin:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from LinearModelCV:\n",
      "     |  \n",
      "     |  fit(self, X, y)\n",
      "     |      Fit linear model with coordinate descent.\n",
      "     |      \n",
      "     |      Fit is on grid of alphas and best alpha estimated by cross-validation.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          Training data. Pass directly as Fortran-contiguous data\n",
      "     |          to avoid unnecessary memory duplication. If y is mono-output,\n",
      "     |          X can be sparse.\n",
      "     |      \n",
      "     |      y : array-like of shape (n_samples,) or (n_samples, n_targets)\n",
      "     |          Target values.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.linear_model._base.LinearModel:\n",
      "     |  \n",
      "     |  predict(self, X)\n",
      "     |      Predict using the linear model.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like or sparse matrix, shape (n_samples, n_features)\n",
      "     |          Samples.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      C : array, shape (n_samples,)\n",
      "     |          Returns predicted values.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |  \n",
      "     |  __repr__(self, N_CHAR_MAX=700)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  get_params(self, deep=True)\n",
      "     |      Get parameters for this estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      deep : bool, default=True\n",
      "     |          If True, will return the parameters for this estimator and\n",
      "     |          contained subobjects that are estimators.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      params : dict\n",
      "     |          Parameter names mapped to their values.\n",
      "     |  \n",
      "     |  set_params(self, **params)\n",
      "     |      Set the parameters of this estimator.\n",
      "     |      \n",
      "     |      The method works on simple estimators as well as on nested objects\n",
      "     |      (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n",
      "     |      parameters of the form ``<component>__<parameter>`` so that it's\n",
      "     |      possible to update each component of a nested object.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      **params : dict\n",
      "     |          Estimator parameters.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : estimator instance\n",
      "     |          Estimator instance.\n",
      "    \n",
      "    class OrthogonalMatchingPursuit(sklearn.base.MultiOutputMixin, sklearn.base.RegressorMixin, sklearn.linear_model._base.LinearModel)\n",
      "     |  OrthogonalMatchingPursuit(*, n_nonzero_coefs=None, tol=None, fit_intercept=True, normalize=True, precompute='auto')\n",
      "     |  \n",
      "     |  Orthogonal Matching Pursuit model (OMP).\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <omp>`.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  n_nonzero_coefs : int, default=None\n",
      "     |      Desired number of non-zero entries in the solution. If None (by\n",
      "     |      default) this value is set to 10% of n_features.\n",
      "     |  \n",
      "     |  tol : float, default=None\n",
      "     |      Maximum norm of the residual. If not None, overrides n_nonzero_coefs.\n",
      "     |  \n",
      "     |  fit_intercept : bool, default=True\n",
      "     |      whether to calculate the intercept for this model. If set\n",
      "     |      to false, no intercept will be used in calculations\n",
      "     |      (i.e. data is expected to be centered).\n",
      "     |  \n",
      "     |  normalize : bool, default=True\n",
      "     |      This parameter is ignored when ``fit_intercept`` is set to False.\n",
      "     |      If True, the regressors X will be normalized before regression by\n",
      "     |      subtracting the mean and dividing by the l2-norm.\n",
      "     |      If you wish to standardize, please use\n",
      "     |      :class:`~sklearn.preprocessing.StandardScaler` before calling ``fit``\n",
      "     |      on an estimator with ``normalize=False``.\n",
      "     |  \n",
      "     |  precompute : 'auto' or bool, default='auto'\n",
      "     |      Whether to use a precomputed Gram and Xy matrix to speed up\n",
      "     |      calculations. Improves performance when :term:`n_targets` or\n",
      "     |      :term:`n_samples` is very large. Note that if you already have such\n",
      "     |      matrices, you can pass them directly to the fit method.\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  coef_ : ndarray of shape (n_features,) or (n_targets, n_features)\n",
      "     |      Parameter vector (w in the formula).\n",
      "     |  \n",
      "     |  intercept_ : float or ndarray of shape (n_targets,)\n",
      "     |      Independent term in decision function.\n",
      "     |  \n",
      "     |  n_iter_ : int or array-like\n",
      "     |      Number of active features across every target.\n",
      "     |  \n",
      "     |  n_nonzero_coefs_ : int\n",
      "     |      The number of non-zero coefficients in the solution. If\n",
      "     |      `n_nonzero_coefs` is None and `tol` is None this value is either set\n",
      "     |      to 10% of `n_features` or 1, whichever is greater.\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  >>> from sklearn.linear_model import OrthogonalMatchingPursuit\n",
      "     |  >>> from sklearn.datasets import make_regression\n",
      "     |  >>> X, y = make_regression(noise=4, random_state=0)\n",
      "     |  >>> reg = OrthogonalMatchingPursuit().fit(X, y)\n",
      "     |  >>> reg.score(X, y)\n",
      "     |  0.9991...\n",
      "     |  >>> reg.predict(X[:1,])\n",
      "     |  array([-78.3854...])\n",
      "     |  \n",
      "     |  Notes\n",
      "     |  -----\n",
      "     |  Orthogonal matching pursuit was introduced in G. Mallat, Z. Zhang,\n",
      "     |  Matching pursuits with time-frequency dictionaries, IEEE Transactions on\n",
      "     |  Signal Processing, Vol. 41, No. 12. (December 1993), pp. 3397-3415.\n",
      "     |  (http://blanche.polytechnique.fr/~mallat/papiers/MallatPursuit93.pdf)\n",
      "     |  \n",
      "     |  This implementation is based on Rubinstein, R., Zibulevsky, M. and Elad,\n",
      "     |  M., Efficient Implementation of the K-SVD Algorithm using Batch Orthogonal\n",
      "     |  Matching Pursuit Technical Report - CS Technion, April 2008.\n",
      "     |  https://www.cs.technion.ac.il/~ronrubin/Publications/KSVD-OMP-v2.pdf\n",
      "     |  \n",
      "     |  See Also\n",
      "     |  --------\n",
      "     |  orthogonal_mp\n",
      "     |  orthogonal_mp_gram\n",
      "     |  lars_path\n",
      "     |  Lars\n",
      "     |  LassoLars\n",
      "     |  sklearn.decomposition.sparse_encode\n",
      "     |  OrthogonalMatchingPursuitCV\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      OrthogonalMatchingPursuit\n",
      "     |      sklearn.base.MultiOutputMixin\n",
      "     |      sklearn.base.RegressorMixin\n",
      "     |      sklearn.linear_model._base.LinearModel\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, *, n_nonzero_coefs=None, tol=None, fit_intercept=True, normalize=True, precompute='auto')\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  fit(self, X, y)\n",
      "     |      Fit the model using X, y as training data.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          Training data.\n",
      "     |      \n",
      "     |      y : array-like of shape (n_samples,) or (n_samples, n_targets)\n",
      "     |          Target values. Will be cast to X's dtype if necessary\n",
      "     |      \n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |          returns an instance of self.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.base.MultiOutputMixin:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.RegressorMixin:\n",
      "     |  \n",
      "     |  score(self, X, y, sample_weight=None)\n",
      "     |      Return the coefficient of determination :math:`R^2` of the\n",
      "     |      prediction.\n",
      "     |      \n",
      "     |      The coefficient :math:`R^2` is defined as :math:`(1 - \\frac{u}{v})`,\n",
      "     |      where :math:`u` is the residual sum of squares ``((y_true - y_pred)\n",
      "     |      ** 2).sum()`` and :math:`v` is the total sum of squares ``((y_true -\n",
      "     |      y_true.mean()) ** 2).sum()``. The best possible score is 1.0 and it\n",
      "     |      can be negative (because the model can be arbitrarily worse). A\n",
      "     |      constant model that always predicts the expected value of `y`,\n",
      "     |      disregarding the input features, would get a :math:`R^2` score of\n",
      "     |      0.0.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          Test samples. For some estimators this may be a precomputed\n",
      "     |          kernel matrix or a list of generic objects instead with shape\n",
      "     |          ``(n_samples, n_samples_fitted)``, where ``n_samples_fitted``\n",
      "     |          is the number of samples used in the fitting for the estimator.\n",
      "     |      \n",
      "     |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      "     |          True values for `X`.\n",
      "     |      \n",
      "     |      sample_weight : array-like of shape (n_samples,), default=None\n",
      "     |          Sample weights.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      score : float\n",
      "     |          :math:`R^2` of ``self.predict(X)`` wrt. `y`.\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      The :math:`R^2` score used when calling ``score`` on a regressor uses\n",
      "     |      ``multioutput='uniform_average'`` from version 0.23 to keep consistent\n",
      "     |      with default value of :func:`~sklearn.metrics.r2_score`.\n",
      "     |      This influences the ``score`` method of all the multioutput\n",
      "     |      regressors (except for\n",
      "     |      :class:`~sklearn.multioutput.MultiOutputRegressor`).\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.linear_model._base.LinearModel:\n",
      "     |  \n",
      "     |  predict(self, X)\n",
      "     |      Predict using the linear model.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like or sparse matrix, shape (n_samples, n_features)\n",
      "     |          Samples.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      C : array, shape (n_samples,)\n",
      "     |          Returns predicted values.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |  \n",
      "     |  __repr__(self, N_CHAR_MAX=700)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  get_params(self, deep=True)\n",
      "     |      Get parameters for this estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      deep : bool, default=True\n",
      "     |          If True, will return the parameters for this estimator and\n",
      "     |          contained subobjects that are estimators.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      params : dict\n",
      "     |          Parameter names mapped to their values.\n",
      "     |  \n",
      "     |  set_params(self, **params)\n",
      "     |      Set the parameters of this estimator.\n",
      "     |      \n",
      "     |      The method works on simple estimators as well as on nested objects\n",
      "     |      (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n",
      "     |      parameters of the form ``<component>__<parameter>`` so that it's\n",
      "     |      possible to update each component of a nested object.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      **params : dict\n",
      "     |          Estimator parameters.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : estimator instance\n",
      "     |          Estimator instance.\n",
      "    \n",
      "    class OrthogonalMatchingPursuitCV(sklearn.base.RegressorMixin, sklearn.linear_model._base.LinearModel)\n",
      "     |  OrthogonalMatchingPursuitCV(*, copy=True, fit_intercept=True, normalize=True, max_iter=None, cv=None, n_jobs=None, verbose=False)\n",
      "     |  \n",
      "     |  Cross-validated Orthogonal Matching Pursuit model (OMP).\n",
      "     |  \n",
      "     |  See glossary entry for :term:`cross-validation estimator`.\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <omp>`.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  copy : bool, default=True\n",
      "     |      Whether the design matrix X must be copied by the algorithm. A false\n",
      "     |      value is only helpful if X is already Fortran-ordered, otherwise a\n",
      "     |      copy is made anyway.\n",
      "     |  \n",
      "     |  fit_intercept : bool, default=True\n",
      "     |      whether to calculate the intercept for this model. If set\n",
      "     |      to false, no intercept will be used in calculations\n",
      "     |      (i.e. data is expected to be centered).\n",
      "     |  \n",
      "     |  normalize : bool, default=True\n",
      "     |      This parameter is ignored when ``fit_intercept`` is set to False.\n",
      "     |      If True, the regressors X will be normalized before regression by\n",
      "     |      subtracting the mean and dividing by the l2-norm.\n",
      "     |      If you wish to standardize, please use\n",
      "     |      :class:`~sklearn.preprocessing.StandardScaler` before calling ``fit``\n",
      "     |      on an estimator with ``normalize=False``.\n",
      "     |  \n",
      "     |  max_iter : int, default=None\n",
      "     |      Maximum numbers of iterations to perform, therefore maximum features\n",
      "     |      to include. 10% of ``n_features`` but at least 5 if available.\n",
      "     |  \n",
      "     |  cv : int, cross-validation generator or iterable, default=None\n",
      "     |      Determines the cross-validation splitting strategy.\n",
      "     |      Possible inputs for cv are:\n",
      "     |  \n",
      "     |      - None, to use the default 5-fold cross-validation,\n",
      "     |      - integer, to specify the number of folds.\n",
      "     |      - :term:`CV splitter`,\n",
      "     |      - An iterable yielding (train, test) splits as arrays of indices.\n",
      "     |  \n",
      "     |      For integer/None inputs, :class:`KFold` is used.\n",
      "     |  \n",
      "     |      Refer :ref:`User Guide <cross_validation>` for the various\n",
      "     |      cross-validation strategies that can be used here.\n",
      "     |  \n",
      "     |      .. versionchanged:: 0.22\n",
      "     |          ``cv`` default value if None changed from 3-fold to 5-fold.\n",
      "     |  \n",
      "     |  n_jobs : int, default=None\n",
      "     |      Number of CPUs to use during the cross validation.\n",
      "     |      ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n",
      "     |      ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n",
      "     |      for more details.\n",
      "     |  \n",
      "     |  verbose : bool or int, default=False\n",
      "     |      Sets the verbosity amount.\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  intercept_ : float or ndarray of shape (n_targets,)\n",
      "     |      Independent term in decision function.\n",
      "     |  \n",
      "     |  coef_ : ndarray of shape (n_features,) or (n_targets, n_features)\n",
      "     |      Parameter vector (w in the problem formulation).\n",
      "     |  \n",
      "     |  n_nonzero_coefs_ : int\n",
      "     |      Estimated number of non-zero coefficients giving the best mean squared\n",
      "     |      error over the cross-validation folds.\n",
      "     |  \n",
      "     |  n_iter_ : int or array-like\n",
      "     |      Number of active features across every target for the model refit with\n",
      "     |      the best hyperparameters got by cross-validating across all folds.\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  >>> from sklearn.linear_model import OrthogonalMatchingPursuitCV\n",
      "     |  >>> from sklearn.datasets import make_regression\n",
      "     |  >>> X, y = make_regression(n_features=100, n_informative=10,\n",
      "     |  ...                        noise=4, random_state=0)\n",
      "     |  >>> reg = OrthogonalMatchingPursuitCV(cv=5).fit(X, y)\n",
      "     |  >>> reg.score(X, y)\n",
      "     |  0.9991...\n",
      "     |  >>> reg.n_nonzero_coefs_\n",
      "     |  10\n",
      "     |  >>> reg.predict(X[:1,])\n",
      "     |  array([-78.3854...])\n",
      "     |  \n",
      "     |  See Also\n",
      "     |  --------\n",
      "     |  orthogonal_mp\n",
      "     |  orthogonal_mp_gram\n",
      "     |  lars_path\n",
      "     |  Lars\n",
      "     |  LassoLars\n",
      "     |  OrthogonalMatchingPursuit\n",
      "     |  LarsCV\n",
      "     |  LassoLarsCV\n",
      "     |  sklearn.decomposition.sparse_encode\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      OrthogonalMatchingPursuitCV\n",
      "     |      sklearn.base.RegressorMixin\n",
      "     |      sklearn.linear_model._base.LinearModel\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, *, copy=True, fit_intercept=True, normalize=True, max_iter=None, cv=None, n_jobs=None, verbose=False)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  fit(self, X, y)\n",
      "     |      Fit the model using X, y as training data.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          Training data.\n",
      "     |      \n",
      "     |      y : array-like of shape (n_samples,)\n",
      "     |          Target values. Will be cast to X's dtype if necessary.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |          returns an instance of self.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.RegressorMixin:\n",
      "     |  \n",
      "     |  score(self, X, y, sample_weight=None)\n",
      "     |      Return the coefficient of determination :math:`R^2` of the\n",
      "     |      prediction.\n",
      "     |      \n",
      "     |      The coefficient :math:`R^2` is defined as :math:`(1 - \\frac{u}{v})`,\n",
      "     |      where :math:`u` is the residual sum of squares ``((y_true - y_pred)\n",
      "     |      ** 2).sum()`` and :math:`v` is the total sum of squares ``((y_true -\n",
      "     |      y_true.mean()) ** 2).sum()``. The best possible score is 1.0 and it\n",
      "     |      can be negative (because the model can be arbitrarily worse). A\n",
      "     |      constant model that always predicts the expected value of `y`,\n",
      "     |      disregarding the input features, would get a :math:`R^2` score of\n",
      "     |      0.0.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          Test samples. For some estimators this may be a precomputed\n",
      "     |          kernel matrix or a list of generic objects instead with shape\n",
      "     |          ``(n_samples, n_samples_fitted)``, where ``n_samples_fitted``\n",
      "     |          is the number of samples used in the fitting for the estimator.\n",
      "     |      \n",
      "     |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      "     |          True values for `X`.\n",
      "     |      \n",
      "     |      sample_weight : array-like of shape (n_samples,), default=None\n",
      "     |          Sample weights.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      score : float\n",
      "     |          :math:`R^2` of ``self.predict(X)`` wrt. `y`.\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      The :math:`R^2` score used when calling ``score`` on a regressor uses\n",
      "     |      ``multioutput='uniform_average'`` from version 0.23 to keep consistent\n",
      "     |      with default value of :func:`~sklearn.metrics.r2_score`.\n",
      "     |      This influences the ``score`` method of all the multioutput\n",
      "     |      regressors (except for\n",
      "     |      :class:`~sklearn.multioutput.MultiOutputRegressor`).\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.base.RegressorMixin:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.linear_model._base.LinearModel:\n",
      "     |  \n",
      "     |  predict(self, X)\n",
      "     |      Predict using the linear model.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like or sparse matrix, shape (n_samples, n_features)\n",
      "     |          Samples.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      C : array, shape (n_samples,)\n",
      "     |          Returns predicted values.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |  \n",
      "     |  __repr__(self, N_CHAR_MAX=700)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  get_params(self, deep=True)\n",
      "     |      Get parameters for this estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      deep : bool, default=True\n",
      "     |          If True, will return the parameters for this estimator and\n",
      "     |          contained subobjects that are estimators.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      params : dict\n",
      "     |          Parameter names mapped to their values.\n",
      "     |  \n",
      "     |  set_params(self, **params)\n",
      "     |      Set the parameters of this estimator.\n",
      "     |      \n",
      "     |      The method works on simple estimators as well as on nested objects\n",
      "     |      (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n",
      "     |      parameters of the form ``<component>__<parameter>`` so that it's\n",
      "     |      possible to update each component of a nested object.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      **params : dict\n",
      "     |          Estimator parameters.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : estimator instance\n",
      "     |          Estimator instance.\n",
      "    \n",
      "    class PassiveAggressiveClassifier(sklearn.linear_model._stochastic_gradient.BaseSGDClassifier)\n",
      "     |  PassiveAggressiveClassifier(*, C=1.0, fit_intercept=True, max_iter=1000, tol=0.001, early_stopping=False, validation_fraction=0.1, n_iter_no_change=5, shuffle=True, verbose=0, loss='hinge', n_jobs=None, random_state=None, warm_start=False, class_weight=None, average=False)\n",
      "     |  \n",
      "     |  Passive Aggressive Classifier\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <passive_aggressive>`.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  \n",
      "     |  C : float, default=1.0\n",
      "     |      Maximum step size (regularization). Defaults to 1.0.\n",
      "     |  \n",
      "     |  fit_intercept : bool, default=True\n",
      "     |      Whether the intercept should be estimated or not. If False, the\n",
      "     |      data is assumed to be already centered.\n",
      "     |  \n",
      "     |  max_iter : int, default=1000\n",
      "     |      The maximum number of passes over the training data (aka epochs).\n",
      "     |      It only impacts the behavior in the ``fit`` method, and not the\n",
      "     |      :meth:`partial_fit` method.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.19\n",
      "     |  \n",
      "     |  tol : float or None, default=1e-3\n",
      "     |      The stopping criterion. If it is not None, the iterations will stop\n",
      "     |      when (loss > previous_loss - tol).\n",
      "     |  \n",
      "     |      .. versionadded:: 0.19\n",
      "     |  \n",
      "     |  early_stopping : bool, default=False\n",
      "     |      Whether to use early stopping to terminate training when validation.\n",
      "     |      score is not improving. If set to True, it will automatically set aside\n",
      "     |      a stratified fraction of training data as validation and terminate\n",
      "     |      training when validation score is not improving by at least tol for\n",
      "     |      n_iter_no_change consecutive epochs.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.20\n",
      "     |  \n",
      "     |  validation_fraction : float, default=0.1\n",
      "     |      The proportion of training data to set aside as validation set for\n",
      "     |      early stopping. Must be between 0 and 1.\n",
      "     |      Only used if early_stopping is True.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.20\n",
      "     |  \n",
      "     |  n_iter_no_change : int, default=5\n",
      "     |      Number of iterations with no improvement to wait before early stopping.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.20\n",
      "     |  \n",
      "     |  shuffle : bool, default=True\n",
      "     |      Whether or not the training data should be shuffled after each epoch.\n",
      "     |  \n",
      "     |  verbose : integer, default=0\n",
      "     |      The verbosity level\n",
      "     |  \n",
      "     |  loss : string, default=\"hinge\"\n",
      "     |      The loss function to be used:\n",
      "     |      hinge: equivalent to PA-I in the reference paper.\n",
      "     |      squared_hinge: equivalent to PA-II in the reference paper.\n",
      "     |  \n",
      "     |  n_jobs : int or None, default=None\n",
      "     |      The number of CPUs to use to do the OVA (One Versus All, for\n",
      "     |      multi-class problems) computation.\n",
      "     |      ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n",
      "     |      ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n",
      "     |      for more details.\n",
      "     |  \n",
      "     |  random_state : int, RandomState instance, default=None\n",
      "     |      Used to shuffle the training data, when ``shuffle`` is set to\n",
      "     |      ``True``. Pass an int for reproducible output across multiple\n",
      "     |      function calls.\n",
      "     |      See :term:`Glossary <random_state>`.\n",
      "     |  \n",
      "     |  warm_start : bool, default=False\n",
      "     |      When set to True, reuse the solution of the previous call to fit as\n",
      "     |      initialization, otherwise, just erase the previous solution.\n",
      "     |      See :term:`the Glossary <warm_start>`.\n",
      "     |  \n",
      "     |      Repeatedly calling fit or partial_fit when warm_start is True can\n",
      "     |      result in a different solution than when calling fit a single time\n",
      "     |      because of the way the data is shuffled.\n",
      "     |  \n",
      "     |  class_weight : dict, {class_label: weight} or \"balanced\" or None,             default=None\n",
      "     |      Preset for the class_weight fit parameter.\n",
      "     |  \n",
      "     |      Weights associated with classes. If not given, all classes\n",
      "     |      are supposed to have weight one.\n",
      "     |  \n",
      "     |      The \"balanced\" mode uses the values of y to automatically adjust\n",
      "     |      weights inversely proportional to class frequencies in the input data\n",
      "     |      as ``n_samples / (n_classes * np.bincount(y))``\n",
      "     |  \n",
      "     |      .. versionadded:: 0.17\n",
      "     |         parameter *class_weight* to automatically weight samples.\n",
      "     |  \n",
      "     |  average : bool or int, default=False\n",
      "     |      When set to True, computes the averaged SGD weights and stores the\n",
      "     |      result in the ``coef_`` attribute. If set to an int greater than 1,\n",
      "     |      averaging will begin once the total number of samples seen reaches\n",
      "     |      average. So average=10 will begin averaging after seeing 10 samples.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.19\n",
      "     |         parameter *average* to use weights averaging in SGD\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  coef_ : array, shape = [1, n_features] if n_classes == 2 else [n_classes,            n_features]\n",
      "     |      Weights assigned to the features.\n",
      "     |  \n",
      "     |  intercept_ : array, shape = [1] if n_classes == 2 else [n_classes]\n",
      "     |      Constants in decision function.\n",
      "     |  \n",
      "     |  n_iter_ : int\n",
      "     |      The actual number of iterations to reach the stopping criterion.\n",
      "     |      For multiclass fits, it is the maximum over every binary fit.\n",
      "     |  \n",
      "     |  classes_ : array of shape (n_classes,)\n",
      "     |      The unique classes labels.\n",
      "     |  \n",
      "     |  t_ : int\n",
      "     |      Number of weight updates performed during training.\n",
      "     |      Same as ``(n_iter_ * n_samples)``.\n",
      "     |  \n",
      "     |  loss_function_ : callable\n",
      "     |      Loss function used by the algorithm.\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  >>> from sklearn.linear_model import PassiveAggressiveClassifier\n",
      "     |  >>> from sklearn.datasets import make_classification\n",
      "     |  \n",
      "     |  >>> X, y = make_classification(n_features=4, random_state=0)\n",
      "     |  >>> clf = PassiveAggressiveClassifier(max_iter=1000, random_state=0,\n",
      "     |  ... tol=1e-3)\n",
      "     |  >>> clf.fit(X, y)\n",
      "     |  PassiveAggressiveClassifier(random_state=0)\n",
      "     |  >>> print(clf.coef_)\n",
      "     |  [[0.26642044 0.45070924 0.67251877 0.64185414]]\n",
      "     |  >>> print(clf.intercept_)\n",
      "     |  [1.84127814]\n",
      "     |  >>> print(clf.predict([[0, 0, 0, 0]]))\n",
      "     |  [1]\n",
      "     |  \n",
      "     |  See Also\n",
      "     |  --------\n",
      "     |  SGDClassifier\n",
      "     |  Perceptron\n",
      "     |  \n",
      "     |  References\n",
      "     |  ----------\n",
      "     |  Online Passive-Aggressive Algorithms\n",
      "     |  <http://jmlr.csail.mit.edu/papers/volume7/crammer06a/crammer06a.pdf>\n",
      "     |  K. Crammer, O. Dekel, J. Keshat, S. Shalev-Shwartz, Y. Singer - JMLR (2006)\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      PassiveAggressiveClassifier\n",
      "     |      sklearn.linear_model._stochastic_gradient.BaseSGDClassifier\n",
      "     |      sklearn.linear_model._base.LinearClassifierMixin\n",
      "     |      sklearn.base.ClassifierMixin\n",
      "     |      sklearn.linear_model._stochastic_gradient.BaseSGD\n",
      "     |      sklearn.linear_model._base.SparseCoefMixin\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, *, C=1.0, fit_intercept=True, max_iter=1000, tol=0.001, early_stopping=False, validation_fraction=0.1, n_iter_no_change=5, shuffle=True, verbose=0, loss='hinge', n_jobs=None, random_state=None, warm_start=False, class_weight=None, average=False)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  fit(self, X, y, coef_init=None, intercept_init=None)\n",
      "     |      Fit linear model with Passive Aggressive algorithm.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          Training data\n",
      "     |      \n",
      "     |      y : numpy array of shape [n_samples]\n",
      "     |          Target values\n",
      "     |      \n",
      "     |      coef_init : array, shape = [n_classes,n_features]\n",
      "     |          The initial coefficients to warm-start the optimization.\n",
      "     |      \n",
      "     |      intercept_init : array, shape = [n_classes]\n",
      "     |          The initial intercept to warm-start the optimization.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : returns an instance of self.\n",
      "     |  \n",
      "     |  partial_fit(self, X, y, classes=None)\n",
      "     |      Fit linear model with Passive Aggressive algorithm.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          Subset of the training data\n",
      "     |      \n",
      "     |      y : numpy array of shape [n_samples]\n",
      "     |          Subset of the target values\n",
      "     |      \n",
      "     |      classes : array, shape = [n_classes]\n",
      "     |          Classes across all calls to partial_fit.\n",
      "     |          Can be obtained by via `np.unique(y_all)`, where y_all is the\n",
      "     |          target vector of the entire dataset.\n",
      "     |          This argument is required for the first call to partial_fit\n",
      "     |          and can be omitted in the subsequent calls.\n",
      "     |          Note that y doesn't need to contain all labels in `classes`.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : returns an instance of self.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from sklearn.linear_model._stochastic_gradient.BaseSGDClassifier:\n",
      "     |  \n",
      "     |  loss_functions = {'epsilon_insensitive': (<class 'sklearn.linear_model...\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.linear_model._base.LinearClassifierMixin:\n",
      "     |  \n",
      "     |  decision_function(self, X)\n",
      "     |      Predict confidence scores for samples.\n",
      "     |      \n",
      "     |      The confidence score for a sample is proportional to the signed\n",
      "     |      distance of that sample to the hyperplane.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like or sparse matrix, shape (n_samples, n_features)\n",
      "     |          Samples.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      array, shape=(n_samples,) if n_classes == 2 else (n_samples, n_classes)\n",
      "     |          Confidence scores per (sample, class) combination. In the binary\n",
      "     |          case, confidence score for self.classes_[1] where >0 means this\n",
      "     |          class would be predicted.\n",
      "     |  \n",
      "     |  predict(self, X)\n",
      "     |      Predict class labels for samples in X.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like or sparse matrix, shape (n_samples, n_features)\n",
      "     |          Samples.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      C : array, shape [n_samples]\n",
      "     |          Predicted class label per sample.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.ClassifierMixin:\n",
      "     |  \n",
      "     |  score(self, X, y, sample_weight=None)\n",
      "     |      Return the mean accuracy on the given test data and labels.\n",
      "     |      \n",
      "     |      In multi-label classification, this is the subset accuracy\n",
      "     |      which is a harsh metric since you require for each sample that\n",
      "     |      each label set be correctly predicted.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          Test samples.\n",
      "     |      \n",
      "     |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      "     |          True labels for `X`.\n",
      "     |      \n",
      "     |      sample_weight : array-like of shape (n_samples,), default=None\n",
      "     |          Sample weights.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      score : float\n",
      "     |          Mean accuracy of ``self.predict(X)`` wrt. `y`.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.base.ClassifierMixin:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.linear_model._stochastic_gradient.BaseSGD:\n",
      "     |  \n",
      "     |  set_params(self, **kwargs)\n",
      "     |      Set and validate the parameters of estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      **kwargs : dict\n",
      "     |          Estimator parameters.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |          Estimator instance.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties inherited from sklearn.linear_model._stochastic_gradient.BaseSGD:\n",
      "     |  \n",
      "     |  average_coef_\n",
      "     |  \n",
      "     |  average_intercept_\n",
      "     |  \n",
      "     |  standard_coef_\n",
      "     |  \n",
      "     |  standard_intercept_\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.linear_model._base.SparseCoefMixin:\n",
      "     |  \n",
      "     |  densify(self)\n",
      "     |      Convert coefficient matrix to dense array format.\n",
      "     |      \n",
      "     |      Converts the ``coef_`` member (back) to a numpy.ndarray. This is the\n",
      "     |      default format of ``coef_`` and is required for fitting, so calling\n",
      "     |      this method is only required on models that have previously been\n",
      "     |      sparsified; otherwise, it is a no-op.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self\n",
      "     |          Fitted estimator.\n",
      "     |  \n",
      "     |  sparsify(self)\n",
      "     |      Convert coefficient matrix to sparse format.\n",
      "     |      \n",
      "     |      Converts the ``coef_`` member to a scipy.sparse matrix, which for\n",
      "     |      L1-regularized models can be much more memory- and storage-efficient\n",
      "     |      than the usual numpy.ndarray representation.\n",
      "     |      \n",
      "     |      The ``intercept_`` member is not converted.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self\n",
      "     |          Fitted estimator.\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      For non-sparse models, i.e. when there are not many zeros in ``coef_``,\n",
      "     |      this may actually *increase* memory usage, so use this method with\n",
      "     |      care. A rule of thumb is that the number of zero elements, which can\n",
      "     |      be computed with ``(coef_ == 0).sum()``, must be more than 50% for this\n",
      "     |      to provide significant benefits.\n",
      "     |      \n",
      "     |      After calling this method, further fitting with the partial_fit\n",
      "     |      method (if any) will not work until you call densify.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |  \n",
      "     |  __repr__(self, N_CHAR_MAX=700)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  get_params(self, deep=True)\n",
      "     |      Get parameters for this estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      deep : bool, default=True\n",
      "     |          If True, will return the parameters for this estimator and\n",
      "     |          contained subobjects that are estimators.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      params : dict\n",
      "     |          Parameter names mapped to their values.\n",
      "    \n",
      "    class PassiveAggressiveRegressor(sklearn.linear_model._stochastic_gradient.BaseSGDRegressor)\n",
      "     |  PassiveAggressiveRegressor(*, C=1.0, fit_intercept=True, max_iter=1000, tol=0.001, early_stopping=False, validation_fraction=0.1, n_iter_no_change=5, shuffle=True, verbose=0, loss='epsilon_insensitive', epsilon=0.1, random_state=None, warm_start=False, average=False)\n",
      "     |  \n",
      "     |  Passive Aggressive Regressor\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <passive_aggressive>`.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  \n",
      "     |  C : float, default=1.0\n",
      "     |      Maximum step size (regularization). Defaults to 1.0.\n",
      "     |  \n",
      "     |  fit_intercept : bool, default=True\n",
      "     |      Whether the intercept should be estimated or not. If False, the\n",
      "     |      data is assumed to be already centered. Defaults to True.\n",
      "     |  \n",
      "     |  max_iter : int, default=1000\n",
      "     |      The maximum number of passes over the training data (aka epochs).\n",
      "     |      It only impacts the behavior in the ``fit`` method, and not the\n",
      "     |      :meth:`partial_fit` method.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.19\n",
      "     |  \n",
      "     |  tol : float or None, default=1e-3\n",
      "     |      The stopping criterion. If it is not None, the iterations will stop\n",
      "     |      when (loss > previous_loss - tol).\n",
      "     |  \n",
      "     |      .. versionadded:: 0.19\n",
      "     |  \n",
      "     |  early_stopping : bool, default=False\n",
      "     |      Whether to use early stopping to terminate training when validation.\n",
      "     |      score is not improving. If set to True, it will automatically set aside\n",
      "     |      a fraction of training data as validation and terminate\n",
      "     |      training when validation score is not improving by at least tol for\n",
      "     |      n_iter_no_change consecutive epochs.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.20\n",
      "     |  \n",
      "     |  validation_fraction : float, default=0.1\n",
      "     |      The proportion of training data to set aside as validation set for\n",
      "     |      early stopping. Must be between 0 and 1.\n",
      "     |      Only used if early_stopping is True.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.20\n",
      "     |  \n",
      "     |  n_iter_no_change : int, default=5\n",
      "     |      Number of iterations with no improvement to wait before early stopping.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.20\n",
      "     |  \n",
      "     |  shuffle : bool, default=True\n",
      "     |      Whether or not the training data should be shuffled after each epoch.\n",
      "     |  \n",
      "     |  verbose : integer, default=0\n",
      "     |      The verbosity level\n",
      "     |  \n",
      "     |  loss : string, default=\"epsilon_insensitive\"\n",
      "     |      The loss function to be used:\n",
      "     |      epsilon_insensitive: equivalent to PA-I in the reference paper.\n",
      "     |      squared_epsilon_insensitive: equivalent to PA-II in the reference\n",
      "     |      paper.\n",
      "     |  \n",
      "     |  epsilon : float, default=0.1\n",
      "     |      If the difference between the current prediction and the correct label\n",
      "     |      is below this threshold, the model is not updated.\n",
      "     |  \n",
      "     |  random_state : int, RandomState instance, default=None\n",
      "     |      Used to shuffle the training data, when ``shuffle`` is set to\n",
      "     |      ``True``. Pass an int for reproducible output across multiple\n",
      "     |      function calls.\n",
      "     |      See :term:`Glossary <random_state>`.\n",
      "     |  \n",
      "     |  warm_start : bool, default=False\n",
      "     |      When set to True, reuse the solution of the previous call to fit as\n",
      "     |      initialization, otherwise, just erase the previous solution.\n",
      "     |      See :term:`the Glossary <warm_start>`.\n",
      "     |  \n",
      "     |      Repeatedly calling fit or partial_fit when warm_start is True can\n",
      "     |      result in a different solution than when calling fit a single time\n",
      "     |      because of the way the data is shuffled.\n",
      "     |  \n",
      "     |  average : bool or int, default=False\n",
      "     |      When set to True, computes the averaged SGD weights and stores the\n",
      "     |      result in the ``coef_`` attribute. If set to an int greater than 1,\n",
      "     |      averaging will begin once the total number of samples seen reaches\n",
      "     |      average. So average=10 will begin averaging after seeing 10 samples.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.19\n",
      "     |         parameter *average* to use weights averaging in SGD\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  coef_ : array, shape = [1, n_features] if n_classes == 2 else [n_classes,            n_features]\n",
      "     |      Weights assigned to the features.\n",
      "     |  \n",
      "     |  intercept_ : array, shape = [1] if n_classes == 2 else [n_classes]\n",
      "     |      Constants in decision function.\n",
      "     |  \n",
      "     |  n_iter_ : int\n",
      "     |      The actual number of iterations to reach the stopping criterion.\n",
      "     |  \n",
      "     |  t_ : int\n",
      "     |      Number of weight updates performed during training.\n",
      "     |      Same as ``(n_iter_ * n_samples)``.\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  >>> from sklearn.linear_model import PassiveAggressiveRegressor\n",
      "     |  >>> from sklearn.datasets import make_regression\n",
      "     |  \n",
      "     |  >>> X, y = make_regression(n_features=4, random_state=0)\n",
      "     |  >>> regr = PassiveAggressiveRegressor(max_iter=100, random_state=0,\n",
      "     |  ... tol=1e-3)\n",
      "     |  >>> regr.fit(X, y)\n",
      "     |  PassiveAggressiveRegressor(max_iter=100, random_state=0)\n",
      "     |  >>> print(regr.coef_)\n",
      "     |  [20.48736655 34.18818427 67.59122734 87.94731329]\n",
      "     |  >>> print(regr.intercept_)\n",
      "     |  [-0.02306214]\n",
      "     |  >>> print(regr.predict([[0, 0, 0, 0]]))\n",
      "     |  [-0.02306214]\n",
      "     |  \n",
      "     |  See Also\n",
      "     |  --------\n",
      "     |  SGDRegressor\n",
      "     |  \n",
      "     |  References\n",
      "     |  ----------\n",
      "     |  Online Passive-Aggressive Algorithms\n",
      "     |  <http://jmlr.csail.mit.edu/papers/volume7/crammer06a/crammer06a.pdf>\n",
      "     |  K. Crammer, O. Dekel, J. Keshat, S. Shalev-Shwartz, Y. Singer - JMLR (2006)\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      PassiveAggressiveRegressor\n",
      "     |      sklearn.linear_model._stochastic_gradient.BaseSGDRegressor\n",
      "     |      sklearn.base.RegressorMixin\n",
      "     |      sklearn.linear_model._stochastic_gradient.BaseSGD\n",
      "     |      sklearn.linear_model._base.SparseCoefMixin\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, *, C=1.0, fit_intercept=True, max_iter=1000, tol=0.001, early_stopping=False, validation_fraction=0.1, n_iter_no_change=5, shuffle=True, verbose=0, loss='epsilon_insensitive', epsilon=0.1, random_state=None, warm_start=False, average=False)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  fit(self, X, y, coef_init=None, intercept_init=None)\n",
      "     |      Fit linear model with Passive Aggressive algorithm.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          Training data\n",
      "     |      \n",
      "     |      y : numpy array of shape [n_samples]\n",
      "     |          Target values\n",
      "     |      \n",
      "     |      coef_init : array, shape = [n_features]\n",
      "     |          The initial coefficients to warm-start the optimization.\n",
      "     |      \n",
      "     |      intercept_init : array, shape = [1]\n",
      "     |          The initial intercept to warm-start the optimization.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : returns an instance of self.\n",
      "     |  \n",
      "     |  partial_fit(self, X, y)\n",
      "     |      Fit linear model with Passive Aggressive algorithm.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          Subset of training data\n",
      "     |      \n",
      "     |      y : numpy array of shape [n_samples]\n",
      "     |          Subset of target values\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : returns an instance of self.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.linear_model._stochastic_gradient.BaseSGDRegressor:\n",
      "     |  \n",
      "     |  predict(self, X)\n",
      "     |      Predict using the linear model\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix}, shape (n_samples, n_features)\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      ndarray of shape (n_samples,)\n",
      "     |         Predicted target values per element in X.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from sklearn.linear_model._stochastic_gradient.BaseSGDRegressor:\n",
      "     |  \n",
      "     |  loss_functions = {'epsilon_insensitive': (<class 'sklearn.linear_model...\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.RegressorMixin:\n",
      "     |  \n",
      "     |  score(self, X, y, sample_weight=None)\n",
      "     |      Return the coefficient of determination :math:`R^2` of the\n",
      "     |      prediction.\n",
      "     |      \n",
      "     |      The coefficient :math:`R^2` is defined as :math:`(1 - \\frac{u}{v})`,\n",
      "     |      where :math:`u` is the residual sum of squares ``((y_true - y_pred)\n",
      "     |      ** 2).sum()`` and :math:`v` is the total sum of squares ``((y_true -\n",
      "     |      y_true.mean()) ** 2).sum()``. The best possible score is 1.0 and it\n",
      "     |      can be negative (because the model can be arbitrarily worse). A\n",
      "     |      constant model that always predicts the expected value of `y`,\n",
      "     |      disregarding the input features, would get a :math:`R^2` score of\n",
      "     |      0.0.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          Test samples. For some estimators this may be a precomputed\n",
      "     |          kernel matrix or a list of generic objects instead with shape\n",
      "     |          ``(n_samples, n_samples_fitted)``, where ``n_samples_fitted``\n",
      "     |          is the number of samples used in the fitting for the estimator.\n",
      "     |      \n",
      "     |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      "     |          True values for `X`.\n",
      "     |      \n",
      "     |      sample_weight : array-like of shape (n_samples,), default=None\n",
      "     |          Sample weights.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      score : float\n",
      "     |          :math:`R^2` of ``self.predict(X)`` wrt. `y`.\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      The :math:`R^2` score used when calling ``score`` on a regressor uses\n",
      "     |      ``multioutput='uniform_average'`` from version 0.23 to keep consistent\n",
      "     |      with default value of :func:`~sklearn.metrics.r2_score`.\n",
      "     |      This influences the ``score`` method of all the multioutput\n",
      "     |      regressors (except for\n",
      "     |      :class:`~sklearn.multioutput.MultiOutputRegressor`).\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.base.RegressorMixin:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.linear_model._stochastic_gradient.BaseSGD:\n",
      "     |  \n",
      "     |  set_params(self, **kwargs)\n",
      "     |      Set and validate the parameters of estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      **kwargs : dict\n",
      "     |          Estimator parameters.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |          Estimator instance.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties inherited from sklearn.linear_model._stochastic_gradient.BaseSGD:\n",
      "     |  \n",
      "     |  average_coef_\n",
      "     |  \n",
      "     |  average_intercept_\n",
      "     |  \n",
      "     |  standard_coef_\n",
      "     |  \n",
      "     |  standard_intercept_\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.linear_model._base.SparseCoefMixin:\n",
      "     |  \n",
      "     |  densify(self)\n",
      "     |      Convert coefficient matrix to dense array format.\n",
      "     |      \n",
      "     |      Converts the ``coef_`` member (back) to a numpy.ndarray. This is the\n",
      "     |      default format of ``coef_`` and is required for fitting, so calling\n",
      "     |      this method is only required on models that have previously been\n",
      "     |      sparsified; otherwise, it is a no-op.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self\n",
      "     |          Fitted estimator.\n",
      "     |  \n",
      "     |  sparsify(self)\n",
      "     |      Convert coefficient matrix to sparse format.\n",
      "     |      \n",
      "     |      Converts the ``coef_`` member to a scipy.sparse matrix, which for\n",
      "     |      L1-regularized models can be much more memory- and storage-efficient\n",
      "     |      than the usual numpy.ndarray representation.\n",
      "     |      \n",
      "     |      The ``intercept_`` member is not converted.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self\n",
      "     |          Fitted estimator.\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      For non-sparse models, i.e. when there are not many zeros in ``coef_``,\n",
      "     |      this may actually *increase* memory usage, so use this method with\n",
      "     |      care. A rule of thumb is that the number of zero elements, which can\n",
      "     |      be computed with ``(coef_ == 0).sum()``, must be more than 50% for this\n",
      "     |      to provide significant benefits.\n",
      "     |      \n",
      "     |      After calling this method, further fitting with the partial_fit\n",
      "     |      method (if any) will not work until you call densify.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |  \n",
      "     |  __repr__(self, N_CHAR_MAX=700)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  get_params(self, deep=True)\n",
      "     |      Get parameters for this estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      deep : bool, default=True\n",
      "     |          If True, will return the parameters for this estimator and\n",
      "     |          contained subobjects that are estimators.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      params : dict\n",
      "     |          Parameter names mapped to their values.\n",
      "    \n",
      "    class Perceptron(sklearn.linear_model._stochastic_gradient.BaseSGDClassifier)\n",
      "     |  Perceptron(*, penalty=None, alpha=0.0001, l1_ratio=0.15, fit_intercept=True, max_iter=1000, tol=0.001, shuffle=True, verbose=0, eta0=1.0, n_jobs=None, random_state=0, early_stopping=False, validation_fraction=0.1, n_iter_no_change=5, class_weight=None, warm_start=False)\n",
      "     |  \n",
      "     |  Perceptron\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <perceptron>`.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  \n",
      "     |  penalty : {'l2','l1','elasticnet'}, default=None\n",
      "     |      The penalty (aka regularization term) to be used.\n",
      "     |  \n",
      "     |  alpha : float, default=0.0001\n",
      "     |      Constant that multiplies the regularization term if regularization is\n",
      "     |      used.\n",
      "     |  \n",
      "     |  l1_ratio : float, default=0.15\n",
      "     |      The Elastic Net mixing parameter, with `0 <= l1_ratio <= 1`.\n",
      "     |      `l1_ratio=0` corresponds to L2 penalty, `l1_ratio=1` to L1.\n",
      "     |      Only used if `penalty='elasticnet'`.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.24\n",
      "     |  \n",
      "     |  fit_intercept : bool, default=True\n",
      "     |      Whether the intercept should be estimated or not. If False, the\n",
      "     |      data is assumed to be already centered.\n",
      "     |  \n",
      "     |  max_iter : int, default=1000\n",
      "     |      The maximum number of passes over the training data (aka epochs).\n",
      "     |      It only impacts the behavior in the ``fit`` method, and not the\n",
      "     |      :meth:`partial_fit` method.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.19\n",
      "     |  \n",
      "     |  tol : float, default=1e-3\n",
      "     |      The stopping criterion. If it is not None, the iterations will stop\n",
      "     |      when (loss > previous_loss - tol).\n",
      "     |  \n",
      "     |      .. versionadded:: 0.19\n",
      "     |  \n",
      "     |  shuffle : bool, default=True\n",
      "     |      Whether or not the training data should be shuffled after each epoch.\n",
      "     |  \n",
      "     |  verbose : int, default=0\n",
      "     |      The verbosity level\n",
      "     |  \n",
      "     |  eta0 : double, default=1\n",
      "     |      Constant by which the updates are multiplied.\n",
      "     |  \n",
      "     |  n_jobs : int, default=None\n",
      "     |      The number of CPUs to use to do the OVA (One Versus All, for\n",
      "     |      multi-class problems) computation.\n",
      "     |      ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n",
      "     |      ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n",
      "     |      for more details.\n",
      "     |  \n",
      "     |  random_state : int, RandomState instance, default=None\n",
      "     |      Used to shuffle the training data, when ``shuffle`` is set to\n",
      "     |      ``True``. Pass an int for reproducible output across multiple\n",
      "     |      function calls.\n",
      "     |      See :term:`Glossary <random_state>`.\n",
      "     |  \n",
      "     |  early_stopping : bool, default=False\n",
      "     |      Whether to use early stopping to terminate training when validation.\n",
      "     |      score is not improving. If set to True, it will automatically set aside\n",
      "     |      a stratified fraction of training data as validation and terminate\n",
      "     |      training when validation score is not improving by at least tol for\n",
      "     |      n_iter_no_change consecutive epochs.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.20\n",
      "     |  \n",
      "     |  validation_fraction : float, default=0.1\n",
      "     |      The proportion of training data to set aside as validation set for\n",
      "     |      early stopping. Must be between 0 and 1.\n",
      "     |      Only used if early_stopping is True.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.20\n",
      "     |  \n",
      "     |  n_iter_no_change : int, default=5\n",
      "     |      Number of iterations with no improvement to wait before early stopping.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.20\n",
      "     |  \n",
      "     |  class_weight : dict, {class_label: weight} or \"balanced\", default=None\n",
      "     |      Preset for the class_weight fit parameter.\n",
      "     |  \n",
      "     |      Weights associated with classes. If not given, all classes\n",
      "     |      are supposed to have weight one.\n",
      "     |  \n",
      "     |      The \"balanced\" mode uses the values of y to automatically adjust\n",
      "     |      weights inversely proportional to class frequencies in the input data\n",
      "     |      as ``n_samples / (n_classes * np.bincount(y))``\n",
      "     |  \n",
      "     |  warm_start : bool, default=False\n",
      "     |      When set to True, reuse the solution of the previous call to fit as\n",
      "     |      initialization, otherwise, just erase the previous solution. See\n",
      "     |      :term:`the Glossary <warm_start>`.\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  classes_ : ndarray of shape (n_classes,)\n",
      "     |      The unique classes labels.\n",
      "     |  \n",
      "     |  coef_ : ndarray of shape (1, n_features) if n_classes == 2 else             (n_classes, n_features)\n",
      "     |      Weights assigned to the features.\n",
      "     |  \n",
      "     |  intercept_ : ndarray of shape (1,) if n_classes == 2 else (n_classes,)\n",
      "     |      Constants in decision function.\n",
      "     |  \n",
      "     |  loss_function_ : concrete LossFunction\n",
      "     |      The function that determines the loss, or difference between the\n",
      "     |      output of the algorithm and the target values.\n",
      "     |  \n",
      "     |  n_iter_ : int\n",
      "     |      The actual number of iterations to reach the stopping criterion.\n",
      "     |      For multiclass fits, it is the maximum over every binary fit.\n",
      "     |  \n",
      "     |  t_ : int\n",
      "     |      Number of weight updates performed during training.\n",
      "     |      Same as ``(n_iter_ * n_samples)``.\n",
      "     |  \n",
      "     |  Notes\n",
      "     |  -----\n",
      "     |  \n",
      "     |  ``Perceptron`` is a classification algorithm which shares the same\n",
      "     |  underlying implementation with ``SGDClassifier``. In fact,\n",
      "     |  ``Perceptron()`` is equivalent to `SGDClassifier(loss=\"perceptron\",\n",
      "     |  eta0=1, learning_rate=\"constant\", penalty=None)`.\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  >>> from sklearn.datasets import load_digits\n",
      "     |  >>> from sklearn.linear_model import Perceptron\n",
      "     |  >>> X, y = load_digits(return_X_y=True)\n",
      "     |  >>> clf = Perceptron(tol=1e-3, random_state=0)\n",
      "     |  >>> clf.fit(X, y)\n",
      "     |  Perceptron()\n",
      "     |  >>> clf.score(X, y)\n",
      "     |  0.939...\n",
      "     |  \n",
      "     |  See Also\n",
      "     |  --------\n",
      "     |  SGDClassifier\n",
      "     |  \n",
      "     |  References\n",
      "     |  ----------\n",
      "     |  \n",
      "     |  https://en.wikipedia.org/wiki/Perceptron and references therein.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      Perceptron\n",
      "     |      sklearn.linear_model._stochastic_gradient.BaseSGDClassifier\n",
      "     |      sklearn.linear_model._base.LinearClassifierMixin\n",
      "     |      sklearn.base.ClassifierMixin\n",
      "     |      sklearn.linear_model._stochastic_gradient.BaseSGD\n",
      "     |      sklearn.linear_model._base.SparseCoefMixin\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, *, penalty=None, alpha=0.0001, l1_ratio=0.15, fit_intercept=True, max_iter=1000, tol=0.001, shuffle=True, verbose=0, eta0=1.0, n_jobs=None, random_state=0, early_stopping=False, validation_fraction=0.1, n_iter_no_change=5, class_weight=None, warm_start=False)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.linear_model._stochastic_gradient.BaseSGDClassifier:\n",
      "     |  \n",
      "     |  fit(self, X, y, coef_init=None, intercept_init=None, sample_weight=None)\n",
      "     |      Fit linear model with Stochastic Gradient Descent.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix}, shape (n_samples, n_features)\n",
      "     |          Training data.\n",
      "     |      \n",
      "     |      y : ndarray of shape (n_samples,)\n",
      "     |          Target values.\n",
      "     |      \n",
      "     |      coef_init : ndarray of shape (n_classes, n_features), default=None\n",
      "     |          The initial coefficients to warm-start the optimization.\n",
      "     |      \n",
      "     |      intercept_init : ndarray of shape (n_classes,), default=None\n",
      "     |          The initial intercept to warm-start the optimization.\n",
      "     |      \n",
      "     |      sample_weight : array-like, shape (n_samples,), default=None\n",
      "     |          Weights applied to individual samples.\n",
      "     |          If not provided, uniform weights are assumed. These weights will\n",
      "     |          be multiplied with class_weight (passed through the\n",
      "     |          constructor) if class_weight is specified.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self :\n",
      "     |          Returns an instance of self.\n",
      "     |  \n",
      "     |  partial_fit(self, X, y, classes=None, sample_weight=None)\n",
      "     |      Perform one epoch of stochastic gradient descent on given samples.\n",
      "     |      \n",
      "     |      Internally, this method uses ``max_iter = 1``. Therefore, it is not\n",
      "     |      guaranteed that a minimum of the cost function is reached after calling\n",
      "     |      it once. Matters such as objective convergence and early stopping\n",
      "     |      should be handled by the user.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix}, shape (n_samples, n_features)\n",
      "     |          Subset of the training data.\n",
      "     |      \n",
      "     |      y : ndarray of shape (n_samples,)\n",
      "     |          Subset of the target values.\n",
      "     |      \n",
      "     |      classes : ndarray of shape (n_classes,), default=None\n",
      "     |          Classes across all calls to partial_fit.\n",
      "     |          Can be obtained by via `np.unique(y_all)`, where y_all is the\n",
      "     |          target vector of the entire dataset.\n",
      "     |          This argument is required for the first call to partial_fit\n",
      "     |          and can be omitted in the subsequent calls.\n",
      "     |          Note that y doesn't need to contain all labels in `classes`.\n",
      "     |      \n",
      "     |      sample_weight : array-like, shape (n_samples,), default=None\n",
      "     |          Weights applied to individual samples.\n",
      "     |          If not provided, uniform weights are assumed.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self :\n",
      "     |          Returns an instance of self.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from sklearn.linear_model._stochastic_gradient.BaseSGDClassifier:\n",
      "     |  \n",
      "     |  loss_functions = {'epsilon_insensitive': (<class 'sklearn.linear_model...\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.linear_model._base.LinearClassifierMixin:\n",
      "     |  \n",
      "     |  decision_function(self, X)\n",
      "     |      Predict confidence scores for samples.\n",
      "     |      \n",
      "     |      The confidence score for a sample is proportional to the signed\n",
      "     |      distance of that sample to the hyperplane.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like or sparse matrix, shape (n_samples, n_features)\n",
      "     |          Samples.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      array, shape=(n_samples,) if n_classes == 2 else (n_samples, n_classes)\n",
      "     |          Confidence scores per (sample, class) combination. In the binary\n",
      "     |          case, confidence score for self.classes_[1] where >0 means this\n",
      "     |          class would be predicted.\n",
      "     |  \n",
      "     |  predict(self, X)\n",
      "     |      Predict class labels for samples in X.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like or sparse matrix, shape (n_samples, n_features)\n",
      "     |          Samples.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      C : array, shape [n_samples]\n",
      "     |          Predicted class label per sample.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.ClassifierMixin:\n",
      "     |  \n",
      "     |  score(self, X, y, sample_weight=None)\n",
      "     |      Return the mean accuracy on the given test data and labels.\n",
      "     |      \n",
      "     |      In multi-label classification, this is the subset accuracy\n",
      "     |      which is a harsh metric since you require for each sample that\n",
      "     |      each label set be correctly predicted.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          Test samples.\n",
      "     |      \n",
      "     |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      "     |          True labels for `X`.\n",
      "     |      \n",
      "     |      sample_weight : array-like of shape (n_samples,), default=None\n",
      "     |          Sample weights.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      score : float\n",
      "     |          Mean accuracy of ``self.predict(X)`` wrt. `y`.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.base.ClassifierMixin:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.linear_model._stochastic_gradient.BaseSGD:\n",
      "     |  \n",
      "     |  set_params(self, **kwargs)\n",
      "     |      Set and validate the parameters of estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      **kwargs : dict\n",
      "     |          Estimator parameters.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |          Estimator instance.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties inherited from sklearn.linear_model._stochastic_gradient.BaseSGD:\n",
      "     |  \n",
      "     |  average_coef_\n",
      "     |  \n",
      "     |  average_intercept_\n",
      "     |  \n",
      "     |  standard_coef_\n",
      "     |  \n",
      "     |  standard_intercept_\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.linear_model._base.SparseCoefMixin:\n",
      "     |  \n",
      "     |  densify(self)\n",
      "     |      Convert coefficient matrix to dense array format.\n",
      "     |      \n",
      "     |      Converts the ``coef_`` member (back) to a numpy.ndarray. This is the\n",
      "     |      default format of ``coef_`` and is required for fitting, so calling\n",
      "     |      this method is only required on models that have previously been\n",
      "     |      sparsified; otherwise, it is a no-op.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self\n",
      "     |          Fitted estimator.\n",
      "     |  \n",
      "     |  sparsify(self)\n",
      "     |      Convert coefficient matrix to sparse format.\n",
      "     |      \n",
      "     |      Converts the ``coef_`` member to a scipy.sparse matrix, which for\n",
      "     |      L1-regularized models can be much more memory- and storage-efficient\n",
      "     |      than the usual numpy.ndarray representation.\n",
      "     |      \n",
      "     |      The ``intercept_`` member is not converted.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self\n",
      "     |          Fitted estimator.\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      For non-sparse models, i.e. when there are not many zeros in ``coef_``,\n",
      "     |      this may actually *increase* memory usage, so use this method with\n",
      "     |      care. A rule of thumb is that the number of zero elements, which can\n",
      "     |      be computed with ``(coef_ == 0).sum()``, must be more than 50% for this\n",
      "     |      to provide significant benefits.\n",
      "     |      \n",
      "     |      After calling this method, further fitting with the partial_fit\n",
      "     |      method (if any) will not work until you call densify.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |  \n",
      "     |  __repr__(self, N_CHAR_MAX=700)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  get_params(self, deep=True)\n",
      "     |      Get parameters for this estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      deep : bool, default=True\n",
      "     |          If True, will return the parameters for this estimator and\n",
      "     |          contained subobjects that are estimators.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      params : dict\n",
      "     |          Parameter names mapped to their values.\n",
      "    \n",
      "    class PoissonRegressor(GeneralizedLinearRegressor)\n",
      "     |  PoissonRegressor(*, alpha=1.0, fit_intercept=True, max_iter=100, tol=0.0001, warm_start=False, verbose=0)\n",
      "     |  \n",
      "     |  Generalized Linear Model with a Poisson distribution.\n",
      "     |  \n",
      "     |  This regressor uses the 'log' link function.\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <Generalized_linear_regression>`.\n",
      "     |  \n",
      "     |  .. versionadded:: 0.23\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  alpha : float, default=1\n",
      "     |      Constant that multiplies the penalty term and thus determines the\n",
      "     |      regularization strength. ``alpha = 0`` is equivalent to unpenalized\n",
      "     |      GLMs. In this case, the design matrix `X` must have full column rank\n",
      "     |      (no collinearities).\n",
      "     |  \n",
      "     |  fit_intercept : bool, default=True\n",
      "     |      Specifies if a constant (a.k.a. bias or intercept) should be\n",
      "     |      added to the linear predictor (X @ coef + intercept).\n",
      "     |  \n",
      "     |  max_iter : int, default=100\n",
      "     |      The maximal number of iterations for the solver.\n",
      "     |  \n",
      "     |  tol : float, default=1e-4\n",
      "     |      Stopping criterion. For the lbfgs solver,\n",
      "     |      the iteration will stop when ``max{|g_j|, j = 1, ..., d} <= tol``\n",
      "     |      where ``g_j`` is the j-th component of the gradient (derivative) of\n",
      "     |      the objective function.\n",
      "     |  \n",
      "     |  warm_start : bool, default=False\n",
      "     |      If set to ``True``, reuse the solution of the previous call to ``fit``\n",
      "     |      as initialization for ``coef_`` and ``intercept_`` .\n",
      "     |  \n",
      "     |  verbose : int, default=0\n",
      "     |      For the lbfgs solver set verbose to any positive number for verbosity.\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  coef_ : array of shape (n_features,)\n",
      "     |      Estimated coefficients for the linear predictor (`X @ coef_ +\n",
      "     |      intercept_`) in the GLM.\n",
      "     |  \n",
      "     |  intercept_ : float\n",
      "     |      Intercept (a.k.a. bias) added to linear predictor.\n",
      "     |  \n",
      "     |  n_iter_ : int\n",
      "     |      Actual number of iterations used in the solver.\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  ----------\n",
      "     |  >>> from sklearn import linear_model\n",
      "     |  >>> clf = linear_model.PoissonRegressor()\n",
      "     |  >>> X = [[1, 2], [2, 3], [3, 4], [4, 3]]\n",
      "     |  >>> y = [12, 17, 22, 21]\n",
      "     |  >>> clf.fit(X, y)\n",
      "     |  PoissonRegressor()\n",
      "     |  >>> clf.score(X, y)\n",
      "     |  0.990...\n",
      "     |  >>> clf.coef_\n",
      "     |  array([0.121..., 0.158...])\n",
      "     |  >>> clf.intercept_\n",
      "     |  2.088...\n",
      "     |  >>> clf.predict([[1, 1], [3, 4]])\n",
      "     |  array([10.676..., 21.875...])\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      PoissonRegressor\n",
      "     |      GeneralizedLinearRegressor\n",
      "     |      sklearn.base.RegressorMixin\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, *, alpha=1.0, fit_intercept=True, max_iter=100, tol=0.0001, warm_start=False, verbose=0)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  family\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from GeneralizedLinearRegressor:\n",
      "     |  \n",
      "     |  fit(self, X, y, sample_weight=None)\n",
      "     |      Fit a Generalized Linear Model.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          Training data.\n",
      "     |      \n",
      "     |      y : array-like of shape (n_samples,)\n",
      "     |          Target values.\n",
      "     |      \n",
      "     |      sample_weight : array-like of shape (n_samples,), default=None\n",
      "     |          Sample weights.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : returns an instance of self.\n",
      "     |  \n",
      "     |  predict(self, X)\n",
      "     |      Predict using GLM with feature matrix X.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          Samples.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      y_pred : array of shape (n_samples,)\n",
      "     |          Returns predicted values.\n",
      "     |  \n",
      "     |  score(self, X, y, sample_weight=None)\n",
      "     |      Compute D^2, the percentage of deviance explained.\n",
      "     |      \n",
      "     |      D^2 is a generalization of the coefficient of determination R^2.\n",
      "     |      R^2 uses squared error and D^2 deviance. Note that those two are equal\n",
      "     |      for ``family='normal'``.\n",
      "     |      \n",
      "     |      D^2 is defined as\n",
      "     |      :math:`D^2 = 1-\\frac{D(y_{true},y_{pred})}{D_{null}}`,\n",
      "     |      :math:`D_{null}` is the null deviance, i.e. the deviance of a model\n",
      "     |      with intercept alone, which corresponds to :math:`y_{pred} = \\bar{y}`.\n",
      "     |      The mean :math:`\\bar{y}` is averaged by sample_weight.\n",
      "     |      Best possible score is 1.0 and it can be negative (because the model\n",
      "     |      can be arbitrarily worse).\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          Test samples.\n",
      "     |      \n",
      "     |      y : array-like of shape (n_samples,)\n",
      "     |          True values of target.\n",
      "     |      \n",
      "     |      sample_weight : array-like of shape (n_samples,), default=None\n",
      "     |          Sample weights.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      score : float\n",
      "     |          D^2 of self.predict(X) w.r.t. y.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.base.RegressorMixin:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |  \n",
      "     |  __repr__(self, N_CHAR_MAX=700)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  get_params(self, deep=True)\n",
      "     |      Get parameters for this estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      deep : bool, default=True\n",
      "     |          If True, will return the parameters for this estimator and\n",
      "     |          contained subobjects that are estimators.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      params : dict\n",
      "     |          Parameter names mapped to their values.\n",
      "     |  \n",
      "     |  set_params(self, **params)\n",
      "     |      Set the parameters of this estimator.\n",
      "     |      \n",
      "     |      The method works on simple estimators as well as on nested objects\n",
      "     |      (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n",
      "     |      parameters of the form ``<component>__<parameter>`` so that it's\n",
      "     |      possible to update each component of a nested object.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      **params : dict\n",
      "     |          Estimator parameters.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : estimator instance\n",
      "     |          Estimator instance.\n",
      "    \n",
      "    class RANSACRegressor(sklearn.base.MetaEstimatorMixin, sklearn.base.RegressorMixin, sklearn.base.MultiOutputMixin, sklearn.base.BaseEstimator)\n",
      "     |  RANSACRegressor(base_estimator=None, *, min_samples=None, residual_threshold=None, is_data_valid=None, is_model_valid=None, max_trials=100, max_skips=inf, stop_n_inliers=inf, stop_score=inf, stop_probability=0.99, loss='absolute_loss', random_state=None)\n",
      "     |  \n",
      "     |  RANSAC (RANdom SAmple Consensus) algorithm.\n",
      "     |  \n",
      "     |  RANSAC is an iterative algorithm for the robust estimation of parameters\n",
      "     |  from a subset of inliers from the complete data set.\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <ransac_regression>`.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  base_estimator : object, default=None\n",
      "     |      Base estimator object which implements the following methods:\n",
      "     |  \n",
      "     |       * `fit(X, y)`: Fit model to given training data and target values.\n",
      "     |       * `score(X, y)`: Returns the mean accuracy on the given test data,\n",
      "     |         which is used for the stop criterion defined by `stop_score`.\n",
      "     |         Additionally, the score is used to decide which of two equally\n",
      "     |         large consensus sets is chosen as the better one.\n",
      "     |       * `predict(X)`: Returns predicted values using the linear model,\n",
      "     |         which is used to compute residual error using loss function.\n",
      "     |  \n",
      "     |      If `base_estimator` is None, then\n",
      "     |      :class:`~sklearn.linear_model.LinearRegression` is used for\n",
      "     |      target values of dtype float.\n",
      "     |  \n",
      "     |      Note that the current implementation only supports regression\n",
      "     |      estimators.\n",
      "     |  \n",
      "     |  min_samples : int (>= 1) or float ([0, 1]), default=None\n",
      "     |      Minimum number of samples chosen randomly from original data. Treated\n",
      "     |      as an absolute number of samples for `min_samples >= 1`, treated as a\n",
      "     |      relative number `ceil(min_samples * X.shape[0]`) for\n",
      "     |      `min_samples < 1`. This is typically chosen as the minimal number of\n",
      "     |      samples necessary to estimate the given `base_estimator`. By default a\n",
      "     |      ``sklearn.linear_model.LinearRegression()`` estimator is assumed and\n",
      "     |      `min_samples` is chosen as ``X.shape[1] + 1``.\n",
      "     |  \n",
      "     |  residual_threshold : float, default=None\n",
      "     |      Maximum residual for a data sample to be classified as an inlier.\n",
      "     |      By default the threshold is chosen as the MAD (median absolute\n",
      "     |      deviation) of the target values `y`.\n",
      "     |  \n",
      "     |  is_data_valid : callable, default=None\n",
      "     |      This function is called with the randomly selected data before the\n",
      "     |      model is fitted to it: `is_data_valid(X, y)`. If its return value is\n",
      "     |      False the current randomly chosen sub-sample is skipped.\n",
      "     |  \n",
      "     |  is_model_valid : callable, default=None\n",
      "     |      This function is called with the estimated model and the randomly\n",
      "     |      selected data: `is_model_valid(model, X, y)`. If its return value is\n",
      "     |      False the current randomly chosen sub-sample is skipped.\n",
      "     |      Rejecting samples with this function is computationally costlier than\n",
      "     |      with `is_data_valid`. `is_model_valid` should therefore only be used if\n",
      "     |      the estimated model is needed for making the rejection decision.\n",
      "     |  \n",
      "     |  max_trials : int, default=100\n",
      "     |      Maximum number of iterations for random sample selection.\n",
      "     |  \n",
      "     |  max_skips : int, default=np.inf\n",
      "     |      Maximum number of iterations that can be skipped due to finding zero\n",
      "     |      inliers or invalid data defined by ``is_data_valid`` or invalid models\n",
      "     |      defined by ``is_model_valid``.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.19\n",
      "     |  \n",
      "     |  stop_n_inliers : int, default=np.inf\n",
      "     |      Stop iteration if at least this number of inliers are found.\n",
      "     |  \n",
      "     |  stop_score : float, default=np.inf\n",
      "     |      Stop iteration if score is greater equal than this threshold.\n",
      "     |  \n",
      "     |  stop_probability : float in range [0, 1], default=0.99\n",
      "     |      RANSAC iteration stops if at least one outlier-free set of the training\n",
      "     |      data is sampled in RANSAC. This requires to generate at least N\n",
      "     |      samples (iterations)::\n",
      "     |  \n",
      "     |          N >= log(1 - probability) / log(1 - e**m)\n",
      "     |  \n",
      "     |      where the probability (confidence) is typically set to high value such\n",
      "     |      as 0.99 (the default) and e is the current fraction of inliers w.r.t.\n",
      "     |      the total number of samples.\n",
      "     |  \n",
      "     |  loss : string, callable, default='absolute_loss'\n",
      "     |      String inputs, \"absolute_loss\" and \"squared_loss\" are supported which\n",
      "     |      find the absolute loss and squared loss per sample\n",
      "     |      respectively.\n",
      "     |  \n",
      "     |      If ``loss`` is a callable, then it should be a function that takes\n",
      "     |      two arrays as inputs, the true and predicted value and returns a 1-D\n",
      "     |      array with the i-th value of the array corresponding to the loss\n",
      "     |      on ``X[i]``.\n",
      "     |  \n",
      "     |      If the loss on a sample is greater than the ``residual_threshold``,\n",
      "     |      then this sample is classified as an outlier.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.18\n",
      "     |  \n",
      "     |  random_state : int, RandomState instance, default=None\n",
      "     |      The generator used to initialize the centers.\n",
      "     |      Pass an int for reproducible output across multiple function calls.\n",
      "     |      See :term:`Glossary <random_state>`.\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  estimator_ : object\n",
      "     |      Best fitted model (copy of the `base_estimator` object).\n",
      "     |  \n",
      "     |  n_trials_ : int\n",
      "     |      Number of random selection trials until one of the stop criteria is\n",
      "     |      met. It is always ``<= max_trials``.\n",
      "     |  \n",
      "     |  inlier_mask_ : bool array of shape [n_samples]\n",
      "     |      Boolean mask of inliers classified as ``True``.\n",
      "     |  \n",
      "     |  n_skips_no_inliers_ : int\n",
      "     |      Number of iterations skipped due to finding zero inliers.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.19\n",
      "     |  \n",
      "     |  n_skips_invalid_data_ : int\n",
      "     |      Number of iterations skipped due to invalid data defined by\n",
      "     |      ``is_data_valid``.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.19\n",
      "     |  \n",
      "     |  n_skips_invalid_model_ : int\n",
      "     |      Number of iterations skipped due to an invalid model defined by\n",
      "     |      ``is_model_valid``.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.19\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  >>> from sklearn.linear_model import RANSACRegressor\n",
      "     |  >>> from sklearn.datasets import make_regression\n",
      "     |  >>> X, y = make_regression(\n",
      "     |  ...     n_samples=200, n_features=2, noise=4.0, random_state=0)\n",
      "     |  >>> reg = RANSACRegressor(random_state=0).fit(X, y)\n",
      "     |  >>> reg.score(X, y)\n",
      "     |  0.9885...\n",
      "     |  >>> reg.predict(X[:1,])\n",
      "     |  array([-31.9417...])\n",
      "     |  \n",
      "     |  References\n",
      "     |  ----------\n",
      "     |  .. [1] https://en.wikipedia.org/wiki/RANSAC\n",
      "     |  .. [2] https://www.sri.com/sites/default/files/publications/ransac-publication.pdf\n",
      "     |  .. [3] http://www.bmva.org/bmvc/2009/Papers/Paper355/Paper355.pdf\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      RANSACRegressor\n",
      "     |      sklearn.base.MetaEstimatorMixin\n",
      "     |      sklearn.base.RegressorMixin\n",
      "     |      sklearn.base.MultiOutputMixin\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, base_estimator=None, *, min_samples=None, residual_threshold=None, is_data_valid=None, is_model_valid=None, max_trials=100, max_skips=inf, stop_n_inliers=inf, stop_score=inf, stop_probability=0.99, loss='absolute_loss', random_state=None)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  fit(self, X, y, sample_weight=None)\n",
      "     |      Fit estimator using RANSAC algorithm.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like or sparse matrix, shape [n_samples, n_features]\n",
      "     |          Training data.\n",
      "     |      \n",
      "     |      y : array-like of shape (n_samples,) or (n_samples, n_targets)\n",
      "     |          Target values.\n",
      "     |      \n",
      "     |      sample_weight : array-like of shape (n_samples,), default=None\n",
      "     |          Individual weights for each sample\n",
      "     |          raises error if sample_weight is passed and base_estimator\n",
      "     |          fit method does not support it.\n",
      "     |      \n",
      "     |          .. versionadded:: 0.18\n",
      "     |      \n",
      "     |      Raises\n",
      "     |      ------\n",
      "     |      ValueError\n",
      "     |          If no valid consensus set could be found. This occurs if\n",
      "     |          `is_data_valid` and `is_model_valid` return False for all\n",
      "     |          `max_trials` randomly chosen sub-samples.\n",
      "     |  \n",
      "     |  predict(self, X)\n",
      "     |      Predict using the estimated model.\n",
      "     |      \n",
      "     |      This is a wrapper for `estimator_.predict(X)`.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : numpy array of shape [n_samples, n_features]\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      y : array, shape = [n_samples] or [n_samples, n_targets]\n",
      "     |          Returns predicted values.\n",
      "     |  \n",
      "     |  score(self, X, y)\n",
      "     |      Returns the score of the prediction.\n",
      "     |      \n",
      "     |      This is a wrapper for `estimator_.score(X, y)`.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : numpy array or sparse matrix of shape [n_samples, n_features]\n",
      "     |          Training data.\n",
      "     |      \n",
      "     |      y : array, shape = [n_samples] or [n_samples, n_targets]\n",
      "     |          Target values.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      z : float\n",
      "     |          Score of the prediction.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.base.MetaEstimatorMixin:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |  \n",
      "     |  __repr__(self, N_CHAR_MAX=700)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  get_params(self, deep=True)\n",
      "     |      Get parameters for this estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      deep : bool, default=True\n",
      "     |          If True, will return the parameters for this estimator and\n",
      "     |          contained subobjects that are estimators.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      params : dict\n",
      "     |          Parameter names mapped to their values.\n",
      "     |  \n",
      "     |  set_params(self, **params)\n",
      "     |      Set the parameters of this estimator.\n",
      "     |      \n",
      "     |      The method works on simple estimators as well as on nested objects\n",
      "     |      (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n",
      "     |      parameters of the form ``<component>__<parameter>`` so that it's\n",
      "     |      possible to update each component of a nested object.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      **params : dict\n",
      "     |          Estimator parameters.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : estimator instance\n",
      "     |          Estimator instance.\n",
      "    \n",
      "    class Ridge(sklearn.base.MultiOutputMixin, sklearn.base.RegressorMixin, _BaseRidge)\n",
      "     |  Ridge(alpha=1.0, *, fit_intercept=True, normalize=False, copy_X=True, max_iter=None, tol=0.001, solver='auto', random_state=None)\n",
      "     |  \n",
      "     |  Linear least squares with l2 regularization.\n",
      "     |  \n",
      "     |  Minimizes the objective function::\n",
      "     |  \n",
      "     |  ||y - Xw||^2_2 + alpha * ||w||^2_2\n",
      "     |  \n",
      "     |  This model solves a regression model where the loss function is\n",
      "     |  the linear least squares function and regularization is given by\n",
      "     |  the l2-norm. Also known as Ridge Regression or Tikhonov regularization.\n",
      "     |  This estimator has built-in support for multi-variate regression\n",
      "     |  (i.e., when y is a 2d-array of shape (n_samples, n_targets)).\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <ridge_regression>`.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  alpha : {float, ndarray of shape (n_targets,)}, default=1.0\n",
      "     |      Regularization strength; must be a positive float. Regularization\n",
      "     |      improves the conditioning of the problem and reduces the variance of\n",
      "     |      the estimates. Larger values specify stronger regularization.\n",
      "     |      Alpha corresponds to ``1 / (2C)`` in other linear models such as\n",
      "     |      :class:`~sklearn.linear_model.LogisticRegression` or\n",
      "     |      :class:`~sklearn.svm.LinearSVC`. If an array is passed, penalties are\n",
      "     |      assumed to be specific to the targets. Hence they must correspond in\n",
      "     |      number.\n",
      "     |  \n",
      "     |  fit_intercept : bool, default=True\n",
      "     |      Whether to fit the intercept for this model. If set\n",
      "     |      to false, no intercept will be used in calculations\n",
      "     |      (i.e. ``X`` and ``y`` are expected to be centered).\n",
      "     |  \n",
      "     |  normalize : bool, default=False\n",
      "     |      This parameter is ignored when ``fit_intercept`` is set to False.\n",
      "     |      If True, the regressors X will be normalized before regression by\n",
      "     |      subtracting the mean and dividing by the l2-norm.\n",
      "     |      If you wish to standardize, please use\n",
      "     |      :class:`~sklearn.preprocessing.StandardScaler` before calling ``fit``\n",
      "     |      on an estimator with ``normalize=False``.\n",
      "     |  \n",
      "     |  copy_X : bool, default=True\n",
      "     |      If True, X will be copied; else, it may be overwritten.\n",
      "     |  \n",
      "     |  max_iter : int, default=None\n",
      "     |      Maximum number of iterations for conjugate gradient solver.\n",
      "     |      For 'sparse_cg' and 'lsqr' solvers, the default value is determined\n",
      "     |      by scipy.sparse.linalg. For 'sag' solver, the default value is 1000.\n",
      "     |  \n",
      "     |  tol : float, default=1e-3\n",
      "     |      Precision of the solution.\n",
      "     |  \n",
      "     |  solver : {'auto', 'svd', 'cholesky', 'lsqr', 'sparse_cg', 'sag', 'saga'},         default='auto'\n",
      "     |      Solver to use in the computational routines:\n",
      "     |  \n",
      "     |      - 'auto' chooses the solver automatically based on the type of data.\n",
      "     |  \n",
      "     |      - 'svd' uses a Singular Value Decomposition of X to compute the Ridge\n",
      "     |        coefficients. More stable for singular matrices than 'cholesky'.\n",
      "     |  \n",
      "     |      - 'cholesky' uses the standard scipy.linalg.solve function to\n",
      "     |        obtain a closed-form solution.\n",
      "     |  \n",
      "     |      - 'sparse_cg' uses the conjugate gradient solver as found in\n",
      "     |        scipy.sparse.linalg.cg. As an iterative algorithm, this solver is\n",
      "     |        more appropriate than 'cholesky' for large-scale data\n",
      "     |        (possibility to set `tol` and `max_iter`).\n",
      "     |  \n",
      "     |      - 'lsqr' uses the dedicated regularized least-squares routine\n",
      "     |        scipy.sparse.linalg.lsqr. It is the fastest and uses an iterative\n",
      "     |        procedure.\n",
      "     |  \n",
      "     |      - 'sag' uses a Stochastic Average Gradient descent, and 'saga' uses\n",
      "     |        its improved, unbiased version named SAGA. Both methods also use an\n",
      "     |        iterative procedure, and are often faster than other solvers when\n",
      "     |        both n_samples and n_features are large. Note that 'sag' and\n",
      "     |        'saga' fast convergence is only guaranteed on features with\n",
      "     |        approximately the same scale. You can preprocess the data with a\n",
      "     |        scaler from sklearn.preprocessing.\n",
      "     |  \n",
      "     |      All last five solvers support both dense and sparse data. However, only\n",
      "     |      'sag' and 'sparse_cg' supports sparse input when `fit_intercept` is\n",
      "     |      True.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.17\n",
      "     |         Stochastic Average Gradient descent solver.\n",
      "     |      .. versionadded:: 0.19\n",
      "     |         SAGA solver.\n",
      "     |  \n",
      "     |  random_state : int, RandomState instance, default=None\n",
      "     |      Used when ``solver`` == 'sag' or 'saga' to shuffle the data.\n",
      "     |      See :term:`Glossary <random_state>` for details.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.17\n",
      "     |         `random_state` to support Stochastic Average Gradient.\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  coef_ : ndarray of shape (n_features,) or (n_targets, n_features)\n",
      "     |      Weight vector(s).\n",
      "     |  \n",
      "     |  intercept_ : float or ndarray of shape (n_targets,)\n",
      "     |      Independent term in decision function. Set to 0.0 if\n",
      "     |      ``fit_intercept = False``.\n",
      "     |  \n",
      "     |  n_iter_ : None or ndarray of shape (n_targets,)\n",
      "     |      Actual number of iterations for each target. Available only for\n",
      "     |      sag and lsqr solvers. Other solvers will return None.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.17\n",
      "     |  \n",
      "     |  See Also\n",
      "     |  --------\n",
      "     |  RidgeClassifier : Ridge classifier.\n",
      "     |  RidgeCV : Ridge regression with built-in cross validation.\n",
      "     |  :class:`~sklearn.kernel_ridge.KernelRidge` : Kernel ridge regression\n",
      "     |      combines ridge regression with the kernel trick.\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  >>> from sklearn.linear_model import Ridge\n",
      "     |  >>> import numpy as np\n",
      "     |  >>> n_samples, n_features = 10, 5\n",
      "     |  >>> rng = np.random.RandomState(0)\n",
      "     |  >>> y = rng.randn(n_samples)\n",
      "     |  >>> X = rng.randn(n_samples, n_features)\n",
      "     |  >>> clf = Ridge(alpha=1.0)\n",
      "     |  >>> clf.fit(X, y)\n",
      "     |  Ridge()\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      Ridge\n",
      "     |      sklearn.base.MultiOutputMixin\n",
      "     |      sklearn.base.RegressorMixin\n",
      "     |      _BaseRidge\n",
      "     |      sklearn.linear_model._base.LinearModel\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, alpha=1.0, *, fit_intercept=True, normalize=False, copy_X=True, max_iter=None, tol=0.001, solver='auto', random_state=None)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  fit(self, X, y, sample_weight=None)\n",
      "     |      Fit Ridge regression model.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {ndarray, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          Training data\n",
      "     |      \n",
      "     |      y : ndarray of shape (n_samples,) or (n_samples, n_targets)\n",
      "     |          Target values\n",
      "     |      \n",
      "     |      sample_weight : float or ndarray of shape (n_samples,), default=None\n",
      "     |          Individual weights for each sample. If given a float, every sample\n",
      "     |          will have the same weight.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : returns an instance of self.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.base.MultiOutputMixin:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.RegressorMixin:\n",
      "     |  \n",
      "     |  score(self, X, y, sample_weight=None)\n",
      "     |      Return the coefficient of determination :math:`R^2` of the\n",
      "     |      prediction.\n",
      "     |      \n",
      "     |      The coefficient :math:`R^2` is defined as :math:`(1 - \\frac{u}{v})`,\n",
      "     |      where :math:`u` is the residual sum of squares ``((y_true - y_pred)\n",
      "     |      ** 2).sum()`` and :math:`v` is the total sum of squares ``((y_true -\n",
      "     |      y_true.mean()) ** 2).sum()``. The best possible score is 1.0 and it\n",
      "     |      can be negative (because the model can be arbitrarily worse). A\n",
      "     |      constant model that always predicts the expected value of `y`,\n",
      "     |      disregarding the input features, would get a :math:`R^2` score of\n",
      "     |      0.0.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          Test samples. For some estimators this may be a precomputed\n",
      "     |          kernel matrix or a list of generic objects instead with shape\n",
      "     |          ``(n_samples, n_samples_fitted)``, where ``n_samples_fitted``\n",
      "     |          is the number of samples used in the fitting for the estimator.\n",
      "     |      \n",
      "     |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      "     |          True values for `X`.\n",
      "     |      \n",
      "     |      sample_weight : array-like of shape (n_samples,), default=None\n",
      "     |          Sample weights.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      score : float\n",
      "     |          :math:`R^2` of ``self.predict(X)`` wrt. `y`.\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      The :math:`R^2` score used when calling ``score`` on a regressor uses\n",
      "     |      ``multioutput='uniform_average'`` from version 0.23 to keep consistent\n",
      "     |      with default value of :func:`~sklearn.metrics.r2_score`.\n",
      "     |      This influences the ``score`` method of all the multioutput\n",
      "     |      regressors (except for\n",
      "     |      :class:`~sklearn.multioutput.MultiOutputRegressor`).\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.linear_model._base.LinearModel:\n",
      "     |  \n",
      "     |  predict(self, X)\n",
      "     |      Predict using the linear model.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like or sparse matrix, shape (n_samples, n_features)\n",
      "     |          Samples.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      C : array, shape (n_samples,)\n",
      "     |          Returns predicted values.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |  \n",
      "     |  __repr__(self, N_CHAR_MAX=700)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  get_params(self, deep=True)\n",
      "     |      Get parameters for this estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      deep : bool, default=True\n",
      "     |          If True, will return the parameters for this estimator and\n",
      "     |          contained subobjects that are estimators.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      params : dict\n",
      "     |          Parameter names mapped to their values.\n",
      "     |  \n",
      "     |  set_params(self, **params)\n",
      "     |      Set the parameters of this estimator.\n",
      "     |      \n",
      "     |      The method works on simple estimators as well as on nested objects\n",
      "     |      (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n",
      "     |      parameters of the form ``<component>__<parameter>`` so that it's\n",
      "     |      possible to update each component of a nested object.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      **params : dict\n",
      "     |          Estimator parameters.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : estimator instance\n",
      "     |          Estimator instance.\n",
      "    \n",
      "    class RidgeCV(sklearn.base.MultiOutputMixin, sklearn.base.RegressorMixin, _BaseRidgeCV)\n",
      "     |  RidgeCV(alphas=(0.1, 1.0, 10.0), *, fit_intercept=True, normalize=False, scoring=None, cv=None, gcv_mode=None, store_cv_values=False, alpha_per_target=False)\n",
      "     |  \n",
      "     |  Ridge regression with built-in cross-validation.\n",
      "     |  \n",
      "     |  See glossary entry for :term:`cross-validation estimator`.\n",
      "     |  \n",
      "     |  By default, it performs efficient Leave-One-Out Cross-Validation.\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <ridge_regression>`.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  alphas : ndarray of shape (n_alphas,), default=(0.1, 1.0, 10.0)\n",
      "     |      Array of alpha values to try.\n",
      "     |      Regularization strength; must be a positive float. Regularization\n",
      "     |      improves the conditioning of the problem and reduces the variance of\n",
      "     |      the estimates. Larger values specify stronger regularization.\n",
      "     |      Alpha corresponds to ``1 / (2C)`` in other linear models such as\n",
      "     |      :class:`~sklearn.linear_model.LogisticRegression` or\n",
      "     |      :class:`~sklearn.svm.LinearSVC`.\n",
      "     |      If using Leave-One-Out cross-validation, alphas must be positive.\n",
      "     |  \n",
      "     |  fit_intercept : bool, default=True\n",
      "     |      Whether to calculate the intercept for this model. If set\n",
      "     |      to false, no intercept will be used in calculations\n",
      "     |      (i.e. data is expected to be centered).\n",
      "     |  \n",
      "     |  normalize : bool, default=False\n",
      "     |      This parameter is ignored when ``fit_intercept`` is set to False.\n",
      "     |      If True, the regressors X will be normalized before regression by\n",
      "     |      subtracting the mean and dividing by the l2-norm.\n",
      "     |      If you wish to standardize, please use\n",
      "     |      :class:`~sklearn.preprocessing.StandardScaler` before calling ``fit``\n",
      "     |      on an estimator with ``normalize=False``.\n",
      "     |  \n",
      "     |  scoring : string, callable, default=None\n",
      "     |      A string (see model evaluation documentation) or\n",
      "     |      a scorer callable object / function with signature\n",
      "     |      ``scorer(estimator, X, y)``.\n",
      "     |      If None, the negative mean squared error if cv is 'auto' or None\n",
      "     |      (i.e. when using leave-one-out cross-validation), and r2 score\n",
      "     |      otherwise.\n",
      "     |  \n",
      "     |  cv : int, cross-validation generator or an iterable, default=None\n",
      "     |      Determines the cross-validation splitting strategy.\n",
      "     |      Possible inputs for cv are:\n",
      "     |  \n",
      "     |      - None, to use the efficient Leave-One-Out cross-validation\n",
      "     |      - integer, to specify the number of folds.\n",
      "     |      - :term:`CV splitter`,\n",
      "     |      - An iterable yielding (train, test) splits as arrays of indices.\n",
      "     |  \n",
      "     |      For integer/None inputs, if ``y`` is binary or multiclass,\n",
      "     |      :class:`~sklearn.model_selection.StratifiedKFold` is used, else,\n",
      "     |      :class:`~sklearn.model_selection.KFold` is used.\n",
      "     |  \n",
      "     |      Refer :ref:`User Guide <cross_validation>` for the various\n",
      "     |      cross-validation strategies that can be used here.\n",
      "     |  \n",
      "     |  gcv_mode : {'auto', 'svd', eigen'}, default='auto'\n",
      "     |      Flag indicating which strategy to use when performing\n",
      "     |      Leave-One-Out Cross-Validation. Options are::\n",
      "     |  \n",
      "     |          'auto' : use 'svd' if n_samples > n_features, otherwise use 'eigen'\n",
      "     |          'svd' : force use of singular value decomposition of X when X is\n",
      "     |              dense, eigenvalue decomposition of X^T.X when X is sparse.\n",
      "     |          'eigen' : force computation via eigendecomposition of X.X^T\n",
      "     |  \n",
      "     |      The 'auto' mode is the default and is intended to pick the cheaper\n",
      "     |      option of the two depending on the shape of the training data.\n",
      "     |  \n",
      "     |  store_cv_values : bool, default=False\n",
      "     |      Flag indicating if the cross-validation values corresponding to\n",
      "     |      each alpha should be stored in the ``cv_values_`` attribute (see\n",
      "     |      below). This flag is only compatible with ``cv=None`` (i.e. using\n",
      "     |      Leave-One-Out Cross-Validation).\n",
      "     |  \n",
      "     |  alpha_per_target : bool, default=False\n",
      "     |      Flag indicating whether to optimize the alpha value (picked from the\n",
      "     |      `alphas` parameter list) for each target separately (for multi-output\n",
      "     |      settings: multiple prediction targets). When set to `True`, after\n",
      "     |      fitting, the `alpha_` attribute will contain a value for each target.\n",
      "     |      When set to `False`, a single alpha is used for all targets.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.24\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  cv_values_ : ndarray of shape (n_samples, n_alphas) or         shape (n_samples, n_targets, n_alphas), optional\n",
      "     |      Cross-validation values for each alpha (only available if\n",
      "     |      ``store_cv_values=True`` and ``cv=None``). After ``fit()`` has been\n",
      "     |      called, this attribute will contain the mean squared errors\n",
      "     |      (by default) or the values of the ``{loss,score}_func`` function\n",
      "     |      (if provided in the constructor).\n",
      "     |  \n",
      "     |  coef_ : ndarray of shape (n_features) or (n_targets, n_features)\n",
      "     |      Weight vector(s).\n",
      "     |  \n",
      "     |  intercept_ : float or ndarray of shape (n_targets,)\n",
      "     |      Independent term in decision function. Set to 0.0 if\n",
      "     |      ``fit_intercept = False``.\n",
      "     |  \n",
      "     |  alpha_ : float or ndarray of shape (n_targets,)\n",
      "     |      Estimated regularization parameter, or, if ``alpha_per_target=True``,\n",
      "     |      the estimated regularization parameter for each target.\n",
      "     |  \n",
      "     |  best_score_ : float or ndarray of shape (n_targets,)\n",
      "     |      Score of base estimator with best alpha, or, if\n",
      "     |      ``alpha_per_target=True``, a score for each target.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.23\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  >>> from sklearn.datasets import load_diabetes\n",
      "     |  >>> from sklearn.linear_model import RidgeCV\n",
      "     |  >>> X, y = load_diabetes(return_X_y=True)\n",
      "     |  >>> clf = RidgeCV(alphas=[1e-3, 1e-2, 1e-1, 1]).fit(X, y)\n",
      "     |  >>> clf.score(X, y)\n",
      "     |  0.5166...\n",
      "     |  \n",
      "     |  See Also\n",
      "     |  --------\n",
      "     |  Ridge : Ridge regression.\n",
      "     |  RidgeClassifier : Ridge classifier.\n",
      "     |  RidgeClassifierCV : Ridge classifier with built-in cross validation.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      RidgeCV\n",
      "     |      sklearn.base.MultiOutputMixin\n",
      "     |      sklearn.base.RegressorMixin\n",
      "     |      _BaseRidgeCV\n",
      "     |      sklearn.linear_model._base.LinearModel\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.base.MultiOutputMixin:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.RegressorMixin:\n",
      "     |  \n",
      "     |  score(self, X, y, sample_weight=None)\n",
      "     |      Return the coefficient of determination :math:`R^2` of the\n",
      "     |      prediction.\n",
      "     |      \n",
      "     |      The coefficient :math:`R^2` is defined as :math:`(1 - \\frac{u}{v})`,\n",
      "     |      where :math:`u` is the residual sum of squares ``((y_true - y_pred)\n",
      "     |      ** 2).sum()`` and :math:`v` is the total sum of squares ``((y_true -\n",
      "     |      y_true.mean()) ** 2).sum()``. The best possible score is 1.0 and it\n",
      "     |      can be negative (because the model can be arbitrarily worse). A\n",
      "     |      constant model that always predicts the expected value of `y`,\n",
      "     |      disregarding the input features, would get a :math:`R^2` score of\n",
      "     |      0.0.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          Test samples. For some estimators this may be a precomputed\n",
      "     |          kernel matrix or a list of generic objects instead with shape\n",
      "     |          ``(n_samples, n_samples_fitted)``, where ``n_samples_fitted``\n",
      "     |          is the number of samples used in the fitting for the estimator.\n",
      "     |      \n",
      "     |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      "     |          True values for `X`.\n",
      "     |      \n",
      "     |      sample_weight : array-like of shape (n_samples,), default=None\n",
      "     |          Sample weights.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      score : float\n",
      "     |          :math:`R^2` of ``self.predict(X)`` wrt. `y`.\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      The :math:`R^2` score used when calling ``score`` on a regressor uses\n",
      "     |      ``multioutput='uniform_average'`` from version 0.23 to keep consistent\n",
      "     |      with default value of :func:`~sklearn.metrics.r2_score`.\n",
      "     |      This influences the ``score`` method of all the multioutput\n",
      "     |      regressors (except for\n",
      "     |      :class:`~sklearn.multioutput.MultiOutputRegressor`).\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from _BaseRidgeCV:\n",
      "     |  \n",
      "     |  __init__(self, alphas=(0.1, 1.0, 10.0), *, fit_intercept=True, normalize=False, scoring=None, cv=None, gcv_mode=None, store_cv_values=False, alpha_per_target=False)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  fit(self, X, y, sample_weight=None)\n",
      "     |      Fit Ridge regression model with cv.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : ndarray of shape (n_samples, n_features)\n",
      "     |          Training data. If using GCV, will be cast to float64\n",
      "     |          if necessary.\n",
      "     |      \n",
      "     |      y : ndarray of shape (n_samples,) or (n_samples, n_targets)\n",
      "     |          Target values. Will be cast to X's dtype if necessary.\n",
      "     |      \n",
      "     |      sample_weight : float or ndarray of shape (n_samples,), default=None\n",
      "     |          Individual weights for each sample. If given a float, every sample\n",
      "     |          will have the same weight.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      When sample_weight is provided, the selected hyperparameter may depend\n",
      "     |      on whether we use leave-one-out cross-validation (cv=None or cv='auto')\n",
      "     |      or another form of cross-validation, because only leave-one-out\n",
      "     |      cross-validation takes the sample weights into account when computing\n",
      "     |      the validation score.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.linear_model._base.LinearModel:\n",
      "     |  \n",
      "     |  predict(self, X)\n",
      "     |      Predict using the linear model.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like or sparse matrix, shape (n_samples, n_features)\n",
      "     |          Samples.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      C : array, shape (n_samples,)\n",
      "     |          Returns predicted values.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |  \n",
      "     |  __repr__(self, N_CHAR_MAX=700)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  get_params(self, deep=True)\n",
      "     |      Get parameters for this estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      deep : bool, default=True\n",
      "     |          If True, will return the parameters for this estimator and\n",
      "     |          contained subobjects that are estimators.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      params : dict\n",
      "     |          Parameter names mapped to their values.\n",
      "     |  \n",
      "     |  set_params(self, **params)\n",
      "     |      Set the parameters of this estimator.\n",
      "     |      \n",
      "     |      The method works on simple estimators as well as on nested objects\n",
      "     |      (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n",
      "     |      parameters of the form ``<component>__<parameter>`` so that it's\n",
      "     |      possible to update each component of a nested object.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      **params : dict\n",
      "     |          Estimator parameters.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : estimator instance\n",
      "     |          Estimator instance.\n",
      "    \n",
      "    class RidgeClassifier(sklearn.linear_model._base.LinearClassifierMixin, _BaseRidge)\n",
      "     |  RidgeClassifier(alpha=1.0, *, fit_intercept=True, normalize=False, copy_X=True, max_iter=None, tol=0.001, class_weight=None, solver='auto', random_state=None)\n",
      "     |  \n",
      "     |  Classifier using Ridge regression.\n",
      "     |  \n",
      "     |  This classifier first converts the target values into ``{-1, 1}`` and\n",
      "     |  then treats the problem as a regression task (multi-output regression in\n",
      "     |  the multiclass case).\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <ridge_regression>`.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  alpha : float, default=1.0\n",
      "     |      Regularization strength; must be a positive float. Regularization\n",
      "     |      improves the conditioning of the problem and reduces the variance of\n",
      "     |      the estimates. Larger values specify stronger regularization.\n",
      "     |      Alpha corresponds to ``1 / (2C)`` in other linear models such as\n",
      "     |      :class:`~sklearn.linear_model.LogisticRegression` or\n",
      "     |      :class:`~sklearn.svm.LinearSVC`.\n",
      "     |  \n",
      "     |  fit_intercept : bool, default=True\n",
      "     |      Whether to calculate the intercept for this model. If set to false, no\n",
      "     |      intercept will be used in calculations (e.g. data is expected to be\n",
      "     |      already centered).\n",
      "     |  \n",
      "     |  normalize : bool, default=False\n",
      "     |      This parameter is ignored when ``fit_intercept`` is set to False.\n",
      "     |      If True, the regressors X will be normalized before regression by\n",
      "     |      subtracting the mean and dividing by the l2-norm.\n",
      "     |      If you wish to standardize, please use\n",
      "     |      :class:`~sklearn.preprocessing.StandardScaler` before calling ``fit``\n",
      "     |      on an estimator with ``normalize=False``.\n",
      "     |  \n",
      "     |  copy_X : bool, default=True\n",
      "     |      If True, X will be copied; else, it may be overwritten.\n",
      "     |  \n",
      "     |  max_iter : int, default=None\n",
      "     |      Maximum number of iterations for conjugate gradient solver.\n",
      "     |      The default value is determined by scipy.sparse.linalg.\n",
      "     |  \n",
      "     |  tol : float, default=1e-3\n",
      "     |      Precision of the solution.\n",
      "     |  \n",
      "     |  class_weight : dict or 'balanced', default=None\n",
      "     |      Weights associated with classes in the form ``{class_label: weight}``.\n",
      "     |      If not given, all classes are supposed to have weight one.\n",
      "     |  \n",
      "     |      The \"balanced\" mode uses the values of y to automatically adjust\n",
      "     |      weights inversely proportional to class frequencies in the input data\n",
      "     |      as ``n_samples / (n_classes * np.bincount(y))``.\n",
      "     |  \n",
      "     |  solver : {'auto', 'svd', 'cholesky', 'lsqr', 'sparse_cg', 'sag', 'saga'},         default='auto'\n",
      "     |      Solver to use in the computational routines:\n",
      "     |  \n",
      "     |      - 'auto' chooses the solver automatically based on the type of data.\n",
      "     |  \n",
      "     |      - 'svd' uses a Singular Value Decomposition of X to compute the Ridge\n",
      "     |        coefficients. More stable for singular matrices than 'cholesky'.\n",
      "     |  \n",
      "     |      - 'cholesky' uses the standard scipy.linalg.solve function to\n",
      "     |        obtain a closed-form solution.\n",
      "     |  \n",
      "     |      - 'sparse_cg' uses the conjugate gradient solver as found in\n",
      "     |        scipy.sparse.linalg.cg. As an iterative algorithm, this solver is\n",
      "     |        more appropriate than 'cholesky' for large-scale data\n",
      "     |        (possibility to set `tol` and `max_iter`).\n",
      "     |  \n",
      "     |      - 'lsqr' uses the dedicated regularized least-squares routine\n",
      "     |        scipy.sparse.linalg.lsqr. It is the fastest and uses an iterative\n",
      "     |        procedure.\n",
      "     |  \n",
      "     |      - 'sag' uses a Stochastic Average Gradient descent, and 'saga' uses\n",
      "     |        its unbiased and more flexible version named SAGA. Both methods\n",
      "     |        use an iterative procedure, and are often faster than other solvers\n",
      "     |        when both n_samples and n_features are large. Note that 'sag' and\n",
      "     |        'saga' fast convergence is only guaranteed on features with\n",
      "     |        approximately the same scale. You can preprocess the data with a\n",
      "     |        scaler from sklearn.preprocessing.\n",
      "     |  \n",
      "     |        .. versionadded:: 0.17\n",
      "     |           Stochastic Average Gradient descent solver.\n",
      "     |        .. versionadded:: 0.19\n",
      "     |         SAGA solver.\n",
      "     |  \n",
      "     |  random_state : int, RandomState instance, default=None\n",
      "     |      Used when ``solver`` == 'sag' or 'saga' to shuffle the data.\n",
      "     |      See :term:`Glossary <random_state>` for details.\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  coef_ : ndarray of shape (1, n_features) or (n_classes, n_features)\n",
      "     |      Coefficient of the features in the decision function.\n",
      "     |  \n",
      "     |      ``coef_`` is of shape (1, n_features) when the given problem is binary.\n",
      "     |  \n",
      "     |  intercept_ : float or ndarray of shape (n_targets,)\n",
      "     |      Independent term in decision function. Set to 0.0 if\n",
      "     |      ``fit_intercept = False``.\n",
      "     |  \n",
      "     |  n_iter_ : None or ndarray of shape (n_targets,)\n",
      "     |      Actual number of iterations for each target. Available only for\n",
      "     |      sag and lsqr solvers. Other solvers will return None.\n",
      "     |  \n",
      "     |  classes_ : ndarray of shape (n_classes,)\n",
      "     |      The classes labels.\n",
      "     |  \n",
      "     |  See Also\n",
      "     |  --------\n",
      "     |  Ridge : Ridge regression.\n",
      "     |  RidgeClassifierCV :  Ridge classifier with built-in cross validation.\n",
      "     |  \n",
      "     |  Notes\n",
      "     |  -----\n",
      "     |  For multi-class classification, n_class classifiers are trained in\n",
      "     |  a one-versus-all approach. Concretely, this is implemented by taking\n",
      "     |  advantage of the multi-variate response support in Ridge.\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  >>> from sklearn.datasets import load_breast_cancer\n",
      "     |  >>> from sklearn.linear_model import RidgeClassifier\n",
      "     |  >>> X, y = load_breast_cancer(return_X_y=True)\n",
      "     |  >>> clf = RidgeClassifier().fit(X, y)\n",
      "     |  >>> clf.score(X, y)\n",
      "     |  0.9595...\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      RidgeClassifier\n",
      "     |      sklearn.linear_model._base.LinearClassifierMixin\n",
      "     |      sklearn.base.ClassifierMixin\n",
      "     |      _BaseRidge\n",
      "     |      sklearn.linear_model._base.LinearModel\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, alpha=1.0, *, fit_intercept=True, normalize=False, copy_X=True, max_iter=None, tol=0.001, class_weight=None, solver='auto', random_state=None)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  fit(self, X, y, sample_weight=None)\n",
      "     |      Fit Ridge classifier model.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {ndarray, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          Training data.\n",
      "     |      \n",
      "     |      y : ndarray of shape (n_samples,)\n",
      "     |          Target values.\n",
      "     |      \n",
      "     |      sample_weight : float or ndarray of shape (n_samples,), default=None\n",
      "     |          Individual weights for each sample. If given a float, every sample\n",
      "     |          will have the same weight.\n",
      "     |      \n",
      "     |          .. versionadded:: 0.17\n",
      "     |             *sample_weight* support to Classifier.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |          Instance of the estimator.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties defined here:\n",
      "     |  \n",
      "     |  classes_\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.linear_model._base.LinearClassifierMixin:\n",
      "     |  \n",
      "     |  decision_function(self, X)\n",
      "     |      Predict confidence scores for samples.\n",
      "     |      \n",
      "     |      The confidence score for a sample is proportional to the signed\n",
      "     |      distance of that sample to the hyperplane.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like or sparse matrix, shape (n_samples, n_features)\n",
      "     |          Samples.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      array, shape=(n_samples,) if n_classes == 2 else (n_samples, n_classes)\n",
      "     |          Confidence scores per (sample, class) combination. In the binary\n",
      "     |          case, confidence score for self.classes_[1] where >0 means this\n",
      "     |          class would be predicted.\n",
      "     |  \n",
      "     |  predict(self, X)\n",
      "     |      Predict class labels for samples in X.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like or sparse matrix, shape (n_samples, n_features)\n",
      "     |          Samples.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      C : array, shape [n_samples]\n",
      "     |          Predicted class label per sample.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.ClassifierMixin:\n",
      "     |  \n",
      "     |  score(self, X, y, sample_weight=None)\n",
      "     |      Return the mean accuracy on the given test data and labels.\n",
      "     |      \n",
      "     |      In multi-label classification, this is the subset accuracy\n",
      "     |      which is a harsh metric since you require for each sample that\n",
      "     |      each label set be correctly predicted.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          Test samples.\n",
      "     |      \n",
      "     |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      "     |          True labels for `X`.\n",
      "     |      \n",
      "     |      sample_weight : array-like of shape (n_samples,), default=None\n",
      "     |          Sample weights.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      score : float\n",
      "     |          Mean accuracy of ``self.predict(X)`` wrt. `y`.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.base.ClassifierMixin:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |  \n",
      "     |  __repr__(self, N_CHAR_MAX=700)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  get_params(self, deep=True)\n",
      "     |      Get parameters for this estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      deep : bool, default=True\n",
      "     |          If True, will return the parameters for this estimator and\n",
      "     |          contained subobjects that are estimators.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      params : dict\n",
      "     |          Parameter names mapped to their values.\n",
      "     |  \n",
      "     |  set_params(self, **params)\n",
      "     |      Set the parameters of this estimator.\n",
      "     |      \n",
      "     |      The method works on simple estimators as well as on nested objects\n",
      "     |      (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n",
      "     |      parameters of the form ``<component>__<parameter>`` so that it's\n",
      "     |      possible to update each component of a nested object.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      **params : dict\n",
      "     |          Estimator parameters.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : estimator instance\n",
      "     |          Estimator instance.\n",
      "    \n",
      "    class RidgeClassifierCV(sklearn.linear_model._base.LinearClassifierMixin, _BaseRidgeCV)\n",
      "     |  RidgeClassifierCV(alphas=(0.1, 1.0, 10.0), *, fit_intercept=True, normalize=False, scoring=None, cv=None, class_weight=None, store_cv_values=False)\n",
      "     |  \n",
      "     |  Ridge classifier with built-in cross-validation.\n",
      "     |  \n",
      "     |  See glossary entry for :term:`cross-validation estimator`.\n",
      "     |  \n",
      "     |  By default, it performs Leave-One-Out Cross-Validation. Currently,\n",
      "     |  only the n_features > n_samples case is handled efficiently.\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <ridge_regression>`.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  alphas : ndarray of shape (n_alphas,), default=(0.1, 1.0, 10.0)\n",
      "     |      Array of alpha values to try.\n",
      "     |      Regularization strength; must be a positive float. Regularization\n",
      "     |      improves the conditioning of the problem and reduces the variance of\n",
      "     |      the estimates. Larger values specify stronger regularization.\n",
      "     |      Alpha corresponds to ``1 / (2C)`` in other linear models such as\n",
      "     |      :class:`~sklearn.linear_model.LogisticRegression` or\n",
      "     |      :class:`~sklearn.svm.LinearSVC`.\n",
      "     |  \n",
      "     |  fit_intercept : bool, default=True\n",
      "     |      Whether to calculate the intercept for this model. If set\n",
      "     |      to false, no intercept will be used in calculations\n",
      "     |      (i.e. data is expected to be centered).\n",
      "     |  \n",
      "     |  normalize : bool, default=False\n",
      "     |      This parameter is ignored when ``fit_intercept`` is set to False.\n",
      "     |      If True, the regressors X will be normalized before regression by\n",
      "     |      subtracting the mean and dividing by the l2-norm.\n",
      "     |      If you wish to standardize, please use\n",
      "     |      :class:`~sklearn.preprocessing.StandardScaler` before calling ``fit``\n",
      "     |      on an estimator with ``normalize=False``.\n",
      "     |  \n",
      "     |  scoring : string, callable, default=None\n",
      "     |      A string (see model evaluation documentation) or\n",
      "     |      a scorer callable object / function with signature\n",
      "     |      ``scorer(estimator, X, y)``.\n",
      "     |  \n",
      "     |  cv : int, cross-validation generator or an iterable, default=None\n",
      "     |      Determines the cross-validation splitting strategy.\n",
      "     |      Possible inputs for cv are:\n",
      "     |  \n",
      "     |      - None, to use the efficient Leave-One-Out cross-validation\n",
      "     |      - integer, to specify the number of folds.\n",
      "     |      - :term:`CV splitter`,\n",
      "     |      - An iterable yielding (train, test) splits as arrays of indices.\n",
      "     |  \n",
      "     |      Refer :ref:`User Guide <cross_validation>` for the various\n",
      "     |      cross-validation strategies that can be used here.\n",
      "     |  \n",
      "     |  class_weight : dict or 'balanced', default=None\n",
      "     |      Weights associated with classes in the form ``{class_label: weight}``.\n",
      "     |      If not given, all classes are supposed to have weight one.\n",
      "     |  \n",
      "     |      The \"balanced\" mode uses the values of y to automatically adjust\n",
      "     |      weights inversely proportional to class frequencies in the input data\n",
      "     |      as ``n_samples / (n_classes * np.bincount(y))``\n",
      "     |  \n",
      "     |  store_cv_values : bool, default=False\n",
      "     |      Flag indicating if the cross-validation values corresponding to\n",
      "     |      each alpha should be stored in the ``cv_values_`` attribute (see\n",
      "     |      below). This flag is only compatible with ``cv=None`` (i.e. using\n",
      "     |      Leave-One-Out Cross-Validation).\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  cv_values_ : ndarray of shape (n_samples, n_targets, n_alphas), optional\n",
      "     |      Cross-validation values for each alpha (if ``store_cv_values=True`` and\n",
      "     |      ``cv=None``). After ``fit()`` has been called, this attribute will\n",
      "     |      contain the mean squared errors (by default) or the values of the\n",
      "     |      ``{loss,score}_func`` function (if provided in the constructor). This\n",
      "     |      attribute exists only when ``store_cv_values`` is True.\n",
      "     |  \n",
      "     |  coef_ : ndarray of shape (1, n_features) or (n_targets, n_features)\n",
      "     |      Coefficient of the features in the decision function.\n",
      "     |  \n",
      "     |      ``coef_`` is of shape (1, n_features) when the given problem is binary.\n",
      "     |  \n",
      "     |  intercept_ : float or ndarray of shape (n_targets,)\n",
      "     |      Independent term in decision function. Set to 0.0 if\n",
      "     |      ``fit_intercept = False``.\n",
      "     |  \n",
      "     |  alpha_ : float\n",
      "     |      Estimated regularization parameter.\n",
      "     |  \n",
      "     |  best_score_ : float\n",
      "     |      Score of base estimator with best alpha.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.23\n",
      "     |  \n",
      "     |  classes_ : ndarray of shape (n_classes,)\n",
      "     |      The classes labels.\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  >>> from sklearn.datasets import load_breast_cancer\n",
      "     |  >>> from sklearn.linear_model import RidgeClassifierCV\n",
      "     |  >>> X, y = load_breast_cancer(return_X_y=True)\n",
      "     |  >>> clf = RidgeClassifierCV(alphas=[1e-3, 1e-2, 1e-1, 1]).fit(X, y)\n",
      "     |  >>> clf.score(X, y)\n",
      "     |  0.9630...\n",
      "     |  \n",
      "     |  See Also\n",
      "     |  --------\n",
      "     |  Ridge : Ridge regression.\n",
      "     |  RidgeClassifier : Ridge classifier.\n",
      "     |  RidgeCV : Ridge regression with built-in cross validation.\n",
      "     |  \n",
      "     |  Notes\n",
      "     |  -----\n",
      "     |  For multi-class classification, n_class classifiers are trained in\n",
      "     |  a one-versus-all approach. Concretely, this is implemented by taking\n",
      "     |  advantage of the multi-variate response support in Ridge.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      RidgeClassifierCV\n",
      "     |      sklearn.linear_model._base.LinearClassifierMixin\n",
      "     |      sklearn.base.ClassifierMixin\n",
      "     |      _BaseRidgeCV\n",
      "     |      sklearn.linear_model._base.LinearModel\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, alphas=(0.1, 1.0, 10.0), *, fit_intercept=True, normalize=False, scoring=None, cv=None, class_weight=None, store_cv_values=False)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  fit(self, X, y, sample_weight=None)\n",
      "     |      Fit Ridge classifier with cv.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : ndarray of shape (n_samples, n_features)\n",
      "     |          Training vectors, where n_samples is the number of samples\n",
      "     |          and n_features is the number of features. When using GCV,\n",
      "     |          will be cast to float64 if necessary.\n",
      "     |      \n",
      "     |      y : ndarray of shape (n_samples,)\n",
      "     |          Target values. Will be cast to X's dtype if necessary.\n",
      "     |      \n",
      "     |      sample_weight : float or ndarray of shape (n_samples,), default=None\n",
      "     |          Individual weights for each sample. If given a float, every sample\n",
      "     |          will have the same weight.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties defined here:\n",
      "     |  \n",
      "     |  classes_\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.linear_model._base.LinearClassifierMixin:\n",
      "     |  \n",
      "     |  decision_function(self, X)\n",
      "     |      Predict confidence scores for samples.\n",
      "     |      \n",
      "     |      The confidence score for a sample is proportional to the signed\n",
      "     |      distance of that sample to the hyperplane.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like or sparse matrix, shape (n_samples, n_features)\n",
      "     |          Samples.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      array, shape=(n_samples,) if n_classes == 2 else (n_samples, n_classes)\n",
      "     |          Confidence scores per (sample, class) combination. In the binary\n",
      "     |          case, confidence score for self.classes_[1] where >0 means this\n",
      "     |          class would be predicted.\n",
      "     |  \n",
      "     |  predict(self, X)\n",
      "     |      Predict class labels for samples in X.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like or sparse matrix, shape (n_samples, n_features)\n",
      "     |          Samples.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      C : array, shape [n_samples]\n",
      "     |          Predicted class label per sample.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.ClassifierMixin:\n",
      "     |  \n",
      "     |  score(self, X, y, sample_weight=None)\n",
      "     |      Return the mean accuracy on the given test data and labels.\n",
      "     |      \n",
      "     |      In multi-label classification, this is the subset accuracy\n",
      "     |      which is a harsh metric since you require for each sample that\n",
      "     |      each label set be correctly predicted.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          Test samples.\n",
      "     |      \n",
      "     |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      "     |          True labels for `X`.\n",
      "     |      \n",
      "     |      sample_weight : array-like of shape (n_samples,), default=None\n",
      "     |          Sample weights.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      score : float\n",
      "     |          Mean accuracy of ``self.predict(X)`` wrt. `y`.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.base.ClassifierMixin:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |  \n",
      "     |  __repr__(self, N_CHAR_MAX=700)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  get_params(self, deep=True)\n",
      "     |      Get parameters for this estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      deep : bool, default=True\n",
      "     |          If True, will return the parameters for this estimator and\n",
      "     |          contained subobjects that are estimators.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      params : dict\n",
      "     |          Parameter names mapped to their values.\n",
      "     |  \n",
      "     |  set_params(self, **params)\n",
      "     |      Set the parameters of this estimator.\n",
      "     |      \n",
      "     |      The method works on simple estimators as well as on nested objects\n",
      "     |      (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n",
      "     |      parameters of the form ``<component>__<parameter>`` so that it's\n",
      "     |      possible to update each component of a nested object.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      **params : dict\n",
      "     |          Estimator parameters.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : estimator instance\n",
      "     |          Estimator instance.\n",
      "    \n",
      "    class SGDClassifier(BaseSGDClassifier)\n",
      "     |  SGDClassifier(loss='hinge', *, penalty='l2', alpha=0.0001, l1_ratio=0.15, fit_intercept=True, max_iter=1000, tol=0.001, shuffle=True, verbose=0, epsilon=0.1, n_jobs=None, random_state=None, learning_rate='optimal', eta0=0.0, power_t=0.5, early_stopping=False, validation_fraction=0.1, n_iter_no_change=5, class_weight=None, warm_start=False, average=False)\n",
      "     |  \n",
      "     |  Linear classifiers (SVM, logistic regression, etc.) with SGD training.\n",
      "     |  \n",
      "     |  This estimator implements regularized linear models with stochastic\n",
      "     |  gradient descent (SGD) learning: the gradient of the loss is estimated\n",
      "     |  each sample at a time and the model is updated along the way with a\n",
      "     |  decreasing strength schedule (aka learning rate). SGD allows minibatch\n",
      "     |  (online/out-of-core) learning via the `partial_fit` method.\n",
      "     |  For best results using the default learning rate schedule, the data should\n",
      "     |  have zero mean and unit variance.\n",
      "     |  \n",
      "     |  This implementation works with data represented as dense or sparse arrays\n",
      "     |  of floating point values for the features. The model it fits can be\n",
      "     |  controlled with the loss parameter; by default, it fits a linear support\n",
      "     |  vector machine (SVM).\n",
      "     |  \n",
      "     |  The regularizer is a penalty added to the loss function that shrinks model\n",
      "     |  parameters towards the zero vector using either the squared euclidean norm\n",
      "     |  L2 or the absolute norm L1 or a combination of both (Elastic Net). If the\n",
      "     |  parameter update crosses the 0.0 value because of the regularizer, the\n",
      "     |  update is truncated to 0.0 to allow for learning sparse models and achieve\n",
      "     |  online feature selection.\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <sgd>`.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  loss : str, default='hinge'\n",
      "     |      The loss function to be used. Defaults to 'hinge', which gives a\n",
      "     |      linear SVM.\n",
      "     |  \n",
      "     |      The possible options are 'hinge', 'log', 'modified_huber',\n",
      "     |      'squared_hinge', 'perceptron', or a regression loss: 'squared_loss',\n",
      "     |      'huber', 'epsilon_insensitive', or 'squared_epsilon_insensitive'.\n",
      "     |  \n",
      "     |      The 'log' loss gives logistic regression, a probabilistic classifier.\n",
      "     |      'modified_huber' is another smooth loss that brings tolerance to\n",
      "     |      outliers as well as probability estimates.\n",
      "     |      'squared_hinge' is like hinge but is quadratically penalized.\n",
      "     |      'perceptron' is the linear loss used by the perceptron algorithm.\n",
      "     |      The other losses are designed for regression but can be useful in\n",
      "     |      classification as well; see\n",
      "     |      :class:`~sklearn.linear_model.SGDRegressor` for a description.\n",
      "     |  \n",
      "     |      More details about the losses formulas can be found in the\n",
      "     |      :ref:`User Guide <sgd_mathematical_formulation>`.\n",
      "     |  \n",
      "     |  penalty : {'l2', 'l1', 'elasticnet'}, default='l2'\n",
      "     |      The penalty (aka regularization term) to be used. Defaults to 'l2'\n",
      "     |      which is the standard regularizer for linear SVM models. 'l1' and\n",
      "     |      'elasticnet' might bring sparsity to the model (feature selection)\n",
      "     |      not achievable with 'l2'.\n",
      "     |  \n",
      "     |  alpha : float, default=0.0001\n",
      "     |      Constant that multiplies the regularization term. The higher the\n",
      "     |      value, the stronger the regularization.\n",
      "     |      Also used to compute the learning rate when set to `learning_rate` is\n",
      "     |      set to 'optimal'.\n",
      "     |  \n",
      "     |  l1_ratio : float, default=0.15\n",
      "     |      The Elastic Net mixing parameter, with 0 <= l1_ratio <= 1.\n",
      "     |      l1_ratio=0 corresponds to L2 penalty, l1_ratio=1 to L1.\n",
      "     |      Only used if `penalty` is 'elasticnet'.\n",
      "     |  \n",
      "     |  fit_intercept : bool, default=True\n",
      "     |      Whether the intercept should be estimated or not. If False, the\n",
      "     |      data is assumed to be already centered.\n",
      "     |  \n",
      "     |  max_iter : int, default=1000\n",
      "     |      The maximum number of passes over the training data (aka epochs).\n",
      "     |      It only impacts the behavior in the ``fit`` method, and not the\n",
      "     |      :meth:`partial_fit` method.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.19\n",
      "     |  \n",
      "     |  tol : float, default=1e-3\n",
      "     |      The stopping criterion. If it is not None, training will stop\n",
      "     |      when (loss > best_loss - tol) for ``n_iter_no_change`` consecutive\n",
      "     |      epochs.\n",
      "     |      Convergence is checked against the training loss or the\n",
      "     |      validation loss depending on the `early_stopping` parameter.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.19\n",
      "     |  \n",
      "     |  shuffle : bool, default=True\n",
      "     |      Whether or not the training data should be shuffled after each epoch.\n",
      "     |  \n",
      "     |  verbose : int, default=0\n",
      "     |      The verbosity level.\n",
      "     |  \n",
      "     |  epsilon : float, default=0.1\n",
      "     |      Epsilon in the epsilon-insensitive loss functions; only if `loss` is\n",
      "     |      'huber', 'epsilon_insensitive', or 'squared_epsilon_insensitive'.\n",
      "     |      For 'huber', determines the threshold at which it becomes less\n",
      "     |      important to get the prediction exactly right.\n",
      "     |      For epsilon-insensitive, any differences between the current prediction\n",
      "     |      and the correct label are ignored if they are less than this threshold.\n",
      "     |  \n",
      "     |  n_jobs : int, default=None\n",
      "     |      The number of CPUs to use to do the OVA (One Versus All, for\n",
      "     |      multi-class problems) computation.\n",
      "     |      ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n",
      "     |      ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n",
      "     |      for more details.\n",
      "     |  \n",
      "     |  random_state : int, RandomState instance, default=None\n",
      "     |      Used for shuffling the data, when ``shuffle`` is set to ``True``.\n",
      "     |      Pass an int for reproducible output across multiple function calls.\n",
      "     |      See :term:`Glossary <random_state>`.\n",
      "     |  \n",
      "     |  learning_rate : str, default='optimal'\n",
      "     |      The learning rate schedule:\n",
      "     |  \n",
      "     |      - 'constant': `eta = eta0`\n",
      "     |      - 'optimal': `eta = 1.0 / (alpha * (t + t0))`\n",
      "     |        where t0 is chosen by a heuristic proposed by Leon Bottou.\n",
      "     |      - 'invscaling': `eta = eta0 / pow(t, power_t)`\n",
      "     |      - 'adaptive': eta = eta0, as long as the training keeps decreasing.\n",
      "     |        Each time n_iter_no_change consecutive epochs fail to decrease the\n",
      "     |        training loss by tol or fail to increase validation score by tol if\n",
      "     |        early_stopping is True, the current learning rate is divided by 5.\n",
      "     |  \n",
      "     |          .. versionadded:: 0.20\n",
      "     |              Added 'adaptive' option\n",
      "     |  \n",
      "     |  eta0 : double, default=0.0\n",
      "     |      The initial learning rate for the 'constant', 'invscaling' or\n",
      "     |      'adaptive' schedules. The default value is 0.0 as eta0 is not used by\n",
      "     |      the default schedule 'optimal'.\n",
      "     |  \n",
      "     |  power_t : double, default=0.5\n",
      "     |      The exponent for inverse scaling learning rate [default 0.5].\n",
      "     |  \n",
      "     |  early_stopping : bool, default=False\n",
      "     |      Whether to use early stopping to terminate training when validation\n",
      "     |      score is not improving. If set to True, it will automatically set aside\n",
      "     |      a stratified fraction of training data as validation and terminate\n",
      "     |      training when validation score returned by the `score` method is not\n",
      "     |      improving by at least tol for n_iter_no_change consecutive epochs.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.20\n",
      "     |          Added 'early_stopping' option\n",
      "     |  \n",
      "     |  validation_fraction : float, default=0.1\n",
      "     |      The proportion of training data to set aside as validation set for\n",
      "     |      early stopping. Must be between 0 and 1.\n",
      "     |      Only used if `early_stopping` is True.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.20\n",
      "     |          Added 'validation_fraction' option\n",
      "     |  \n",
      "     |  n_iter_no_change : int, default=5\n",
      "     |      Number of iterations with no improvement to wait before stopping\n",
      "     |      fitting.\n",
      "     |      Convergence is checked against the training loss or the\n",
      "     |      validation loss depending on the `early_stopping` parameter.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.20\n",
      "     |          Added 'n_iter_no_change' option\n",
      "     |  \n",
      "     |  class_weight : dict, {class_label: weight} or \"balanced\", default=None\n",
      "     |      Preset for the class_weight fit parameter.\n",
      "     |  \n",
      "     |      Weights associated with classes. If not given, all classes\n",
      "     |      are supposed to have weight one.\n",
      "     |  \n",
      "     |      The \"balanced\" mode uses the values of y to automatically adjust\n",
      "     |      weights inversely proportional to class frequencies in the input data\n",
      "     |      as ``n_samples / (n_classes * np.bincount(y))``.\n",
      "     |  \n",
      "     |  warm_start : bool, default=False\n",
      "     |      When set to True, reuse the solution of the previous call to fit as\n",
      "     |      initialization, otherwise, just erase the previous solution.\n",
      "     |      See :term:`the Glossary <warm_start>`.\n",
      "     |  \n",
      "     |      Repeatedly calling fit or partial_fit when warm_start is True can\n",
      "     |      result in a different solution than when calling fit a single time\n",
      "     |      because of the way the data is shuffled.\n",
      "     |      If a dynamic learning rate is used, the learning rate is adapted\n",
      "     |      depending on the number of samples already seen. Calling ``fit`` resets\n",
      "     |      this counter, while ``partial_fit`` will result in increasing the\n",
      "     |      existing counter.\n",
      "     |  \n",
      "     |  average : bool or int, default=False\n",
      "     |      When set to True, computes the averaged SGD weights accross all\n",
      "     |      updates and stores the result in the ``coef_`` attribute. If set to\n",
      "     |      an int greater than 1, averaging will begin once the total number of\n",
      "     |      samples seen reaches `average`. So ``average=10`` will begin\n",
      "     |      averaging after seeing 10 samples.\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  coef_ : ndarray of shape (1, n_features) if n_classes == 2 else             (n_classes, n_features)\n",
      "     |      Weights assigned to the features.\n",
      "     |  \n",
      "     |  intercept_ : ndarray of shape (1,) if n_classes == 2 else (n_classes,)\n",
      "     |      Constants in decision function.\n",
      "     |  \n",
      "     |  n_iter_ : int\n",
      "     |      The actual number of iterations before reaching the stopping criterion.\n",
      "     |      For multiclass fits, it is the maximum over every binary fit.\n",
      "     |  \n",
      "     |  loss_function_ : concrete ``LossFunction``\n",
      "     |  \n",
      "     |  classes_ : array of shape (n_classes,)\n",
      "     |  \n",
      "     |  t_ : int\n",
      "     |      Number of weight updates performed during training.\n",
      "     |      Same as ``(n_iter_ * n_samples)``.\n",
      "     |  \n",
      "     |  See Also\n",
      "     |  --------\n",
      "     |  sklearn.svm.LinearSVC : Linear support vector classification.\n",
      "     |  LogisticRegression : Logistic regression.\n",
      "     |  Perceptron : Inherits from SGDClassifier. ``Perceptron()`` is equivalent to\n",
      "     |      ``SGDClassifier(loss=\"perceptron\", eta0=1, learning_rate=\"constant\",\n",
      "     |      penalty=None)``.\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  >>> import numpy as np\n",
      "     |  >>> from sklearn.linear_model import SGDClassifier\n",
      "     |  >>> from sklearn.preprocessing import StandardScaler\n",
      "     |  >>> from sklearn.pipeline import make_pipeline\n",
      "     |  >>> X = np.array([[-1, -1], [-2, -1], [1, 1], [2, 1]])\n",
      "     |  >>> Y = np.array([1, 1, 2, 2])\n",
      "     |  >>> # Always scale the input. The most convenient way is to use a pipeline.\n",
      "     |  >>> clf = make_pipeline(StandardScaler(),\n",
      "     |  ...                     SGDClassifier(max_iter=1000, tol=1e-3))\n",
      "     |  >>> clf.fit(X, Y)\n",
      "     |  Pipeline(steps=[('standardscaler', StandardScaler()),\n",
      "     |                  ('sgdclassifier', SGDClassifier())])\n",
      "     |  >>> print(clf.predict([[-0.8, -1]]))\n",
      "     |  [1]\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      SGDClassifier\n",
      "     |      BaseSGDClassifier\n",
      "     |      sklearn.linear_model._base.LinearClassifierMixin\n",
      "     |      sklearn.base.ClassifierMixin\n",
      "     |      BaseSGD\n",
      "     |      sklearn.linear_model._base.SparseCoefMixin\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, loss='hinge', *, penalty='l2', alpha=0.0001, l1_ratio=0.15, fit_intercept=True, max_iter=1000, tol=0.001, shuffle=True, verbose=0, epsilon=0.1, n_jobs=None, random_state=None, learning_rate='optimal', eta0=0.0, power_t=0.5, early_stopping=False, validation_fraction=0.1, n_iter_no_change=5, class_weight=None, warm_start=False, average=False)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties defined here:\n",
      "     |  \n",
      "     |  predict_log_proba\n",
      "     |      Log of probability estimates.\n",
      "     |      \n",
      "     |      This method is only available for log loss and modified Huber loss.\n",
      "     |      \n",
      "     |      When loss=\"modified_huber\", probability estimates may be hard zeros\n",
      "     |      and ones, so taking the logarithm is not possible.\n",
      "     |      \n",
      "     |      See ``predict_proba`` for details.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          Input data for prediction.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      T : array-like, shape (n_samples, n_classes)\n",
      "     |          Returns the log-probability of the sample for each class in the\n",
      "     |          model, where classes are ordered as they are in\n",
      "     |          `self.classes_`.\n",
      "     |  \n",
      "     |  predict_proba\n",
      "     |      Probability estimates.\n",
      "     |      \n",
      "     |      This method is only available for log loss and modified Huber loss.\n",
      "     |      \n",
      "     |      Multiclass probability estimates are derived from binary (one-vs.-rest)\n",
      "     |      estimates by simple normalization, as recommended by Zadrozny and\n",
      "     |      Elkan.\n",
      "     |      \n",
      "     |      Binary probability estimates for loss=\"modified_huber\" are given by\n",
      "     |      (clip(decision_function(X), -1, 1) + 1) / 2. For other loss functions\n",
      "     |      it is necessary to perform proper probability calibration by wrapping\n",
      "     |      the classifier with\n",
      "     |      :class:`~sklearn.calibration.CalibratedClassifierCV` instead.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix}, shape (n_samples, n_features)\n",
      "     |          Input data for prediction.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      ndarray of shape (n_samples, n_classes)\n",
      "     |          Returns the probability of the sample for each class in the model,\n",
      "     |          where classes are ordered as they are in `self.classes_`.\n",
      "     |      \n",
      "     |      References\n",
      "     |      ----------\n",
      "     |      Zadrozny and Elkan, \"Transforming classifier scores into multiclass\n",
      "     |      probability estimates\", SIGKDD'02,\n",
      "     |      http://www.research.ibm.com/people/z/zadrozny/kdd2002-Transf.pdf\n",
      "     |      \n",
      "     |      The justification for the formula in the loss=\"modified_huber\"\n",
      "     |      case is in the appendix B in:\n",
      "     |      http://jmlr.csail.mit.edu/papers/volume2/zhang02c/zhang02c.pdf\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from BaseSGDClassifier:\n",
      "     |  \n",
      "     |  fit(self, X, y, coef_init=None, intercept_init=None, sample_weight=None)\n",
      "     |      Fit linear model with Stochastic Gradient Descent.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix}, shape (n_samples, n_features)\n",
      "     |          Training data.\n",
      "     |      \n",
      "     |      y : ndarray of shape (n_samples,)\n",
      "     |          Target values.\n",
      "     |      \n",
      "     |      coef_init : ndarray of shape (n_classes, n_features), default=None\n",
      "     |          The initial coefficients to warm-start the optimization.\n",
      "     |      \n",
      "     |      intercept_init : ndarray of shape (n_classes,), default=None\n",
      "     |          The initial intercept to warm-start the optimization.\n",
      "     |      \n",
      "     |      sample_weight : array-like, shape (n_samples,), default=None\n",
      "     |          Weights applied to individual samples.\n",
      "     |          If not provided, uniform weights are assumed. These weights will\n",
      "     |          be multiplied with class_weight (passed through the\n",
      "     |          constructor) if class_weight is specified.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self :\n",
      "     |          Returns an instance of self.\n",
      "     |  \n",
      "     |  partial_fit(self, X, y, classes=None, sample_weight=None)\n",
      "     |      Perform one epoch of stochastic gradient descent on given samples.\n",
      "     |      \n",
      "     |      Internally, this method uses ``max_iter = 1``. Therefore, it is not\n",
      "     |      guaranteed that a minimum of the cost function is reached after calling\n",
      "     |      it once. Matters such as objective convergence and early stopping\n",
      "     |      should be handled by the user.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix}, shape (n_samples, n_features)\n",
      "     |          Subset of the training data.\n",
      "     |      \n",
      "     |      y : ndarray of shape (n_samples,)\n",
      "     |          Subset of the target values.\n",
      "     |      \n",
      "     |      classes : ndarray of shape (n_classes,), default=None\n",
      "     |          Classes across all calls to partial_fit.\n",
      "     |          Can be obtained by via `np.unique(y_all)`, where y_all is the\n",
      "     |          target vector of the entire dataset.\n",
      "     |          This argument is required for the first call to partial_fit\n",
      "     |          and can be omitted in the subsequent calls.\n",
      "     |          Note that y doesn't need to contain all labels in `classes`.\n",
      "     |      \n",
      "     |      sample_weight : array-like, shape (n_samples,), default=None\n",
      "     |          Weights applied to individual samples.\n",
      "     |          If not provided, uniform weights are assumed.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self :\n",
      "     |          Returns an instance of self.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from BaseSGDClassifier:\n",
      "     |  \n",
      "     |  loss_functions = {'epsilon_insensitive': (<class 'sklearn.linear_model...\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.linear_model._base.LinearClassifierMixin:\n",
      "     |  \n",
      "     |  decision_function(self, X)\n",
      "     |      Predict confidence scores for samples.\n",
      "     |      \n",
      "     |      The confidence score for a sample is proportional to the signed\n",
      "     |      distance of that sample to the hyperplane.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like or sparse matrix, shape (n_samples, n_features)\n",
      "     |          Samples.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      array, shape=(n_samples,) if n_classes == 2 else (n_samples, n_classes)\n",
      "     |          Confidence scores per (sample, class) combination. In the binary\n",
      "     |          case, confidence score for self.classes_[1] where >0 means this\n",
      "     |          class would be predicted.\n",
      "     |  \n",
      "     |  predict(self, X)\n",
      "     |      Predict class labels for samples in X.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like or sparse matrix, shape (n_samples, n_features)\n",
      "     |          Samples.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      C : array, shape [n_samples]\n",
      "     |          Predicted class label per sample.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.ClassifierMixin:\n",
      "     |  \n",
      "     |  score(self, X, y, sample_weight=None)\n",
      "     |      Return the mean accuracy on the given test data and labels.\n",
      "     |      \n",
      "     |      In multi-label classification, this is the subset accuracy\n",
      "     |      which is a harsh metric since you require for each sample that\n",
      "     |      each label set be correctly predicted.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          Test samples.\n",
      "     |      \n",
      "     |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      "     |          True labels for `X`.\n",
      "     |      \n",
      "     |      sample_weight : array-like of shape (n_samples,), default=None\n",
      "     |          Sample weights.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      score : float\n",
      "     |          Mean accuracy of ``self.predict(X)`` wrt. `y`.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.base.ClassifierMixin:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from BaseSGD:\n",
      "     |  \n",
      "     |  set_params(self, **kwargs)\n",
      "     |      Set and validate the parameters of estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      **kwargs : dict\n",
      "     |          Estimator parameters.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |          Estimator instance.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties inherited from BaseSGD:\n",
      "     |  \n",
      "     |  average_coef_\n",
      "     |  \n",
      "     |  average_intercept_\n",
      "     |  \n",
      "     |  standard_coef_\n",
      "     |  \n",
      "     |  standard_intercept_\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.linear_model._base.SparseCoefMixin:\n",
      "     |  \n",
      "     |  densify(self)\n",
      "     |      Convert coefficient matrix to dense array format.\n",
      "     |      \n",
      "     |      Converts the ``coef_`` member (back) to a numpy.ndarray. This is the\n",
      "     |      default format of ``coef_`` and is required for fitting, so calling\n",
      "     |      this method is only required on models that have previously been\n",
      "     |      sparsified; otherwise, it is a no-op.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self\n",
      "     |          Fitted estimator.\n",
      "     |  \n",
      "     |  sparsify(self)\n",
      "     |      Convert coefficient matrix to sparse format.\n",
      "     |      \n",
      "     |      Converts the ``coef_`` member to a scipy.sparse matrix, which for\n",
      "     |      L1-regularized models can be much more memory- and storage-efficient\n",
      "     |      than the usual numpy.ndarray representation.\n",
      "     |      \n",
      "     |      The ``intercept_`` member is not converted.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self\n",
      "     |          Fitted estimator.\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      For non-sparse models, i.e. when there are not many zeros in ``coef_``,\n",
      "     |      this may actually *increase* memory usage, so use this method with\n",
      "     |      care. A rule of thumb is that the number of zero elements, which can\n",
      "     |      be computed with ``(coef_ == 0).sum()``, must be more than 50% for this\n",
      "     |      to provide significant benefits.\n",
      "     |      \n",
      "     |      After calling this method, further fitting with the partial_fit\n",
      "     |      method (if any) will not work until you call densify.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |  \n",
      "     |  __repr__(self, N_CHAR_MAX=700)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  get_params(self, deep=True)\n",
      "     |      Get parameters for this estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      deep : bool, default=True\n",
      "     |          If True, will return the parameters for this estimator and\n",
      "     |          contained subobjects that are estimators.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      params : dict\n",
      "     |          Parameter names mapped to their values.\n",
      "    \n",
      "    class SGDRegressor(BaseSGDRegressor)\n",
      "     |  SGDRegressor(loss='squared_loss', *, penalty='l2', alpha=0.0001, l1_ratio=0.15, fit_intercept=True, max_iter=1000, tol=0.001, shuffle=True, verbose=0, epsilon=0.1, random_state=None, learning_rate='invscaling', eta0=0.01, power_t=0.25, early_stopping=False, validation_fraction=0.1, n_iter_no_change=5, warm_start=False, average=False)\n",
      "     |  \n",
      "     |  Linear model fitted by minimizing a regularized empirical loss with SGD\n",
      "     |  \n",
      "     |  SGD stands for Stochastic Gradient Descent: the gradient of the loss is\n",
      "     |  estimated each sample at a time and the model is updated along the way with\n",
      "     |  a decreasing strength schedule (aka learning rate).\n",
      "     |  \n",
      "     |  The regularizer is a penalty added to the loss function that shrinks model\n",
      "     |  parameters towards the zero vector using either the squared euclidean norm\n",
      "     |  L2 or the absolute norm L1 or a combination of both (Elastic Net). If the\n",
      "     |  parameter update crosses the 0.0 value because of the regularizer, the\n",
      "     |  update is truncated to 0.0 to allow for learning sparse models and achieve\n",
      "     |  online feature selection.\n",
      "     |  \n",
      "     |  This implementation works with data represented as dense numpy arrays of\n",
      "     |  floating point values for the features.\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <sgd>`.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  loss : str, default='squared_loss'\n",
      "     |      The loss function to be used. The possible values are 'squared_loss',\n",
      "     |      'huber', 'epsilon_insensitive', or 'squared_epsilon_insensitive'\n",
      "     |  \n",
      "     |      The 'squared_loss' refers to the ordinary least squares fit.\n",
      "     |      'huber' modifies 'squared_loss' to focus less on getting outliers\n",
      "     |      correct by switching from squared to linear loss past a distance of\n",
      "     |      epsilon. 'epsilon_insensitive' ignores errors less than epsilon and is\n",
      "     |      linear past that; this is the loss function used in SVR.\n",
      "     |      'squared_epsilon_insensitive' is the same but becomes squared loss past\n",
      "     |      a tolerance of epsilon.\n",
      "     |  \n",
      "     |      More details about the losses formulas can be found in the\n",
      "     |      :ref:`User Guide <sgd_mathematical_formulation>`.\n",
      "     |  \n",
      "     |  penalty : {'l2', 'l1', 'elasticnet'}, default='l2'\n",
      "     |      The penalty (aka regularization term) to be used. Defaults to 'l2'\n",
      "     |      which is the standard regularizer for linear SVM models. 'l1' and\n",
      "     |      'elasticnet' might bring sparsity to the model (feature selection)\n",
      "     |      not achievable with 'l2'.\n",
      "     |  \n",
      "     |  alpha : float, default=0.0001\n",
      "     |      Constant that multiplies the regularization term. The higher the\n",
      "     |      value, the stronger the regularization.\n",
      "     |      Also used to compute the learning rate when set to `learning_rate` is\n",
      "     |      set to 'optimal'.\n",
      "     |  \n",
      "     |  l1_ratio : float, default=0.15\n",
      "     |      The Elastic Net mixing parameter, with 0 <= l1_ratio <= 1.\n",
      "     |      l1_ratio=0 corresponds to L2 penalty, l1_ratio=1 to L1.\n",
      "     |      Only used if `penalty` is 'elasticnet'.\n",
      "     |  \n",
      "     |  fit_intercept : bool, default=True\n",
      "     |      Whether the intercept should be estimated or not. If False, the\n",
      "     |      data is assumed to be already centered.\n",
      "     |  \n",
      "     |  max_iter : int, default=1000\n",
      "     |      The maximum number of passes over the training data (aka epochs).\n",
      "     |      It only impacts the behavior in the ``fit`` method, and not the\n",
      "     |      :meth:`partial_fit` method.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.19\n",
      "     |  \n",
      "     |  tol : float, default=1e-3\n",
      "     |      The stopping criterion. If it is not None, training will stop\n",
      "     |      when (loss > best_loss - tol) for ``n_iter_no_change`` consecutive\n",
      "     |      epochs.\n",
      "     |      Convergence is checked against the training loss or the\n",
      "     |      validation loss depending on the `early_stopping` parameter.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.19\n",
      "     |  \n",
      "     |  shuffle : bool, default=True\n",
      "     |      Whether or not the training data should be shuffled after each epoch.\n",
      "     |  \n",
      "     |  verbose : int, default=0\n",
      "     |      The verbosity level.\n",
      "     |  \n",
      "     |  epsilon : float, default=0.1\n",
      "     |      Epsilon in the epsilon-insensitive loss functions; only if `loss` is\n",
      "     |      'huber', 'epsilon_insensitive', or 'squared_epsilon_insensitive'.\n",
      "     |      For 'huber', determines the threshold at which it becomes less\n",
      "     |      important to get the prediction exactly right.\n",
      "     |      For epsilon-insensitive, any differences between the current prediction\n",
      "     |      and the correct label are ignored if they are less than this threshold.\n",
      "     |  \n",
      "     |  random_state : int, RandomState instance, default=None\n",
      "     |      Used for shuffling the data, when ``shuffle`` is set to ``True``.\n",
      "     |      Pass an int for reproducible output across multiple function calls.\n",
      "     |      See :term:`Glossary <random_state>`.\n",
      "     |  \n",
      "     |  learning_rate : string, default='invscaling'\n",
      "     |      The learning rate schedule:\n",
      "     |  \n",
      "     |      - 'constant': `eta = eta0`\n",
      "     |      - 'optimal': `eta = 1.0 / (alpha * (t + t0))`\n",
      "     |        where t0 is chosen by a heuristic proposed by Leon Bottou.\n",
      "     |      - 'invscaling': `eta = eta0 / pow(t, power_t)`\n",
      "     |      - 'adaptive': eta = eta0, as long as the training keeps decreasing.\n",
      "     |        Each time n_iter_no_change consecutive epochs fail to decrease the\n",
      "     |        training loss by tol or fail to increase validation score by tol if\n",
      "     |        early_stopping is True, the current learning rate is divided by 5.\n",
      "     |  \n",
      "     |          .. versionadded:: 0.20\n",
      "     |              Added 'adaptive' option\n",
      "     |  \n",
      "     |  eta0 : double, default=0.01\n",
      "     |      The initial learning rate for the 'constant', 'invscaling' or\n",
      "     |      'adaptive' schedules. The default value is 0.01.\n",
      "     |  \n",
      "     |  power_t : double, default=0.25\n",
      "     |      The exponent for inverse scaling learning rate.\n",
      "     |  \n",
      "     |  early_stopping : bool, default=False\n",
      "     |      Whether to use early stopping to terminate training when validation\n",
      "     |      score is not improving. If set to True, it will automatically set aside\n",
      "     |      a fraction of training data as validation and terminate\n",
      "     |      training when validation score returned by the `score` method is not\n",
      "     |      improving by at least `tol` for `n_iter_no_change` consecutive\n",
      "     |      epochs.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.20\n",
      "     |          Added 'early_stopping' option\n",
      "     |  \n",
      "     |  validation_fraction : float, default=0.1\n",
      "     |      The proportion of training data to set aside as validation set for\n",
      "     |      early stopping. Must be between 0 and 1.\n",
      "     |      Only used if `early_stopping` is True.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.20\n",
      "     |          Added 'validation_fraction' option\n",
      "     |  \n",
      "     |  n_iter_no_change : int, default=5\n",
      "     |      Number of iterations with no improvement to wait before stopping\n",
      "     |      fitting.\n",
      "     |      Convergence is checked against the training loss or the\n",
      "     |      validation loss depending on the `early_stopping` parameter.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.20\n",
      "     |          Added 'n_iter_no_change' option\n",
      "     |  \n",
      "     |  warm_start : bool, default=False\n",
      "     |      When set to True, reuse the solution of the previous call to fit as\n",
      "     |      initialization, otherwise, just erase the previous solution.\n",
      "     |      See :term:`the Glossary <warm_start>`.\n",
      "     |  \n",
      "     |      Repeatedly calling fit or partial_fit when warm_start is True can\n",
      "     |      result in a different solution than when calling fit a single time\n",
      "     |      because of the way the data is shuffled.\n",
      "     |      If a dynamic learning rate is used, the learning rate is adapted\n",
      "     |      depending on the number of samples already seen. Calling ``fit`` resets\n",
      "     |      this counter, while ``partial_fit``  will result in increasing the\n",
      "     |      existing counter.\n",
      "     |  \n",
      "     |  average : bool or int, default=False\n",
      "     |      When set to True, computes the averaged SGD weights accross all\n",
      "     |      updates and stores the result in the ``coef_`` attribute. If set to\n",
      "     |      an int greater than 1, averaging will begin once the total number of\n",
      "     |      samples seen reaches `average`. So ``average=10`` will begin\n",
      "     |      averaging after seeing 10 samples.\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  coef_ : ndarray of shape (n_features,)\n",
      "     |      Weights assigned to the features.\n",
      "     |  \n",
      "     |  intercept_ : ndarray of shape (1,)\n",
      "     |      The intercept term.\n",
      "     |  \n",
      "     |  average_coef_ : ndarray of shape (n_features,)\n",
      "     |      Averaged weights assigned to the features. Only available\n",
      "     |      if ``average=True``.\n",
      "     |  \n",
      "     |      .. deprecated:: 0.23\n",
      "     |          Attribute ``average_coef_`` was deprecated\n",
      "     |          in version 0.23 and will be removed in 1.0 (renaming of 0.25).\n",
      "     |  \n",
      "     |  average_intercept_ : ndarray of shape (1,)\n",
      "     |      The averaged intercept term. Only available if ``average=True``.\n",
      "     |  \n",
      "     |      .. deprecated:: 0.23\n",
      "     |          Attribute ``average_intercept_`` was deprecated\n",
      "     |          in version 0.23 and will be removed in 1.0 (renaming of 0.25).\n",
      "     |  \n",
      "     |  n_iter_ : int\n",
      "     |      The actual number of iterations before reaching the stopping criterion.\n",
      "     |  \n",
      "     |  t_ : int\n",
      "     |      Number of weight updates performed during training.\n",
      "     |      Same as ``(n_iter_ * n_samples)``.\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  >>> import numpy as np\n",
      "     |  >>> from sklearn.linear_model import SGDRegressor\n",
      "     |  >>> from sklearn.pipeline import make_pipeline\n",
      "     |  >>> from sklearn.preprocessing import StandardScaler\n",
      "     |  >>> n_samples, n_features = 10, 5\n",
      "     |  >>> rng = np.random.RandomState(0)\n",
      "     |  >>> y = rng.randn(n_samples)\n",
      "     |  >>> X = rng.randn(n_samples, n_features)\n",
      "     |  >>> # Always scale the input. The most convenient way is to use a pipeline.\n",
      "     |  >>> reg = make_pipeline(StandardScaler(),\n",
      "     |  ...                     SGDRegressor(max_iter=1000, tol=1e-3))\n",
      "     |  >>> reg.fit(X, y)\n",
      "     |  Pipeline(steps=[('standardscaler', StandardScaler()),\n",
      "     |                  ('sgdregressor', SGDRegressor())])\n",
      "     |  \n",
      "     |  See Also\n",
      "     |  --------\n",
      "     |  Ridge, ElasticNet, Lasso, sklearn.svm.SVR\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      SGDRegressor\n",
      "     |      BaseSGDRegressor\n",
      "     |      sklearn.base.RegressorMixin\n",
      "     |      BaseSGD\n",
      "     |      sklearn.linear_model._base.SparseCoefMixin\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, loss='squared_loss', *, penalty='l2', alpha=0.0001, l1_ratio=0.15, fit_intercept=True, max_iter=1000, tol=0.001, shuffle=True, verbose=0, epsilon=0.1, random_state=None, learning_rate='invscaling', eta0=0.01, power_t=0.25, early_stopping=False, validation_fraction=0.1, n_iter_no_change=5, warm_start=False, average=False)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from BaseSGDRegressor:\n",
      "     |  \n",
      "     |  fit(self, X, y, coef_init=None, intercept_init=None, sample_weight=None)\n",
      "     |      Fit linear model with Stochastic Gradient Descent.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix}, shape (n_samples, n_features)\n",
      "     |          Training data\n",
      "     |      \n",
      "     |      y : ndarray of shape (n_samples,)\n",
      "     |          Target values\n",
      "     |      \n",
      "     |      coef_init : ndarray of shape (n_features,), default=None\n",
      "     |          The initial coefficients to warm-start the optimization.\n",
      "     |      \n",
      "     |      intercept_init : ndarray of shape (1,), default=None\n",
      "     |          The initial intercept to warm-start the optimization.\n",
      "     |      \n",
      "     |      sample_weight : array-like, shape (n_samples,), default=None\n",
      "     |          Weights applied to individual samples (1. for unweighted).\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : returns an instance of self.\n",
      "     |  \n",
      "     |  partial_fit(self, X, y, sample_weight=None)\n",
      "     |      Perform one epoch of stochastic gradient descent on given samples.\n",
      "     |      \n",
      "     |      Internally, this method uses ``max_iter = 1``. Therefore, it is not\n",
      "     |      guaranteed that a minimum of the cost function is reached after calling\n",
      "     |      it once. Matters such as objective convergence and early stopping\n",
      "     |      should be handled by the user.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix}, shape (n_samples, n_features)\n",
      "     |          Subset of training data\n",
      "     |      \n",
      "     |      y : numpy array of shape (n_samples,)\n",
      "     |          Subset of target values\n",
      "     |      \n",
      "     |      sample_weight : array-like, shape (n_samples,), default=None\n",
      "     |          Weights applied to individual samples.\n",
      "     |          If not provided, uniform weights are assumed.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : returns an instance of self.\n",
      "     |  \n",
      "     |  predict(self, X)\n",
      "     |      Predict using the linear model\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix}, shape (n_samples, n_features)\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      ndarray of shape (n_samples,)\n",
      "     |         Predicted target values per element in X.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from BaseSGDRegressor:\n",
      "     |  \n",
      "     |  loss_functions = {'epsilon_insensitive': (<class 'sklearn.linear_model...\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.RegressorMixin:\n",
      "     |  \n",
      "     |  score(self, X, y, sample_weight=None)\n",
      "     |      Return the coefficient of determination :math:`R^2` of the\n",
      "     |      prediction.\n",
      "     |      \n",
      "     |      The coefficient :math:`R^2` is defined as :math:`(1 - \\frac{u}{v})`,\n",
      "     |      where :math:`u` is the residual sum of squares ``((y_true - y_pred)\n",
      "     |      ** 2).sum()`` and :math:`v` is the total sum of squares ``((y_true -\n",
      "     |      y_true.mean()) ** 2).sum()``. The best possible score is 1.0 and it\n",
      "     |      can be negative (because the model can be arbitrarily worse). A\n",
      "     |      constant model that always predicts the expected value of `y`,\n",
      "     |      disregarding the input features, would get a :math:`R^2` score of\n",
      "     |      0.0.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          Test samples. For some estimators this may be a precomputed\n",
      "     |          kernel matrix or a list of generic objects instead with shape\n",
      "     |          ``(n_samples, n_samples_fitted)``, where ``n_samples_fitted``\n",
      "     |          is the number of samples used in the fitting for the estimator.\n",
      "     |      \n",
      "     |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      "     |          True values for `X`.\n",
      "     |      \n",
      "     |      sample_weight : array-like of shape (n_samples,), default=None\n",
      "     |          Sample weights.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      score : float\n",
      "     |          :math:`R^2` of ``self.predict(X)`` wrt. `y`.\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      The :math:`R^2` score used when calling ``score`` on a regressor uses\n",
      "     |      ``multioutput='uniform_average'`` from version 0.23 to keep consistent\n",
      "     |      with default value of :func:`~sklearn.metrics.r2_score`.\n",
      "     |      This influences the ``score`` method of all the multioutput\n",
      "     |      regressors (except for\n",
      "     |      :class:`~sklearn.multioutput.MultiOutputRegressor`).\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.base.RegressorMixin:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from BaseSGD:\n",
      "     |  \n",
      "     |  set_params(self, **kwargs)\n",
      "     |      Set and validate the parameters of estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      **kwargs : dict\n",
      "     |          Estimator parameters.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |          Estimator instance.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties inherited from BaseSGD:\n",
      "     |  \n",
      "     |  average_coef_\n",
      "     |  \n",
      "     |  average_intercept_\n",
      "     |  \n",
      "     |  standard_coef_\n",
      "     |  \n",
      "     |  standard_intercept_\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.linear_model._base.SparseCoefMixin:\n",
      "     |  \n",
      "     |  densify(self)\n",
      "     |      Convert coefficient matrix to dense array format.\n",
      "     |      \n",
      "     |      Converts the ``coef_`` member (back) to a numpy.ndarray. This is the\n",
      "     |      default format of ``coef_`` and is required for fitting, so calling\n",
      "     |      this method is only required on models that have previously been\n",
      "     |      sparsified; otherwise, it is a no-op.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self\n",
      "     |          Fitted estimator.\n",
      "     |  \n",
      "     |  sparsify(self)\n",
      "     |      Convert coefficient matrix to sparse format.\n",
      "     |      \n",
      "     |      Converts the ``coef_`` member to a scipy.sparse matrix, which for\n",
      "     |      L1-regularized models can be much more memory- and storage-efficient\n",
      "     |      than the usual numpy.ndarray representation.\n",
      "     |      \n",
      "     |      The ``intercept_`` member is not converted.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self\n",
      "     |          Fitted estimator.\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      For non-sparse models, i.e. when there are not many zeros in ``coef_``,\n",
      "     |      this may actually *increase* memory usage, so use this method with\n",
      "     |      care. A rule of thumb is that the number of zero elements, which can\n",
      "     |      be computed with ``(coef_ == 0).sum()``, must be more than 50% for this\n",
      "     |      to provide significant benefits.\n",
      "     |      \n",
      "     |      After calling this method, further fitting with the partial_fit\n",
      "     |      method (if any) will not work until you call densify.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |  \n",
      "     |  __repr__(self, N_CHAR_MAX=700)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  get_params(self, deep=True)\n",
      "     |      Get parameters for this estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      deep : bool, default=True\n",
      "     |          If True, will return the parameters for this estimator and\n",
      "     |          contained subobjects that are estimators.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      params : dict\n",
      "     |          Parameter names mapped to their values.\n",
      "    \n",
      "    class SquaredLoss(Regression)\n",
      "     |  Squared loss traditional used in linear regression.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      SquaredLoss\n",
      "     |      Regression\n",
      "     |      LossFunction\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __reduce__(...)\n",
      "     |      Helper for pickle.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods defined here:\n",
      "     |  \n",
      "     |  __new__(*args, **kwargs) from builtins.type\n",
      "     |      Create and return a new object.  See help(type) for accurate signature.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __pyx_vtable__ = <capsule object NULL>\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from Regression:\n",
      "     |  \n",
      "     |  __setstate__ = __setstate_cython__(...)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from LossFunction:\n",
      "     |  \n",
      "     |  py_dloss(...)\n",
      "     |      Python version of `dloss` for testing.\n",
      "     |      \n",
      "     |      Pytest needs a python function and can't use cdef functions.\n",
      "    \n",
      "    class TheilSenRegressor(sklearn.base.RegressorMixin, sklearn.linear_model._base.LinearModel)\n",
      "     |  TheilSenRegressor(*, fit_intercept=True, copy_X=True, max_subpopulation=10000.0, n_subsamples=None, max_iter=300, tol=0.001, random_state=None, n_jobs=None, verbose=False)\n",
      "     |  \n",
      "     |  Theil-Sen Estimator: robust multivariate regression model.\n",
      "     |  \n",
      "     |  The algorithm calculates least square solutions on subsets with size\n",
      "     |  n_subsamples of the samples in X. Any value of n_subsamples between the\n",
      "     |  number of features and samples leads to an estimator with a compromise\n",
      "     |  between robustness and efficiency. Since the number of least square\n",
      "     |  solutions is \"n_samples choose n_subsamples\", it can be extremely large\n",
      "     |  and can therefore be limited with max_subpopulation. If this limit is\n",
      "     |  reached, the subsets are chosen randomly. In a final step, the spatial\n",
      "     |  median (or L1 median) is calculated of all least square solutions.\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <theil_sen_regression>`.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  fit_intercept : bool, default=True\n",
      "     |      Whether to calculate the intercept for this model. If set\n",
      "     |      to false, no intercept will be used in calculations.\n",
      "     |  \n",
      "     |  copy_X : bool, default=True\n",
      "     |      If True, X will be copied; else, it may be overwritten.\n",
      "     |  \n",
      "     |  max_subpopulation : int, default=1e4\n",
      "     |      Instead of computing with a set of cardinality 'n choose k', where n is\n",
      "     |      the number of samples and k is the number of subsamples (at least\n",
      "     |      number of features), consider only a stochastic subpopulation of a\n",
      "     |      given maximal size if 'n choose k' is larger than max_subpopulation.\n",
      "     |      For other than small problem sizes this parameter will determine\n",
      "     |      memory usage and runtime if n_subsamples is not changed.\n",
      "     |  \n",
      "     |  n_subsamples : int, default=None\n",
      "     |      Number of samples to calculate the parameters. This is at least the\n",
      "     |      number of features (plus 1 if fit_intercept=True) and the number of\n",
      "     |      samples as a maximum. A lower number leads to a higher breakdown\n",
      "     |      point and a low efficiency while a high number leads to a low\n",
      "     |      breakdown point and a high efficiency. If None, take the\n",
      "     |      minimum number of subsamples leading to maximal robustness.\n",
      "     |      If n_subsamples is set to n_samples, Theil-Sen is identical to least\n",
      "     |      squares.\n",
      "     |  \n",
      "     |  max_iter : int, default=300\n",
      "     |      Maximum number of iterations for the calculation of spatial median.\n",
      "     |  \n",
      "     |  tol : float, default=1.e-3\n",
      "     |      Tolerance when calculating spatial median.\n",
      "     |  \n",
      "     |  random_state : int, RandomState instance or None, default=None\n",
      "     |      A random number generator instance to define the state of the random\n",
      "     |      permutations generator. Pass an int for reproducible output across\n",
      "     |      multiple function calls.\n",
      "     |      See :term:`Glossary <random_state>`\n",
      "     |  \n",
      "     |  n_jobs : int, default=None\n",
      "     |      Number of CPUs to use during the cross validation.\n",
      "     |      ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n",
      "     |      ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n",
      "     |      for more details.\n",
      "     |  \n",
      "     |  verbose : bool, default=False\n",
      "     |      Verbose mode when fitting the model.\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  coef_ : ndarray of shape (n_features,)\n",
      "     |      Coefficients of the regression model (median of distribution).\n",
      "     |  \n",
      "     |  intercept_ : float\n",
      "     |      Estimated intercept of regression model.\n",
      "     |  \n",
      "     |  breakdown_ : float\n",
      "     |      Approximated breakdown point.\n",
      "     |  \n",
      "     |  n_iter_ : int\n",
      "     |      Number of iterations needed for the spatial median.\n",
      "     |  \n",
      "     |  n_subpopulation_ : int\n",
      "     |      Number of combinations taken into account from 'n choose k', where n is\n",
      "     |      the number of samples and k is the number of subsamples.\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  >>> from sklearn.linear_model import TheilSenRegressor\n",
      "     |  >>> from sklearn.datasets import make_regression\n",
      "     |  >>> X, y = make_regression(\n",
      "     |  ...     n_samples=200, n_features=2, noise=4.0, random_state=0)\n",
      "     |  >>> reg = TheilSenRegressor(random_state=0).fit(X, y)\n",
      "     |  >>> reg.score(X, y)\n",
      "     |  0.9884...\n",
      "     |  >>> reg.predict(X[:1,])\n",
      "     |  array([-31.5871...])\n",
      "     |  \n",
      "     |  References\n",
      "     |  ----------\n",
      "     |  - Theil-Sen Estimators in a Multiple Linear Regression Model, 2009\n",
      "     |    Xin Dang, Hanxiang Peng, Xueqin Wang and Heping Zhang\n",
      "     |    http://home.olemiss.edu/~xdang/papers/MTSE.pdf\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      TheilSenRegressor\n",
      "     |      sklearn.base.RegressorMixin\n",
      "     |      sklearn.linear_model._base.LinearModel\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, *, fit_intercept=True, copy_X=True, max_subpopulation=10000.0, n_subsamples=None, max_iter=300, tol=0.001, random_state=None, n_jobs=None, verbose=False)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  fit(self, X, y)\n",
      "     |      Fit linear model.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : ndarray of shape (n_samples, n_features)\n",
      "     |          Training data.\n",
      "     |      y : ndarray of shape (n_samples,)\n",
      "     |          Target values.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : returns an instance of self.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.RegressorMixin:\n",
      "     |  \n",
      "     |  score(self, X, y, sample_weight=None)\n",
      "     |      Return the coefficient of determination :math:`R^2` of the\n",
      "     |      prediction.\n",
      "     |      \n",
      "     |      The coefficient :math:`R^2` is defined as :math:`(1 - \\frac{u}{v})`,\n",
      "     |      where :math:`u` is the residual sum of squares ``((y_true - y_pred)\n",
      "     |      ** 2).sum()`` and :math:`v` is the total sum of squares ``((y_true -\n",
      "     |      y_true.mean()) ** 2).sum()``. The best possible score is 1.0 and it\n",
      "     |      can be negative (because the model can be arbitrarily worse). A\n",
      "     |      constant model that always predicts the expected value of `y`,\n",
      "     |      disregarding the input features, would get a :math:`R^2` score of\n",
      "     |      0.0.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          Test samples. For some estimators this may be a precomputed\n",
      "     |          kernel matrix or a list of generic objects instead with shape\n",
      "     |          ``(n_samples, n_samples_fitted)``, where ``n_samples_fitted``\n",
      "     |          is the number of samples used in the fitting for the estimator.\n",
      "     |      \n",
      "     |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      "     |          True values for `X`.\n",
      "     |      \n",
      "     |      sample_weight : array-like of shape (n_samples,), default=None\n",
      "     |          Sample weights.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      score : float\n",
      "     |          :math:`R^2` of ``self.predict(X)`` wrt. `y`.\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      The :math:`R^2` score used when calling ``score`` on a regressor uses\n",
      "     |      ``multioutput='uniform_average'`` from version 0.23 to keep consistent\n",
      "     |      with default value of :func:`~sklearn.metrics.r2_score`.\n",
      "     |      This influences the ``score`` method of all the multioutput\n",
      "     |      regressors (except for\n",
      "     |      :class:`~sklearn.multioutput.MultiOutputRegressor`).\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.base.RegressorMixin:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.linear_model._base.LinearModel:\n",
      "     |  \n",
      "     |  predict(self, X)\n",
      "     |      Predict using the linear model.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like or sparse matrix, shape (n_samples, n_features)\n",
      "     |          Samples.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      C : array, shape (n_samples,)\n",
      "     |          Returns predicted values.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |  \n",
      "     |  __repr__(self, N_CHAR_MAX=700)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  get_params(self, deep=True)\n",
      "     |      Get parameters for this estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      deep : bool, default=True\n",
      "     |          If True, will return the parameters for this estimator and\n",
      "     |          contained subobjects that are estimators.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      params : dict\n",
      "     |          Parameter names mapped to their values.\n",
      "     |  \n",
      "     |  set_params(self, **params)\n",
      "     |      Set the parameters of this estimator.\n",
      "     |      \n",
      "     |      The method works on simple estimators as well as on nested objects\n",
      "     |      (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n",
      "     |      parameters of the form ``<component>__<parameter>`` so that it's\n",
      "     |      possible to update each component of a nested object.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      **params : dict\n",
      "     |          Estimator parameters.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : estimator instance\n",
      "     |          Estimator instance.\n",
      "    \n",
      "    class TweedieRegressor(GeneralizedLinearRegressor)\n",
      "     |  TweedieRegressor(*, power=0.0, alpha=1.0, fit_intercept=True, link='auto', max_iter=100, tol=0.0001, warm_start=False, verbose=0)\n",
      "     |  \n",
      "     |  Generalized Linear Model with a Tweedie distribution.\n",
      "     |  \n",
      "     |  This estimator can be used to model different GLMs depending on the\n",
      "     |  ``power`` parameter, which determines the underlying distribution.\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <Generalized_linear_regression>`.\n",
      "     |  \n",
      "     |  .. versionadded:: 0.23\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  power : float, default=0\n",
      "     |          The power determines the underlying target distribution according\n",
      "     |          to the following table:\n",
      "     |  \n",
      "     |          +-------+------------------------+\n",
      "     |          | Power | Distribution           |\n",
      "     |          +=======+========================+\n",
      "     |          | 0     | Normal                 |\n",
      "     |          +-------+------------------------+\n",
      "     |          | 1     | Poisson                |\n",
      "     |          +-------+------------------------+\n",
      "     |          | (1,2) | Compound Poisson Gamma |\n",
      "     |          +-------+------------------------+\n",
      "     |          | 2     | Gamma                  |\n",
      "     |          +-------+------------------------+\n",
      "     |          | 3     | Inverse Gaussian       |\n",
      "     |          +-------+------------------------+\n",
      "     |  \n",
      "     |          For ``0 < power < 1``, no distribution exists.\n",
      "     |  \n",
      "     |  alpha : float, default=1\n",
      "     |      Constant that multiplies the penalty term and thus determines the\n",
      "     |      regularization strength. ``alpha = 0`` is equivalent to unpenalized\n",
      "     |      GLMs. In this case, the design matrix `X` must have full column rank\n",
      "     |      (no collinearities).\n",
      "     |  \n",
      "     |  fit_intercept : bool, default=True\n",
      "     |      Specifies if a constant (a.k.a. bias or intercept) should be\n",
      "     |      added to the linear predictor (X @ coef + intercept).\n",
      "     |  \n",
      "     |  link : {'auto', 'identity', 'log'}, default='auto'\n",
      "     |      The link function of the GLM, i.e. mapping from linear predictor\n",
      "     |      `X @ coeff + intercept` to prediction `y_pred`. Option 'auto' sets\n",
      "     |      the link depending on the chosen family as follows:\n",
      "     |  \n",
      "     |      - 'identity' for Normal distribution\n",
      "     |      - 'log' for Poisson,  Gamma and Inverse Gaussian distributions\n",
      "     |  \n",
      "     |  max_iter : int, default=100\n",
      "     |      The maximal number of iterations for the solver.\n",
      "     |  \n",
      "     |  tol : float, default=1e-4\n",
      "     |      Stopping criterion. For the lbfgs solver,\n",
      "     |      the iteration will stop when ``max{|g_j|, j = 1, ..., d} <= tol``\n",
      "     |      where ``g_j`` is the j-th component of the gradient (derivative) of\n",
      "     |      the objective function.\n",
      "     |  \n",
      "     |  warm_start : bool, default=False\n",
      "     |      If set to ``True``, reuse the solution of the previous call to ``fit``\n",
      "     |      as initialization for ``coef_`` and ``intercept_`` .\n",
      "     |  \n",
      "     |  verbose : int, default=0\n",
      "     |      For the lbfgs solver set verbose to any positive number for verbosity.\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  coef_ : array of shape (n_features,)\n",
      "     |      Estimated coefficients for the linear predictor (`X @ coef_ +\n",
      "     |      intercept_`) in the GLM.\n",
      "     |  \n",
      "     |  intercept_ : float\n",
      "     |      Intercept (a.k.a. bias) added to linear predictor.\n",
      "     |  \n",
      "     |  n_iter_ : int\n",
      "     |      Actual number of iterations used in the solver.\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  ----------\n",
      "     |  >>> from sklearn import linear_model\n",
      "     |  >>> clf = linear_model.TweedieRegressor()\n",
      "     |  >>> X = [[1, 2], [2, 3], [3, 4], [4, 3]]\n",
      "     |  >>> y = [2, 3.5, 5, 5.5]\n",
      "     |  >>> clf.fit(X, y)\n",
      "     |  TweedieRegressor()\n",
      "     |  >>> clf.score(X, y)\n",
      "     |  0.839...\n",
      "     |  >>> clf.coef_\n",
      "     |  array([0.599..., 0.299...])\n",
      "     |  >>> clf.intercept_\n",
      "     |  1.600...\n",
      "     |  >>> clf.predict([[1, 1], [3, 4]])\n",
      "     |  array([2.500..., 4.599...])\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      TweedieRegressor\n",
      "     |      GeneralizedLinearRegressor\n",
      "     |      sklearn.base.RegressorMixin\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, *, power=0.0, alpha=1.0, fit_intercept=True, link='auto', max_iter=100, tol=0.0001, warm_start=False, verbose=0)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  family\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from GeneralizedLinearRegressor:\n",
      "     |  \n",
      "     |  fit(self, X, y, sample_weight=None)\n",
      "     |      Fit a Generalized Linear Model.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          Training data.\n",
      "     |      \n",
      "     |      y : array-like of shape (n_samples,)\n",
      "     |          Target values.\n",
      "     |      \n",
      "     |      sample_weight : array-like of shape (n_samples,), default=None\n",
      "     |          Sample weights.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : returns an instance of self.\n",
      "     |  \n",
      "     |  predict(self, X)\n",
      "     |      Predict using GLM with feature matrix X.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          Samples.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      y_pred : array of shape (n_samples,)\n",
      "     |          Returns predicted values.\n",
      "     |  \n",
      "     |  score(self, X, y, sample_weight=None)\n",
      "     |      Compute D^2, the percentage of deviance explained.\n",
      "     |      \n",
      "     |      D^2 is a generalization of the coefficient of determination R^2.\n",
      "     |      R^2 uses squared error and D^2 deviance. Note that those two are equal\n",
      "     |      for ``family='normal'``.\n",
      "     |      \n",
      "     |      D^2 is defined as\n",
      "     |      :math:`D^2 = 1-\\frac{D(y_{true},y_{pred})}{D_{null}}`,\n",
      "     |      :math:`D_{null}` is the null deviance, i.e. the deviance of a model\n",
      "     |      with intercept alone, which corresponds to :math:`y_{pred} = \\bar{y}`.\n",
      "     |      The mean :math:`\\bar{y}` is averaged by sample_weight.\n",
      "     |      Best possible score is 1.0 and it can be negative (because the model\n",
      "     |      can be arbitrarily worse).\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          Test samples.\n",
      "     |      \n",
      "     |      y : array-like of shape (n_samples,)\n",
      "     |          True values of target.\n",
      "     |      \n",
      "     |      sample_weight : array-like of shape (n_samples,), default=None\n",
      "     |          Sample weights.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      score : float\n",
      "     |          D^2 of self.predict(X) w.r.t. y.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.base.RegressorMixin:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |  \n",
      "     |  __repr__(self, N_CHAR_MAX=700)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  get_params(self, deep=True)\n",
      "     |      Get parameters for this estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      deep : bool, default=True\n",
      "     |          If True, will return the parameters for this estimator and\n",
      "     |          contained subobjects that are estimators.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      params : dict\n",
      "     |          Parameter names mapped to their values.\n",
      "     |  \n",
      "     |  set_params(self, **params)\n",
      "     |      Set the parameters of this estimator.\n",
      "     |      \n",
      "     |      The method works on simple estimators as well as on nested objects\n",
      "     |      (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n",
      "     |      parameters of the form ``<component>__<parameter>`` so that it's\n",
      "     |      possible to update each component of a nested object.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      **params : dict\n",
      "     |          Estimator parameters.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : estimator instance\n",
      "     |          Estimator instance.\n",
      "\n",
      "FUNCTIONS\n",
      "    enet_path(X, y, *, l1_ratio=0.5, eps=0.001, n_alphas=100, alphas=None, precompute='auto', Xy=None, copy_X=True, coef_init=None, verbose=False, return_n_iter=False, positive=False, check_input=True, **params)\n",
      "        Compute elastic net path with coordinate descent.\n",
      "        \n",
      "        The elastic net optimization function varies for mono and multi-outputs.\n",
      "        \n",
      "        For mono-output tasks it is::\n",
      "        \n",
      "            1 / (2 * n_samples) * ||y - Xw||^2_2\n",
      "            + alpha * l1_ratio * ||w||_1\n",
      "            + 0.5 * alpha * (1 - l1_ratio) * ||w||^2_2\n",
      "        \n",
      "        For multi-output tasks it is::\n",
      "        \n",
      "            (1 / (2 * n_samples)) * ||Y - XW||_Fro^2\n",
      "            + alpha * l1_ratio * ||W||_21\n",
      "            + 0.5 * alpha * (1 - l1_ratio) * ||W||_Fro^2\n",
      "        \n",
      "        Where::\n",
      "        \n",
      "            ||W||_21 = \\sum_i \\sqrt{\\sum_j w_{ij}^2}\n",
      "        \n",
      "        i.e. the sum of norm of each row.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <elastic_net>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "            Training data. Pass directly as Fortran-contiguous data to avoid\n",
      "            unnecessary memory duplication. If ``y`` is mono-output then ``X``\n",
      "            can be sparse.\n",
      "        \n",
      "        y : {array-like, sparse matrix} of shape (n_samples,) or         (n_samples, n_outputs)\n",
      "            Target values.\n",
      "        \n",
      "        l1_ratio : float, default=0.5\n",
      "            Number between 0 and 1 passed to elastic net (scaling between\n",
      "            l1 and l2 penalties). ``l1_ratio=1`` corresponds to the Lasso.\n",
      "        \n",
      "        eps : float, default=1e-3\n",
      "            Length of the path. ``eps=1e-3`` means that\n",
      "            ``alpha_min / alpha_max = 1e-3``.\n",
      "        \n",
      "        n_alphas : int, default=100\n",
      "            Number of alphas along the regularization path.\n",
      "        \n",
      "        alphas : ndarray, default=None\n",
      "            List of alphas where to compute the models.\n",
      "            If None alphas are set automatically.\n",
      "        \n",
      "        precompute : 'auto', bool or array-like of shape (n_features, n_features),                 default='auto'\n",
      "            Whether to use a precomputed Gram matrix to speed up\n",
      "            calculations. If set to ``'auto'`` let us decide. The Gram\n",
      "            matrix can also be passed as argument.\n",
      "        \n",
      "        Xy : array-like of shape (n_features,) or (n_features, n_outputs),         default=None\n",
      "            Xy = np.dot(X.T, y) that can be precomputed. It is useful\n",
      "            only when the Gram matrix is precomputed.\n",
      "        \n",
      "        copy_X : bool, default=True\n",
      "            If ``True``, X will be copied; else, it may be overwritten.\n",
      "        \n",
      "        coef_init : ndarray of shape (n_features, ), default=None\n",
      "            The initial values of the coefficients.\n",
      "        \n",
      "        verbose : bool or int, default=False\n",
      "            Amount of verbosity.\n",
      "        \n",
      "        return_n_iter : bool, default=False\n",
      "            Whether to return the number of iterations or not.\n",
      "        \n",
      "        positive : bool, default=False\n",
      "            If set to True, forces coefficients to be positive.\n",
      "            (Only allowed when ``y.ndim == 1``).\n",
      "        \n",
      "        check_input : bool, default=True\n",
      "            If set to False, the input validation checks are skipped (including the\n",
      "            Gram matrix when provided). It is assumed that they are handled\n",
      "            by the caller.\n",
      "        \n",
      "        **params : kwargs\n",
      "            Keyword arguments passed to the coordinate descent solver.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        alphas : ndarray of shape (n_alphas,)\n",
      "            The alphas along the path where models are computed.\n",
      "        \n",
      "        coefs : ndarray of shape (n_features, n_alphas) or             (n_outputs, n_features, n_alphas)\n",
      "            Coefficients along the path.\n",
      "        \n",
      "        dual_gaps : ndarray of shape (n_alphas,)\n",
      "            The dual gaps at the end of the optimization for each alpha.\n",
      "        \n",
      "        n_iters : list of int\n",
      "            The number of iterations taken by the coordinate descent optimizer to\n",
      "            reach the specified tolerance for each alpha.\n",
      "            (Is returned when ``return_n_iter`` is set to True).\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        MultiTaskElasticNet\n",
      "        MultiTaskElasticNetCV\n",
      "        ElasticNet\n",
      "        ElasticNetCV\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        For an example, see\n",
      "        :ref:`examples/linear_model/plot_lasso_coordinate_descent_path.py\n",
      "        <sphx_glr_auto_examples_linear_model_plot_lasso_coordinate_descent_path.py>`.\n",
      "    \n",
      "    lars_path(X, y, Xy=None, *, Gram=None, max_iter=500, alpha_min=0, method='lar', copy_X=True, eps=2.220446049250313e-16, copy_Gram=True, verbose=0, return_path=True, return_n_iter=False, positive=False)\n",
      "        Compute Least Angle Regression or Lasso path using LARS algorithm [1]\n",
      "        \n",
      "        The optimization objective for the case method='lasso' is::\n",
      "        \n",
      "        (1 / (2 * n_samples)) * ||y - Xw||^2_2 + alpha * ||w||_1\n",
      "        \n",
      "        in the case of method='lars', the objective function is only known in\n",
      "        the form of an implicit equation (see discussion in [1])\n",
      "        \n",
      "        Read more in the :ref:`User Guide <least_angle_regression>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        X : None or array-like of shape (n_samples, n_features)\n",
      "            Input data. Note that if X is None then the Gram matrix must be\n",
      "            specified, i.e., cannot be None or False.\n",
      "        \n",
      "        y : None or array-like of shape (n_samples,)\n",
      "            Input targets.\n",
      "        \n",
      "        Xy : array-like of shape (n_samples,) or (n_samples, n_targets),             default=None\n",
      "            Xy = np.dot(X.T, y) that can be precomputed. It is useful\n",
      "            only when the Gram matrix is precomputed.\n",
      "        \n",
      "        Gram : None, 'auto', array-like of shape (n_features, n_features),             default=None\n",
      "            Precomputed Gram matrix (X' * X), if ``'auto'``, the Gram\n",
      "            matrix is precomputed from the given X, if there are more samples\n",
      "            than features.\n",
      "        \n",
      "        max_iter : int, default=500\n",
      "            Maximum number of iterations to perform, set to infinity for no limit.\n",
      "        \n",
      "        alpha_min : float, default=0\n",
      "            Minimum correlation along the path. It corresponds to the\n",
      "            regularization parameter alpha parameter in the Lasso.\n",
      "        \n",
      "        method : {'lar', 'lasso'}, default='lar'\n",
      "            Specifies the returned model. Select ``'lar'`` for Least Angle\n",
      "            Regression, ``'lasso'`` for the Lasso.\n",
      "        \n",
      "        copy_X : bool, default=True\n",
      "            If ``False``, ``X`` is overwritten.\n",
      "        \n",
      "        eps : float, default=np.finfo(float).eps\n",
      "            The machine-precision regularization in the computation of the\n",
      "            Cholesky diagonal factors. Increase this for very ill-conditioned\n",
      "            systems. Unlike the ``tol`` parameter in some iterative\n",
      "            optimization-based algorithms, this parameter does not control\n",
      "            the tolerance of the optimization.\n",
      "        \n",
      "        copy_Gram : bool, default=True\n",
      "            If ``False``, ``Gram`` is overwritten.\n",
      "        \n",
      "        verbose : int, default=0\n",
      "            Controls output verbosity.\n",
      "        \n",
      "        return_path : bool, default=True\n",
      "            If ``return_path==True`` returns the entire path, else returns only the\n",
      "            last point of the path.\n",
      "        \n",
      "        return_n_iter : bool, default=False\n",
      "            Whether to return the number of iterations.\n",
      "        \n",
      "        positive : bool, default=False\n",
      "            Restrict coefficients to be >= 0.\n",
      "            This option is only allowed with method 'lasso'. Note that the model\n",
      "            coefficients will not converge to the ordinary-least-squares solution\n",
      "            for small values of alpha. Only coefficients up to the smallest alpha\n",
      "            value (``alphas_[alphas_ > 0.].min()`` when fit_path=True) reached by\n",
      "            the stepwise Lars-Lasso algorithm are typically in congruence with the\n",
      "            solution of the coordinate descent lasso_path function.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        alphas : array-like of shape (n_alphas + 1,)\n",
      "            Maximum of covariances (in absolute value) at each iteration.\n",
      "            ``n_alphas`` is either ``max_iter``, ``n_features`` or the\n",
      "            number of nodes in the path with ``alpha >= alpha_min``, whichever\n",
      "            is smaller.\n",
      "        \n",
      "        active : array-like of shape (n_alphas,)\n",
      "            Indices of active variables at the end of the path.\n",
      "        \n",
      "        coefs : array-like of shape (n_features, n_alphas + 1)\n",
      "            Coefficients along the path\n",
      "        \n",
      "        n_iter : int\n",
      "            Number of iterations run. Returned only if return_n_iter is set\n",
      "            to True.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        lars_path_gram\n",
      "        lasso_path\n",
      "        lasso_path_gram\n",
      "        LassoLars\n",
      "        Lars\n",
      "        LassoLarsCV\n",
      "        LarsCV\n",
      "        sklearn.decomposition.sparse_encode\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] \"Least Angle Regression\", Efron et al.\n",
      "               http://statweb.stanford.edu/~tibs/ftp/lars.pdf\n",
      "        \n",
      "        .. [2] `Wikipedia entry on the Least-angle regression\n",
      "               <https://en.wikipedia.org/wiki/Least-angle_regression>`_\n",
      "        \n",
      "        .. [3] `Wikipedia entry on the Lasso\n",
      "               <https://en.wikipedia.org/wiki/Lasso_(statistics)>`_\n",
      "    \n",
      "    lars_path_gram(Xy, Gram, *, n_samples, max_iter=500, alpha_min=0, method='lar', copy_X=True, eps=2.220446049250313e-16, copy_Gram=True, verbose=0, return_path=True, return_n_iter=False, positive=False)\n",
      "        lars_path in the sufficient stats mode [1]\n",
      "        \n",
      "        The optimization objective for the case method='lasso' is::\n",
      "        \n",
      "        (1 / (2 * n_samples)) * ||y - Xw||^2_2 + alpha * ||w||_1\n",
      "        \n",
      "        in the case of method='lars', the objective function is only known in\n",
      "        the form of an implicit equation (see discussion in [1])\n",
      "        \n",
      "        Read more in the :ref:`User Guide <least_angle_regression>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        Xy : array-like of shape (n_samples,) or (n_samples, n_targets)\n",
      "            Xy = np.dot(X.T, y).\n",
      "        \n",
      "        Gram : array-like of shape (n_features, n_features)\n",
      "            Gram = np.dot(X.T * X).\n",
      "        \n",
      "        n_samples : int or float\n",
      "            Equivalent size of sample.\n",
      "        \n",
      "        max_iter : int, default=500\n",
      "            Maximum number of iterations to perform, set to infinity for no limit.\n",
      "        \n",
      "        alpha_min : float, default=0\n",
      "            Minimum correlation along the path. It corresponds to the\n",
      "            regularization parameter alpha parameter in the Lasso.\n",
      "        \n",
      "        method : {'lar', 'lasso'}, default='lar'\n",
      "            Specifies the returned model. Select ``'lar'`` for Least Angle\n",
      "            Regression, ``'lasso'`` for the Lasso.\n",
      "        \n",
      "        copy_X : bool, default=True\n",
      "            If ``False``, ``X`` is overwritten.\n",
      "        \n",
      "        eps : float, default=np.finfo(float).eps\n",
      "            The machine-precision regularization in the computation of the\n",
      "            Cholesky diagonal factors. Increase this for very ill-conditioned\n",
      "            systems. Unlike the ``tol`` parameter in some iterative\n",
      "            optimization-based algorithms, this parameter does not control\n",
      "            the tolerance of the optimization.\n",
      "        \n",
      "        copy_Gram : bool, default=True\n",
      "            If ``False``, ``Gram`` is overwritten.\n",
      "        \n",
      "        verbose : int, default=0\n",
      "            Controls output verbosity.\n",
      "        \n",
      "        return_path : bool, default=True\n",
      "            If ``return_path==True`` returns the entire path, else returns only the\n",
      "            last point of the path.\n",
      "        \n",
      "        return_n_iter : bool, default=False\n",
      "            Whether to return the number of iterations.\n",
      "        \n",
      "        positive : bool, default=False\n",
      "            Restrict coefficients to be >= 0.\n",
      "            This option is only allowed with method 'lasso'. Note that the model\n",
      "            coefficients will not converge to the ordinary-least-squares solution\n",
      "            for small values of alpha. Only coefficients up to the smallest alpha\n",
      "            value (``alphas_[alphas_ > 0.].min()`` when fit_path=True) reached by\n",
      "            the stepwise Lars-Lasso algorithm are typically in congruence with the\n",
      "            solution of the coordinate descent lasso_path function.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        alphas : array-like of shape (n_alphas + 1,)\n",
      "            Maximum of covariances (in absolute value) at each iteration.\n",
      "            ``n_alphas`` is either ``max_iter``, ``n_features`` or the\n",
      "            number of nodes in the path with ``alpha >= alpha_min``, whichever\n",
      "            is smaller.\n",
      "        \n",
      "        active : array-like of shape (n_alphas,)\n",
      "            Indices of active variables at the end of the path.\n",
      "        \n",
      "        coefs : array-like of shape (n_features, n_alphas + 1)\n",
      "            Coefficients along the path\n",
      "        \n",
      "        n_iter : int\n",
      "            Number of iterations run. Returned only if return_n_iter is set\n",
      "            to True.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        lars_path\n",
      "        lasso_path\n",
      "        lasso_path_gram\n",
      "        LassoLars\n",
      "        Lars\n",
      "        LassoLarsCV\n",
      "        LarsCV\n",
      "        sklearn.decomposition.sparse_encode\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] \"Least Angle Regression\", Efron et al.\n",
      "               http://statweb.stanford.edu/~tibs/ftp/lars.pdf\n",
      "        \n",
      "        .. [2] `Wikipedia entry on the Least-angle regression\n",
      "               <https://en.wikipedia.org/wiki/Least-angle_regression>`_\n",
      "        \n",
      "        .. [3] `Wikipedia entry on the Lasso\n",
      "               <https://en.wikipedia.org/wiki/Lasso_(statistics)>`_\n",
      "    \n",
      "    lasso_path(X, y, *, eps=0.001, n_alphas=100, alphas=None, precompute='auto', Xy=None, copy_X=True, coef_init=None, verbose=False, return_n_iter=False, positive=False, **params)\n",
      "        Compute Lasso path with coordinate descent\n",
      "        \n",
      "        The Lasso optimization function varies for mono and multi-outputs.\n",
      "        \n",
      "        For mono-output tasks it is::\n",
      "        \n",
      "            (1 / (2 * n_samples)) * ||y - Xw||^2_2 + alpha * ||w||_1\n",
      "        \n",
      "        For multi-output tasks it is::\n",
      "        \n",
      "            (1 / (2 * n_samples)) * ||Y - XW||^2_Fro + alpha * ||W||_21\n",
      "        \n",
      "        Where::\n",
      "        \n",
      "            ||W||_21 = \\sum_i \\sqrt{\\sum_j w_{ij}^2}\n",
      "        \n",
      "        i.e. the sum of norm of each row.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <lasso>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "            Training data. Pass directly as Fortran-contiguous data to avoid\n",
      "            unnecessary memory duplication. If ``y`` is mono-output then ``X``\n",
      "            can be sparse.\n",
      "        \n",
      "        y : {array-like, sparse matrix} of shape (n_samples,) or         (n_samples, n_outputs)\n",
      "            Target values\n",
      "        \n",
      "        eps : float, default=1e-3\n",
      "            Length of the path. ``eps=1e-3`` means that\n",
      "            ``alpha_min / alpha_max = 1e-3``\n",
      "        \n",
      "        n_alphas : int, default=100\n",
      "            Number of alphas along the regularization path\n",
      "        \n",
      "        alphas : ndarray, default=None\n",
      "            List of alphas where to compute the models.\n",
      "            If ``None`` alphas are set automatically\n",
      "        \n",
      "        precompute : 'auto', bool or array-like of shape (n_features, n_features),                 default='auto'\n",
      "            Whether to use a precomputed Gram matrix to speed up\n",
      "            calculations. If set to ``'auto'`` let us decide. The Gram\n",
      "            matrix can also be passed as argument.\n",
      "        \n",
      "        Xy : array-like of shape (n_features,) or (n_features, n_outputs),         default=None\n",
      "            Xy = np.dot(X.T, y) that can be precomputed. It is useful\n",
      "            only when the Gram matrix is precomputed.\n",
      "        \n",
      "        copy_X : bool, default=True\n",
      "            If ``True``, X will be copied; else, it may be overwritten.\n",
      "        \n",
      "        coef_init : ndarray of shape (n_features, ), default=None\n",
      "            The initial values of the coefficients.\n",
      "        \n",
      "        verbose : bool or int, default=False\n",
      "            Amount of verbosity.\n",
      "        \n",
      "        return_n_iter : bool, default=False\n",
      "            whether to return the number of iterations or not.\n",
      "        \n",
      "        positive : bool, default=False\n",
      "            If set to True, forces coefficients to be positive.\n",
      "            (Only allowed when ``y.ndim == 1``).\n",
      "        \n",
      "        **params : kwargs\n",
      "            keyword arguments passed to the coordinate descent solver.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        alphas : ndarray of shape (n_alphas,)\n",
      "            The alphas along the path where models are computed.\n",
      "        \n",
      "        coefs : ndarray of shape (n_features, n_alphas) or             (n_outputs, n_features, n_alphas)\n",
      "            Coefficients along the path.\n",
      "        \n",
      "        dual_gaps : ndarray of shape (n_alphas,)\n",
      "            The dual gaps at the end of the optimization for each alpha.\n",
      "        \n",
      "        n_iters : list of int\n",
      "            The number of iterations taken by the coordinate descent optimizer to\n",
      "            reach the specified tolerance for each alpha.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        For an example, see\n",
      "        :ref:`examples/linear_model/plot_lasso_coordinate_descent_path.py\n",
      "        <sphx_glr_auto_examples_linear_model_plot_lasso_coordinate_descent_path.py>`.\n",
      "        \n",
      "        To avoid unnecessary memory duplication the X argument of the fit method\n",
      "        should be directly passed as a Fortran-contiguous numpy array.\n",
      "        \n",
      "        Note that in certain cases, the Lars solver may be significantly\n",
      "        faster to implement this functionality. In particular, linear\n",
      "        interpolation can be used to retrieve model coefficients between the\n",
      "        values output by lars_path\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        \n",
      "        Comparing lasso_path and lars_path with interpolation:\n",
      "        \n",
      "        >>> X = np.array([[1, 2, 3.1], [2.3, 5.4, 4.3]]).T\n",
      "        >>> y = np.array([1, 2, 3.1])\n",
      "        >>> # Use lasso_path to compute a coefficient path\n",
      "        >>> _, coef_path, _ = lasso_path(X, y, alphas=[5., 1., .5])\n",
      "        >>> print(coef_path)\n",
      "        [[0.         0.         0.46874778]\n",
      "         [0.2159048  0.4425765  0.23689075]]\n",
      "        \n",
      "        >>> # Now use lars_path and 1D linear interpolation to compute the\n",
      "        >>> # same path\n",
      "        >>> from sklearn.linear_model import lars_path\n",
      "        >>> alphas, active, coef_path_lars = lars_path(X, y, method='lasso')\n",
      "        >>> from scipy import interpolate\n",
      "        >>> coef_path_continuous = interpolate.interp1d(alphas[::-1],\n",
      "        ...                                             coef_path_lars[:, ::-1])\n",
      "        >>> print(coef_path_continuous([5., 1., .5]))\n",
      "        [[0.         0.         0.46915237]\n",
      "         [0.2159048  0.4425765  0.23668876]]\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        lars_path\n",
      "        Lasso\n",
      "        LassoLars\n",
      "        LassoCV\n",
      "        LassoLarsCV\n",
      "        sklearn.decomposition.sparse_encode\n",
      "    \n",
      "    orthogonal_mp(X, y, *, n_nonzero_coefs=None, tol=None, precompute=False, copy_X=True, return_path=False, return_n_iter=False)\n",
      "        Orthogonal Matching Pursuit (OMP).\n",
      "        \n",
      "        Solves n_targets Orthogonal Matching Pursuit problems.\n",
      "        An instance of the problem has the form:\n",
      "        \n",
      "        When parametrized by the number of non-zero coefficients using\n",
      "        `n_nonzero_coefs`:\n",
      "        argmin ||y - X\\gamma||^2 subject to ||\\gamma||_0 <= n_{nonzero coefs}\n",
      "        \n",
      "        When parametrized by error using the parameter `tol`:\n",
      "        argmin ||\\gamma||_0 subject to ||y - X\\gamma||^2 <= tol\n",
      "        \n",
      "        Read more in the :ref:`User Guide <omp>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        X : ndarray of shape (n_samples, n_features)\n",
      "            Input data. Columns are assumed to have unit norm.\n",
      "        \n",
      "        y : ndarray of shape (n_samples,) or (n_samples, n_targets)\n",
      "            Input targets.\n",
      "        \n",
      "        n_nonzero_coefs : int, default=None\n",
      "            Desired number of non-zero entries in the solution. If None (by\n",
      "            default) this value is set to 10% of n_features.\n",
      "        \n",
      "        tol : float, default=None\n",
      "            Maximum norm of the residual. If not None, overrides n_nonzero_coefs.\n",
      "        \n",
      "        precompute : 'auto' or bool, default=False\n",
      "            Whether to perform precomputations. Improves performance when n_targets\n",
      "            or n_samples is very large.\n",
      "        \n",
      "        copy_X : bool, default=True\n",
      "            Whether the design matrix X must be copied by the algorithm. A false\n",
      "            value is only helpful if X is already Fortran-ordered, otherwise a\n",
      "            copy is made anyway.\n",
      "        \n",
      "        return_path : bool, default=False\n",
      "            Whether to return every value of the nonzero coefficients along the\n",
      "            forward path. Useful for cross-validation.\n",
      "        \n",
      "        return_n_iter : bool, default=False\n",
      "            Whether or not to return the number of iterations.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        coef : ndarray of shape (n_features,) or (n_features, n_targets)\n",
      "            Coefficients of the OMP solution. If `return_path=True`, this contains\n",
      "            the whole coefficient path. In this case its shape is\n",
      "            (n_features, n_features) or (n_features, n_targets, n_features) and\n",
      "            iterating over the last axis yields coefficients in increasing order\n",
      "            of active features.\n",
      "        \n",
      "        n_iters : array-like or int\n",
      "            Number of active features across every target. Returned only if\n",
      "            `return_n_iter` is set to True.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        OrthogonalMatchingPursuit\n",
      "        orthogonal_mp_gram\n",
      "        lars_path\n",
      "        sklearn.decomposition.sparse_encode\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        Orthogonal matching pursuit was introduced in S. Mallat, Z. Zhang,\n",
      "        Matching pursuits with time-frequency dictionaries, IEEE Transactions on\n",
      "        Signal Processing, Vol. 41, No. 12. (December 1993), pp. 3397-3415.\n",
      "        (http://blanche.polytechnique.fr/~mallat/papiers/MallatPursuit93.pdf)\n",
      "        \n",
      "        This implementation is based on Rubinstein, R., Zibulevsky, M. and Elad,\n",
      "        M., Efficient Implementation of the K-SVD Algorithm using Batch Orthogonal\n",
      "        Matching Pursuit Technical Report - CS Technion, April 2008.\n",
      "        https://www.cs.technion.ac.il/~ronrubin/Publications/KSVD-OMP-v2.pdf\n",
      "    \n",
      "    orthogonal_mp_gram(Gram, Xy, *, n_nonzero_coefs=None, tol=None, norms_squared=None, copy_Gram=True, copy_Xy=True, return_path=False, return_n_iter=False)\n",
      "        Gram Orthogonal Matching Pursuit (OMP).\n",
      "        \n",
      "        Solves n_targets Orthogonal Matching Pursuit problems using only\n",
      "        the Gram matrix X.T * X and the product X.T * y.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <omp>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        Gram : ndarray of shape (n_features, n_features)\n",
      "            Gram matrix of the input data: X.T * X.\n",
      "        \n",
      "        Xy : ndarray of shape (n_features,) or (n_features, n_targets)\n",
      "            Input targets multiplied by X: X.T * y.\n",
      "        \n",
      "        n_nonzero_coefs : int, default=None\n",
      "            Desired number of non-zero entries in the solution. If None (by\n",
      "            default) this value is set to 10% of n_features.\n",
      "        \n",
      "        tol : float, default=None\n",
      "            Maximum norm of the residual. If not None, overrides n_nonzero_coefs.\n",
      "        \n",
      "        norms_squared : array-like of shape (n_targets,), default=None\n",
      "            Squared L2 norms of the lines of y. Required if tol is not None.\n",
      "        \n",
      "        copy_Gram : bool, default=True\n",
      "            Whether the gram matrix must be copied by the algorithm. A false\n",
      "            value is only helpful if it is already Fortran-ordered, otherwise a\n",
      "            copy is made anyway.\n",
      "        \n",
      "        copy_Xy : bool, default=True\n",
      "            Whether the covariance vector Xy must be copied by the algorithm.\n",
      "            If False, it may be overwritten.\n",
      "        \n",
      "        return_path : bool, default=False\n",
      "            Whether to return every value of the nonzero coefficients along the\n",
      "            forward path. Useful for cross-validation.\n",
      "        \n",
      "        return_n_iter : bool, default=False\n",
      "            Whether or not to return the number of iterations.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        coef : ndarray of shape (n_features,) or (n_features, n_targets)\n",
      "            Coefficients of the OMP solution. If `return_path=True`, this contains\n",
      "            the whole coefficient path. In this case its shape is\n",
      "            (n_features, n_features) or (n_features, n_targets, n_features) and\n",
      "            iterating over the last axis yields coefficients in increasing order\n",
      "            of active features.\n",
      "        \n",
      "        n_iters : array-like or int\n",
      "            Number of active features across every target. Returned only if\n",
      "            `return_n_iter` is set to True.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        OrthogonalMatchingPursuit\n",
      "        orthogonal_mp\n",
      "        lars_path\n",
      "        sklearn.decomposition.sparse_encode\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        Orthogonal matching pursuit was introduced in G. Mallat, Z. Zhang,\n",
      "        Matching pursuits with time-frequency dictionaries, IEEE Transactions on\n",
      "        Signal Processing, Vol. 41, No. 12. (December 1993), pp. 3397-3415.\n",
      "        (http://blanche.polytechnique.fr/~mallat/papiers/MallatPursuit93.pdf)\n",
      "        \n",
      "        This implementation is based on Rubinstein, R., Zibulevsky, M. and Elad,\n",
      "        M., Efficient Implementation of the K-SVD Algorithm using Batch Orthogonal\n",
      "        Matching Pursuit Technical Report - CS Technion, April 2008.\n",
      "        https://www.cs.technion.ac.il/~ronrubin/Publications/KSVD-OMP-v2.pdf\n",
      "    \n",
      "    ridge_regression(X, y, alpha, *, sample_weight=None, solver='auto', max_iter=None, tol=0.001, verbose=0, random_state=None, return_n_iter=False, return_intercept=False, check_input=True)\n",
      "        Solve the ridge equation by the method of normal equations.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <ridge_regression>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        X : {ndarray, sparse matrix, LinearOperator} of shape         (n_samples, n_features)\n",
      "            Training data\n",
      "        \n",
      "        y : ndarray of shape (n_samples,) or (n_samples, n_targets)\n",
      "            Target values\n",
      "        \n",
      "        alpha : float or array-like of shape (n_targets,)\n",
      "            Regularization strength; must be a positive float. Regularization\n",
      "            improves the conditioning of the problem and reduces the variance of\n",
      "            the estimates. Larger values specify stronger regularization.\n",
      "            Alpha corresponds to ``1 / (2C)`` in other linear models such as\n",
      "            :class:`~sklearn.linear_model.LogisticRegression` or\n",
      "            :class:`~sklearn.svm.LinearSVC`. If an array is passed, penalties are\n",
      "            assumed to be specific to the targets. Hence they must correspond in\n",
      "            number.\n",
      "        \n",
      "        sample_weight : float or array-like of shape (n_samples,), default=None\n",
      "            Individual weights for each sample. If given a float, every sample\n",
      "            will have the same weight. If sample_weight is not None and\n",
      "            solver='auto', the solver will be set to 'cholesky'.\n",
      "        \n",
      "            .. versionadded:: 0.17\n",
      "        \n",
      "        solver : {'auto', 'svd', 'cholesky', 'lsqr', 'sparse_cg', 'sag', 'saga'},         default='auto'\n",
      "            Solver to use in the computational routines:\n",
      "        \n",
      "            - 'auto' chooses the solver automatically based on the type of data.\n",
      "        \n",
      "            - 'svd' uses a Singular Value Decomposition of X to compute the Ridge\n",
      "              coefficients. More stable for singular matrices than 'cholesky'.\n",
      "        \n",
      "            - 'cholesky' uses the standard scipy.linalg.solve function to\n",
      "              obtain a closed-form solution via a Cholesky decomposition of\n",
      "              dot(X.T, X)\n",
      "        \n",
      "            - 'sparse_cg' uses the conjugate gradient solver as found in\n",
      "              scipy.sparse.linalg.cg. As an iterative algorithm, this solver is\n",
      "              more appropriate than 'cholesky' for large-scale data\n",
      "              (possibility to set `tol` and `max_iter`).\n",
      "        \n",
      "            - 'lsqr' uses the dedicated regularized least-squares routine\n",
      "              scipy.sparse.linalg.lsqr. It is the fastest and uses an iterative\n",
      "              procedure.\n",
      "        \n",
      "            - 'sag' uses a Stochastic Average Gradient descent, and 'saga' uses\n",
      "              its improved, unbiased version named SAGA. Both methods also use an\n",
      "              iterative procedure, and are often faster than other solvers when\n",
      "              both n_samples and n_features are large. Note that 'sag' and\n",
      "              'saga' fast convergence is only guaranteed on features with\n",
      "              approximately the same scale. You can preprocess the data with a\n",
      "              scaler from sklearn.preprocessing.\n",
      "        \n",
      "        \n",
      "            All last five solvers support both dense and sparse data. However, only\n",
      "            'sag' and 'sparse_cg' supports sparse input when `fit_intercept` is\n",
      "            True.\n",
      "        \n",
      "            .. versionadded:: 0.17\n",
      "               Stochastic Average Gradient descent solver.\n",
      "            .. versionadded:: 0.19\n",
      "               SAGA solver.\n",
      "        \n",
      "        max_iter : int, default=None\n",
      "            Maximum number of iterations for conjugate gradient solver.\n",
      "            For the 'sparse_cg' and 'lsqr' solvers, the default value is determined\n",
      "            by scipy.sparse.linalg. For 'sag' and saga solver, the default value is\n",
      "            1000.\n",
      "        \n",
      "        tol : float, default=1e-3\n",
      "            Precision of the solution.\n",
      "        \n",
      "        verbose : int, default=0\n",
      "            Verbosity level. Setting verbose > 0 will display additional\n",
      "            information depending on the solver used.\n",
      "        \n",
      "        random_state : int, RandomState instance, default=None\n",
      "            Used when ``solver`` == 'sag' or 'saga' to shuffle the data.\n",
      "            See :term:`Glossary <random_state>` for details.\n",
      "        \n",
      "        return_n_iter : bool, default=False\n",
      "            If True, the method also returns `n_iter`, the actual number of\n",
      "            iteration performed by the solver.\n",
      "        \n",
      "            .. versionadded:: 0.17\n",
      "        \n",
      "        return_intercept : bool, default=False\n",
      "            If True and if X is sparse, the method also returns the intercept,\n",
      "            and the solver is automatically changed to 'sag'. This is only a\n",
      "            temporary fix for fitting the intercept with sparse data. For dense\n",
      "            data, use sklearn.linear_model._preprocess_data before your regression.\n",
      "        \n",
      "            .. versionadded:: 0.17\n",
      "        \n",
      "        check_input : bool, default=True\n",
      "            If False, the input arrays X and y will not be checked.\n",
      "        \n",
      "            .. versionadded:: 0.21\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        coef : ndarray of shape (n_features,) or (n_targets, n_features)\n",
      "            Weight vector(s).\n",
      "        \n",
      "        n_iter : int, optional\n",
      "            The actual number of iteration performed by the solver.\n",
      "            Only returned if `return_n_iter` is True.\n",
      "        \n",
      "        intercept : float or ndarray of shape (n_targets,)\n",
      "            The intercept of the model. Only returned if `return_intercept`\n",
      "            is True and if X is a scipy sparse array.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        This function won't compute the intercept.\n",
      "\n",
      "DATA\n",
      "    __all__ = ['ARDRegression', 'BayesianRidge', 'ElasticNet', 'ElasticNet...\n",
      "\n",
      "FILE\n",
      "    c:\\users\\whs38\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\__init__.py\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "be41c4cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "27580b35",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv(\"C://Users//whs38//OneDrive//바탕 화면//archive//auto_mpg.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f2cd7205",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 398 entries, 0 to 397\n",
      "Data columns (total 7 columns):\n",
      " #   Column        Non-Null Count  Dtype  \n",
      "---  ------        --------------  -----  \n",
      " 0   mpg           398 non-null    float64\n",
      " 1   cylinders     398 non-null    int64  \n",
      " 2   displacement  398 non-null    float64\n",
      " 3   horsepower    396 non-null    float64\n",
      " 4   weight        398 non-null    int64  \n",
      " 5   acceleration  398 non-null    float64\n",
      " 6   model-year    398 non-null    int64  \n",
      "dtypes: float64(4), int64(3)\n",
      "memory usage: 21.9 KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3ae01afc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "df.plot(kind=\"scatter\",x=\"cylinders\",y=\"mpg\")\n",
    "plt.show()\n",
    "\n",
    "df.plot(kind=\"scatter\",x=\"displacement\",y=\"mpg\")\n",
    "plt.show()\n",
    "\n",
    "df.plot(kind=\"scatter\",x=\"horsepower\",y=\"mpg\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "114c9a0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX4AAAEGCAYAAABiq/5QAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/MnkTPAAAACXBIWXMAAAsTAAALEwEAmpwYAABBGklEQVR4nO2de5gU9ZX3v6eqLwwz3BwQgWFEMxIzEBh1VpaArqLJGkHcrEiyXnA3a0j2EZM1KpjNEi88yapozEbZ7Et83Y2RZJfgRgy47xsVfBEvuGAGBLwwiQoMhMsIOIPQM9193j+qqqmurqqu6unqrp4+n+eZZ3qqf1W/U9U9p351rsTMEARBEKoHpdwCCIIgCKVFFL8gCEKVIYpfEAShyhDFLwiCUGWI4hcEQagyIuUWwAvDhw/ncePGlVsMQRCEimLLli2HmXmEdXtFKP5x48Zh8+bN5RZDEAShoiCiD+22i6lHEAShyhDFLwiCUGWI4hcEQagyRPELgiBUGaL4BUEQqgxR/CGhszuBrXuOorM7UW5RBEHo51REOGd/Z3VbBxY9vQ1RRUFvOo0Hr5mE2S1jyi2WIAj9FFnxl5nO7gQWPb0NJ3vT6EokcbI3jYVPb5OVvyAIgSGKv8zsPXICUSX7Y4gqCvYeOVEmiQRB6O+I4i8zDcNq0JtOZ23rTafRMKymTBIJgtDfEcVfZurr4njwmkkYEFUwKB7BgKiCB6+ZhPq6eLlFEwShnyLO3RAwu2UMpjUNx94jJ9AwrEaUviAIgSKKPyTU18VF4QuCUBICN/UQkUpEvyOiNfrf9xBRBxG16T9XBi2DIAiCcIpSrPi/BeBtAINN2x5h5odKMLcgCIJgIdAVPxE1AJgJ4PEg5xEEQRC8E7Sp50cAFgJIW7YvIKJtRPQEEQ0LWIaiIqUVBEGodAJT/EQ0C8BBZt5ieesnAD4FoAXAfgAPO+w/n4g2E9HmQ4cOBSWmL1a3dWDaA+tww+ObMO2BdXi2raPPx5QbiSAIpYaYOZgDE/0TgBsBJAEMgGbj/y9mvsE0ZhyANcw80e1Yra2tXO7Wi53dCUx7YB1O9p56eBkQVfDKohkFR+NIjR5BEIKEiLYwc6t1e2Arfmb+DjM3MPM4AF8BsI6ZbyCiUaZhXwKwPSgZikmxSys41ehpP9AlTwCCIARKOeL4HySiFgAM4AMAXy+DDL4pdmkF40Zy0uT+4DTjykc3Iq7KE4AgCMFRkpINzPwSM8/SX9/IzJ9l5knMPJuZ95dChr5S7NIKdjeSRIrRk5QqnYIgBItk7vqgmKUVjBvJQt3Gn0imoCiU5UMwTEl+5+nsTkj5B0EQHBHF75NillYw30hqYypmPbYx6/1CTEniMBYEIR9SnTNAvIRq1tfFMXnsUDSNHNRnU5I0dREEwQuy4g+IQlbefTUl2TmMCzUXCYLQfxHFHwDmlbehhBc+vQ3TmobnVcB9MSVJUxdBELwgpp4AKFc7RWnqIgiCF2TFHwDlXHlLUxdBEPIhK/4AKPfK23AYi9IXBMEOWfEHhKy8BUEIK6L4A0TaKQqCEEbE1BNCpFSzIAhBIiv+kCGZt4IgBI2s+EOEZN4KglAKRPF7pBTml3LF/wuCUF2IqccDpTC/dHYncOxED3pSqaztknkrCEKxEcWfh76UX/CK+caSZiCiADXRSOYmI5FBgiAUE1H8eQi68JndjSUeUbDs+vMxYfRgUfqCIBQdsfHnIejyC3Z2/ZiqYEhNVJS+IAiBELjiJyKViH5HRGv0v08joueJaJf+e1jQMvSFoMsvSEVNQRBKTSlMPd8C8DaAwfrfdwF4kZnvJ6K79L8XlUCOggmy/IK1BaPY9QVBCJpAFT8RNQCYCeD7AL6tb74awCX6658BeAkhV/xAsOUXpK6PIAilJOgV/48ALAQwyLRtJDPvBwBm3k9Ep9vtSETzAcwHgMbGxoDFLA/Wpuii8AVBKAWBKX4imgXgIDNvIaJL/O7PzMsBLAeA1tZWLq505UdKMwiCUC6CdO5OAzCbiD4A8B8AZhDRUwAOENEoANB/HwxQBl/4yc7tSyavlGYQBKGcBLbiZ+bvAPgOAOgr/juY+QYiWgrgJgD3679XByWDH/yswPu6Wpem6IIglJNyxPHfD+DzRLQLwOf1v8uKnxV4MVbrEsIpCEI5KYniZ+aXmHmW/rqTmS9j5nP03x+VQgY3/BRHK0YhtXK3ZhQEobqRkg3wtwIv1mpdQjgFQSgXUrIB/lbgXse2H+jCqs17sPn9TkcnsLUpunTe8o9cM0HwDzGHP1KytbWVN2/eHPg81rj6Qsd+75m38OTruzN/R1WCqlCgDuNqRK6ZILhDRFuYudW6XVb8Jqwr8ELGth/oylL6ANCb4iwnsHWVKuGd/pFrJgiFIzb+ItO256jje1FFwYpNu/EvL7VnrVLPrK+V8E6fSEisIBSOrPiLTMvYoY7v9aTSWLZ+V84qtTam5jiMe1IS3umGhMQKQuGI4i8yTSMHYd7U7NpCUZUQjyi45vwxiKq5oaDHe1J48JpJiKqU2Z5Kp/FK++GSyFyJSEisIBSOmHoC4L6rP4vZk0Zjw67DmNwwBDv2d2HZ+l145ncd+KTXfpXaMKwGyim9j2S6+C0ezfhxZIcVCYkVhMIQxR8A5miTnlQaqXQayTQAnIqgiqkERY/0qa+LY+ueo4ipKhLJZGZMUDbr/hQNI1VNBcE/oviLjF0PXTsYwJoF09E0UqtYXSqbdSmaxwuCEG7Exl9k7Eo62BGLaLZ9g1LZrItRcqJcSLKWIBQHWfG70H6gC217jqJl7NDMyjwfdiv3iALd1HOKVJpzVvOlsFlXajRMfzJPCUK5kRW/A9975i1c/sgG3LFqGy5/ZANueWqLp5WmeeVeG1MRUwn3zp6I739pImIRBbVx1XE1b3a4AshJ8ipktWvdrxKjYSRZSxCKi6z4bbDLvl27/Y94/u0/4qFrW/KuNGe3jEHXySTuXbMTsYiCJWt34sFrJuG1u2Y4rubNK9oTvUkQEQZEtPj+uRc0YOWWvb5Xu06r5EqLhpFkLUEoLrLit8Ep+7YnBduVpl0JhiVrd6InmUZ3IpVZoQKwLfNgXdEm01qZB2N1++Tru32vdvOtkv2Upyg3lWqeEoSwIorfBrfsW6sjdHVbB6Y9sA43PL4J0x5Yh2fbOnw7UL06hL0cy+2YleLEtVKJ5ilBCDNi6rHByL598rXdOe+ZV5pOoZFrFkz3tUK1W9G64WW1299WyZVmnhKEMBPYip+IBhDRG0S0lYh2ENG9+vZ7iKiDiNr0nyuDkqEQh6ixz7cuG48XbrsY11/YiJgK25Wm06raKMHgdYVqXdFGFK3Mg7HvvKmNvle7ha6SwxwyWUnmKUEIM4HV4yciAlDLzN1EFAWwEcC3AFwBoJuZH/J6rELq8RcS/ue0j1N5g87uBKY9sA4nTWUYBkQVvLJoBurr4r7LIlijesz7Flpiwc9+EjIpCP0Lp3r8gZl6WLujdOt/RvWfknR9KSQ7Nd8+bt24FlqUpTHWbzkB63i714aN3utxvcogGb2CUD0EauMnIhXAFgBNAJYx8yYi+iKABUQ0D8BmALcz8xGbfecDmA8AjY2N1rddKST8r9CQQb+2Z68rcOs4r6vxQp8MJGRSEKqHQBU/M6cAtBDRUAC/JqKJAH4CYAm01f8SAA8D+KrNvssBLAc0U4+feQtxbPbFGep1Ve1VeVvHffvy8Xjot++iJ8Wuq/G+mGr6mzNYEARnShLOycxHAbwE4ApmPsDMKWZOA/gpgAuLPV8hjs1ihwx2diew4b1D2PDewUy7RS/Zp3bjfvDf76AnlX3vU4mw/p2Dvto3ujluJWRSEKqHwFb8RDQCQC8zHyWiGgCXA3iAiEYx83592JcAbA9i/kLC/4oVMri6rQO3r2zL1OeJqoRvzjjHkynFzuRix/GeFO75zQ784+rtnto3enkakJBJQagOgjT1jALwM93OrwBYycxriOjnRNQCzdTzAYCvByVAIbXa+1rfvbM7gYWrtmYVZetNMR5dtwtaoJNpu40ppWFYDXpSKXihO6GNy5c74MdxK/XtBaH/E5iph5m3MfN5zDyJmScy83369huZ+bP69tmm1X+/YMe+YwAoZ3tEVbDg0qYsU8riWc3Ye+RElullY/thpH3GPuXLHehrFm+YY/sFQfCPZO4WkdVtHVi4aisSyVzNnUozrpvSiOumNGLvkRPY3nEMS9bszDK9TGsajkVPb0OvyZ4fVQnpNCPlcjMwVvaTxw61NdX0xXErsf2C0P+QWj1FwjCn2Cn9qEpYOmdSxozSMKwGS9buzHHE7th3LGdlPiCiYvGsz6AmmvtRDYzllni2y27tSxavlEMWhP6HrPhd8NOIxc4pWxNRsOiL5+KqyaPzOnA1hU+2K/OJo4fkrPjjEcK/3nA+JoweEpjj2i2233hfnMCCUHmI4nfge8+8lVWTf97URtx39Wcdx9uZU9JgnD2i1tPY3nQaE0YPzskEnntBA2544g2QXlpjgL7yf/CaSbh4/Om+zsmv49ZJzu0dx/Dl5a8Fbv4pNBlNEAR3AqvVU0wKqdXTF9oPdOHyRzbkbH/htotdV/7PtnVklLa1mYpVOZrHWt83FF5tTMWsxzZm1QKKqYTnvnmR51aQfcUq5+KZzRkzlYG5PlGxEN+CIPSdktfqqWScGrG07TnqqnANc8qOfR/ja09uRiKZRm8qCSA3fHJa03Asv/ECAIQJowfn2OTr6+LYuudojqklHlGzmrQb2K2Oi7FitpqISlHaQeoGCUKwiOK3wakRi1uDFoP6ujiG1EQRUxUkkv6SqayKOl80jjF+e8cxLFmbHSHEQNFWzFYTUdClHaRukCAEiyh+G+wascxtbcDxnhQ6uxN5lY9dElYimUZtTHVczXadTOYo79ktYxyrfxo3D5Uo8wRgHO/OVVsBEBLJ4q+YnSqSAlpz+GLY46VukCAEi9j4XTCiej463oMfvvCe6+rZvFrf2H4Yd/xqa1Y8PgCoBMy/+Gw89fpudCWSme21cRW9yXRWPR63uv52fQDMRBVCNKLgE5NJaFA8gqdunoLJHp5avGA932Lb4918IIIgeENs/AXQNHIQhtXGMkrWafVsNt30pFJIM3KUPgCkGPjJ//sDoqqldEOKEVWVrKcEs2nDamrJV8+nN82gVLArZkOeHfs+xsJV24r+dCF1gwQhOCSBKw/5yh1Yk5wSSbZV+mZSaUY8ciqZ6u6rmpGyPHn1pUfvgKiCW2ecE2ilTaPJ/Dd+viXLlwEUr6m7tFoUhGCQFX8e8tmbvVbTNJNmIJ1OY/6lTbhuSiPq6+IYFI84dvKyYrazm238ZszlIQpdMbu1nDRudnaIPV4Qwk3VKH4/oY3WsW7tFe1uDCoBqkI5NfTN9KaBZS+147opWncxv6YN8/jt+3Lr/phLOBSCNfJo8cxmTBwzxDGkE9BKSKSZM/NLApYghJOqUPx+koGcxjopZfONAUBmFeym9A2sIYpeMmutytQwh1wx4YyiKVm7yKPvPrMddXEVyTRj8azmnJudtYSEJGAJQnjp9zZ+P4XG3Ma62Ztnt4zBmgXTkdbrKXvQ+QD8m0QMu/oNj2/CtAfW4dm2jsx7xbSH2/k1AK3+/8neNJas2YnFM5uzfAhL50zGxeNPz6z0w1zcTcpMC9VOv1/x+0kG6kvi0PGeFOIRFT2ppOs4AKiJKkilGYtnNntS1J3dicCiZ+zI5zyOKgomjhmCVxbNsH3KCHMCljyJCEIVrPj9JAP1JXEon7I0UEmL6olFFCxZuzNr1W5HKaJnrJjLONfG1Jz3jWvi9JQR1gSssD+JCEKpCEzxE9EAInqDiLYS0Q4iulfffhoRPU9Eu/Tfw4KSAfBXi74vDcfN+8b1OP0BUQVRlRBRtASqeISg6E5fw2zipnjMiuqT3tzInSCV6eyWMXhl0Qz84mt/iu9/aWJZG9cXi752IhOE/kKQpp4EgBnM3E1EUQAbiei/AfwlgBeZ+X4iugvAXQAWBSiHr4iZviQOmfetjanYd+wEAMLoIQNwvCeFYyd6cMuK32UKtwEAgbBj37GcEsvtB7rw7NZ9UHO7OOZEz+RDMxVpLSGtBeHc6IvzOIwJWF6eRCQSSagGAlP8rNWC6Nb/jOo/DOBqAJfo238G4CUErPgBf7XoncZ6UQrGvna25GlNw3MUz4neFP7m3/4Hj3y5JWNrtvYCMOO3Acvqto6s8hERBfjh3Bbfdm2/tfzDqEDzheaK/V+oFgKt1UNEKoAtAJoALGPmRUR0lJmHmsYcYWZXc0+5avUYdHYnsGLTbixb346Yml8p2NXSMWrv/J8df8R3f709Z594hPDqXZfh/UPdmPO/Xs95f2BMQZrhSxl1difwufvX5fgGjLmCUshhV6BOJaydPrOw3LgEwS9lqdXDzCkALUQ0FMCviWii132JaD6A+QDQ2NgYjIAesDZQN5SoW0SNW1TLxNFDUBNVcKLXmvSlYMWm3Xh03S5bOW6efjZu+tw4X0po75ETUJVcW5FKwUXYVEItfbunlzBHIglCsSlJVA8zH4Vm0rkCwAEiGgUA+u+DDvssZ+ZWZm4dMWJEKcTMwbWBuotT0M2W3DCsBmmbh6xkOoVl69sd6/zMtvTt9ULDsBqkbCZLcXBO4Up1oIY1EkkQgiDIqJ4R+kofRFQD4HIA7wB4FsBN+rCbAKwOSoa+4pTIBACJVDon1HHz+5344W/fxfuHul2jWhZc2oSI6bARBbh1xnjEVPu55rY2oGnkIN+JR/V1cSydMymrGmhEAZbOmewpQ7iQJKdKVaBhjUQShCDwZOMnor+02XwMwFvMbLtiJ6JJ0Jy3KrQbzEpmvo+I6gGsBNAIYDeAa5n5I7f5y2Xjd6t7H1cJpFDGfn3D469jY3tn5v2Lmurxo6+cl2VLzi7fnMZXp43D1E8Nx4TRgwHAdq6aCIGJMLe1ASs37y3Ibu43qsfNRu/FaVvJtfTD6JQWhEJxsvF7VfxrAUwFsF7fdAmA1wGMB3AfM/+8eKLmUk7nrlmJJZIppNKcVZJhQFTBsr86D3/75JacfVd9/U/RelY9AG/OQ2MuVSEcT+TG7ZsJyvHoJqefhiuiQAWh/PTVuZsG8BlmPqAfbCSAnwCYAmADgEAVfzkxx6MfO9GLW1a8mdU9K6oo+O3OA7b7bth1OKP4vTgPjbnWv3MQdz+7w7bcstO+xcJJzh37jrk6be2Kx4nCF4Rw4tXGP85Q+joHAYzXTTS9xRcrXBhJTBNGD7a1X3+heaTtfhefMzzz2qvtu74ujkvPPT2nMYuVoOzmTnIC5Oi0dSseJwhC+PCq+F8mojVEdBMR3QTNQbuBiGoBHA1MupDh5AC8rPkMXNRUnzX2oqb6zGrfbV/zqrj9QBdWbd6DI8d7sHhmM6IqoSaqIB5RMG9qY86+gNbgvP1AV5+qTZoduU5y2t30TiaT6E2mcurf3Llqa0XWv5GqnUK14NXGT9BKLUwHQAA2AniaS9SpvdwJXFac7Neb3+/Ehl2HcfE5w7OUvpd9rdm6CiET9qkS8MiXW7JKIBj2dk4zEinGgKh2D/frSHVy5NrJ+WxbB769sg3mfLCIQlAJSFjCUG///Hjcetk5jvOGzQcQ9qQzQSiEPjl39QOcAc2mnwbwP8z8x+KK6EzYFH+xaT/Qhcsf2eA6xpxt6xZt5Mfp6zdbtbM7gan/9KKnJjPxiIJX77I/TtiUrGTtCv0VJ8XvydRDRDcDeAPAlwDMAfA6EX21uCJWL217juYdY2TbAu75BX6SpbwmWxkmkB37jiFiM2/UppJcTLWXI4ylkSs16UwQCsVrVM+dAM5j5k4A0GPxXwXwRFCCVRMtY4fmHWPOtnWr/d+TSnl2+todJ5FMZSWmZecepGwzge0eGp2cz2EqjWCYm2pjakUmnQlCoXh17u4F0GX6uwvAnuKLE16CdPwNq41h5sQzsraRaREdUYDvzZqAvUdO5Dhg45bVdpqBV9oPe5rX7jiKQpj12EY829aRszo3SleoWbIR7MyFi2c1Z8xS5uvWMKwGPansMFWvSraYn4E5EmnWYxsxt7VBsnaFqsHrir8DwCYiMsorzAbwBhF9GwCY+YdBCBcWgrRJm4+tAJl1MDNw+WdGYN7Us7Dno0+wZO1O2wbwO/Z9jK89uTlTPK43xb6Kos1uGYPmUYNx5aMbAXDGzr3w6W1YfmNrzuq8JhrBsuvPBwC89vvDeHzj+7AU/8TAqIK4qmDF6x/myM1AVq2iiAJPSraYn4FdIbmVm/dizYLpON6TCo3DWRCCwqvi/73+Y/zLrtZfDwpCqDARZLVJu2ObeeHtQ/jGxZ/CkrU7HecfUhNFTFWySi/7NZ0c70khrirosRwDYFsTiFFi4mtPbrYtKvdJbxp3P7sdx3u0fQ2571y1FQBl7aMqCqY1Dc85hplifwZO5qbjPSlM9mB2Kzdhi4gSKg+viv85AP8AYJxpH2bmSUEIFSaCtEnbHdvKhl2HXecvRlE0p2NMGD0ED14zCXeu0spIpNKnun79+MVdOXX+zRhK34xKihYMbMJwApvr4luVWrE/g0otJAeELyJKqEy82vifgubI/UsAs/Sfq4ISKkwEqSS8NGi/+Jzcrl3m+YtRVdLtGNranPVnPe2vzu4Elq3P7RsQUQi1MeevVIrTOc5h87k4ZQAX+zOo1EqcYYyIEioTryv+Q8z8m0AlCSn52vUV89if9GZHzcyb2ojWs+rzzl+M/rZ2x8juR6A5ZDXb/wWIqSoSyWTWMb520Vn4t1c/yDl2bVzNPC0Yx7CeSz5zTrE/gzD2BM5HmCKihMrGq+K/m4geB/AitCbqAABm/q9ApAoZhgO0bc9RtIwdiqaRzq6Nzu4EXvv9YRzu7sH0puGuY41jmxXQkeM9OfNMaxqO5Te2AmBMGD0EALDhvUN4948f42BXAn/ePBJnjahzncep3aBbYTU7RaOA8PGJZM4KPB4h3HzR2fjMqMFZCnrxzGZMHDMka147hZtPqQWhqCutkFwlm6iEcOFV8f8NgHOhNUzPBJ4AqArF79WuurqtA7f9Z1tW1Mq8qY247+rPuh7frIDq6+JZNwvr3HNbG/CLTbuzIml++vL7AIBB8YitfHbyM5D3nOwUzSe9Kdz+q624umU0nvldB6KqghSfsv17UdB2CteLUqs0RV1sgnz6FKoLr7V63mJmd+0VIOUs2eA1nd+tnMELt12cd+XvdW4vmOWzO0Y8okXsmFtKOpUoeLatA3eaeg6bqY0p6E0Dd1/VjOunnJn3XPpzA5dSIlE9glf6Wo//dSJqZuadRZYr9Hi1q+49cgJaLbtcBdm252hBit9L1I8dnOaMfHbHUBUCmGDY7Z3OCdBMUUMHRvGNp97EJ5b+AEbkzpI1O3HFhDMclZDXJybz00JtTMXxnlQmYa0/0VfFXe1PPkLf8ar4pwO4iYjeh2bjJ1RJOKdXu2rDsBrbDFbAW0kGr3N7IZHiTNkFu2NoDmTn6BorE0YPQdrlydDppmG0fFyoPzF4icGvr4v76vRVaUg4phAGvIZzXgHgHABfgBbGmTeck4jGEtF6InqbiHYQ0bf07fcQUQcRtek/V/blBILGa+hffV0cD107GYolTn3e1Mac1b5T6QHrdru5/3zC6dZQ+BwGRJVM9676ujgWz2pGLKKgNq5iQFTB0jmTsHTO5LznZMgDIHOMgZYG84B206iNqdjw3kFseO8QOrsTmdDMbzz1Zo6ZyK0AmlPIYl97DoQBCccUwoKnFT8zf1jAsZMAbmfmN4loEIAtRPS8/t4jzPxQAccsC14jSoxxblE9Tis+p+3muX/w3E783x2netufVV+DaU0jsHLznhzfgjk2fsmanYgqhN5kGndfNSGzwnQ7J7M8J3qTICLEVAW9qTTmto7Bs1v3Zzmcv/jjlzMZuSppNX/ssnoB96cLO9MUpxlXProRcbWyV8kSjimEBa+mHt8w834A+/XXXUT0NoDK+2/V8WpXra+LY9Zk5wbkdrHqzaMGO243asf0JlPY9P6RrOO933kCS+eMxoVnnZY3Nt5gydqduGLiGa59ce1LSTB69eJqz27dn6lrUxtTMfPRjVlKPsVAykbpD4yqSINdI1FsK4amNNOUUVKiWCUzSo2EYwphITDFb4aIxgE4D8AmANMALCCieQA2Q3sqOGKzz3wA8wGgsbGxFGIGjtOKr23P0byr3D87Z4TtMTfsOoxvf+HTBcXG+5HTjLmuzdY9RzVnsQdu+tyZuPmis13ntoYsJpIpKApl3bzMpqJKim6RcEwhLASu+ImoDsDTAP6emT8mop8AWALNu7gEwMMAcpq6MPNyAMsBLZwzaDlLgdOKr2Xs0Lyr3PXvHrI9ptHQvdDYeK9yOh2jYViNbY1+lbSVv5l/e/UD3HzR2a5zA7nRPTMf3Zgz//aOY/jy8tcqzklaiRnDQv/Dq3O3IIgoCk3przCyfJn5ADOnmDkN4KcALgxShjDh5ChuGjkIi2c2I6YSamMqYirpsfaniEcUtDQMydpmNHR3chYb88V1p2w8omSZgaz7mJ25hpy1MRUqaV227BzB9XVxLJ0zKasLl0rAV/6kMaduT76uVtam75PHDsWO/R8jZboJRVXC4lnNmYqlpxq8V46T1Dg3UfpCuQhsxa83aP/fAN421+snolG6/R/QWjluD0qGMGK34lvd1oEla3ciFlHQk2JcMfEMPLt1f9Z+vek0/vdf/wneP9Sd1dA9X3jgqSJrWo5B18kkfvziLixbvwsxVXXN5F08sxn3rtmJmpiK3lQa8y8+G9dNacxRWAytOXxNVEGiNw1FITzT1pFTodPtaWPF6x/i3jU7EVMJSb2uz7Sm4Vj09LasLGWFgLHDBuaYohLJNH6xabdrg/egkcQqoVLw3Gzd94GJpgN4GcBbOFXm4R8A/BWAFmj64gMAXzfdCGzpz83WvWbnfv9LE3OyY/NlFXs9djxCACirzLLdNqeM5XxzmIu02ZljVrz+Ib77TPb9f0BUwUNzJmHh029lJY4Nikew7Prz8LUnt+SUhTY3pC81Ep8vhJG+Zu76hpk3Iqf6OgCttr+g4yU7tzamYuLoITnb8zlvvWb+2tXJt9vmlLHsNseAiII7v/BpXDV5tGME0b2/2ZGznRm4bWUberOThTN9AhZc2oSHn38v672Yqpatd29QzXoEIQgCtfEL+fGSnZtitjWR5HPees38TaZzm6jnq53vVf6TyTT+6b/fwS827ba1we89cgJRNfdrmEimc5S+2Udx3ZRG/anEXb5SYNz8zOTzZwhCORHFX2bsHL7zpjY6ZtUaDtD2A13Ye+QEFs9sdhxrPnatTcatwa0zxmPpHH1cXEUsouB7V03IbDMfG0COU/iWS5oQj2jOX5WQk72cSKbx8PPv4XP3v5hprmLQMKwGKRtzo7WJ/MCoioevnYQz62szzl9z9nE8QrjlkibvF76IVFp8fjGb1guVSWA2/mLSn238BlbHoJ2j0LAjc5qRSDEGRLX79uJZzZg4eoijU9E41qb3O/GD597Jei8eUfDqXZrdfsXrH+Le3+zIKrVsdkRba+jMbW3Ays17EVUU9KTSuOzc0/HiOwehEHDCweZv5ycwqnKqROhNpXHHFz6Npb9919KblxBRkOWQnt0yBp3dCazYtBvL1rdrmcVlsq9XSmVR8UVUF042flH8FYKbE9WppLIdKzZ9iHt/sxNRlbIcrsVyFOdjUDyCp26ektPU3HyjA4ApP3gBLi19M7IB8FQ2uxSEParHa4lxof9QcueuUFzcnKh+6r1cP+VMXDHhDN9ZvoWWiLbiZAIxJ6Bt3XMUNdEIuhLJnHFW2YzXVrl37DuGITWx0CrhciC1ggQDUfwVgpsT1a89uZAs30JKRMcjhK9OPwtPbHw/y0STT8nUxlQkkinXMWbZrHKdTKbwtSc355iFgqQSTCiV5osQgkOcuxWC2VFrOD4HRBXHksp9OX4+R7GTE9r699I5k7Hois/g1bsuw1M3T8Eri2bkVYar2zow67GNUHQPcVwlV4e3Va54RAGz1l2sVKWPK6XcstcS40L/R2z8FYZhRzY6VHkxZZj32XfsBADChNGDHR3BO/YdcxyTzwntx85tzPXxiSQG10QwekgNZj22McsGHYsoeO7W6RhWG/Mk17ETvbhlxZtZZiInv4LTdconu3Xc1j1HccPjmwqasxyE3RchFA+x8fcT/LbdM0wQALIUakQBfji3JWcFnq/7lXX+fH+7yXXHr7ZmRe5EFIIlihNxVcFz2/+If3mp3dWMYszb2Z0oyJzh1VRjN25a0/CKMqFI60ZBTD39mKyMUks0TjIN3Llqa06RtnwmCz8x4E5j2w905Sh9TSbWq5KeoieVxrL1uzybUQoxZ9idt/XauF0fAGJCESoKWfH3Y/JF4qiUHdGRL+rDjwPTrdPYnau2OXbniqkKAEY8ojlmb7mkCcs3/AGJ5CkzSr5IFL+lj+3OO5HknKJvbtdHyi0LlYQo/n5MvkicFGebI9yiPvzUo8nXaazHJUCfCFh760UZ/wUALHup3VYmN/yYMxqG1aAnlRtF9Nj69qxqpPmiYsSEIlQKYuqpMPyYWsxmDyPL1yCiAEvnTM6qzQ8Acy9oyBo3t7UhE8cfsdRiMOLlrfI41a4xOo05EVUJ37uqOctpbWe6WTyrGXuPnCjYBGWlvi6OBZfmlnOOqdn1diQqRugvyIq/gigkVtwwQazYtBuPrdsFVSGk0sDds5tzmrz3pFKwNtNauXkvvnXZeGzvOIbuRPaq2Cle3k+nsZhKuHf2BIwZVoM9H53QG8M7N5zf3nEsZ4y1l8Dimc2YOMa5hIUd101pxGPr27NKPds9WYhJR+gPSDhnhdCXdHunfdcsmJ4TPmnFqH8//+dbcsZZ2yua5XGqXeO03cv52Y2x6xsAAHVxNdPQxXxzdAtl9FNvxymstv1AF9r2HEXL2KFoGjnI8boKQimQcM4Kpy/p9n6avFvRVuiUM64mooAUymqSYpbHaWXstN3L+dmNsesbACDzdGL2Q+R7YvK6mreGyMZVAimEPzlzGF5u78yMmze1Efdd/VnHaysI5UJs/BVCX9Lt/ZheIopWsdNsw54wenDOuDQ4b71+p96y5u2Gbb42puY9PzsnrF3fADPGzcNrdq0hG5BbfhqwD5FNpBgne9NZSh8AnnxtN9oPdDnKZhxPSiQLpSbInrtjATwJ4AxorReXM/M/E9FpAP4TwDhorRfnMvORoOToLxiORaspwku2rHlfVSH0phiLZzWjaeQg22ParXofvGYS7ly1FSopSHEaS+dMBgBbebxgV07ZXObZsNUbztX6ujg2th/O8kEYDmpDjnSa0WOTB3DsRC+OnehxfaIwX0O3JDa/xera9hx1NPnkewKRDFshKII09SQB3M7MbxLRIABbiOh5AH8N4EVmvp+I7gJwF4BFAcrRb3Bq1O7F4Tu7ZQy6TiYz9faXrNmJQfGIo3nDrqE6QJpZhclRHi+sbuvAwlVbkUhqRzXs8ys378WaBdNxvCelOXHX7sy6CSxZuzMr/p+I0DxqMJpGDkLzqMG48tGNGUkNkqk0blnxJnpSqZwyzyeTmm3ezsHdm2LbsFW/xepaHEo25AuPrYSib0LlEpiph5n3M/Ob+usuAG8DGAPgagA/04f9DMBfBCVDf8RqJvFaHKyzO4Ela3eiJ8U43pPKGutkkjHvu+jpbUgk0/ikJ4VE0vu+zsfKNc9EFSXjKF2ydmfWed37mx054aS9KcaVj27Es20dON6TQtymhWOKga5EEolkrmmKmXHkeE/WNUwkOSe5zFwCur4ujrmt2SGvEUVzRF/UVJ+1fd7URsfVvlu7xkop+iZULiVx7hLROADnAdgEYCQz7we0mwMRne6wz3wA8wGgsbGxFGJWHH4cvkE4hwup4+5mKjFs+rbzqUqOGQcAevSb0JoF032Xja6JRtC25yhUsvEO28gFaDeulZv3Zr2vEGHNguloGjnIc1SPm8/Gy/UWM5DQFwJ37hJRHYCnAfw9M3/sdT9mXs7MrczcOmLEiOAErGD8OHyDcA4XUoTMyVRibqRuNybFjLuvakbMWsUNp54UrOWZI3m+3b3pNDqP9+B4T7bD2M7BbY0sypZdzRxjWG0M54wchGG1Mcd5DaVt7Zds+DTyObpXt3Vg2gPrcMPjmzDtgXU5fYwFIR+BrviJKApN6a9g5v/SNx8golH6an8UgINBytCfsTp8e1Ip3HJJE44c77G12Ts5h/3OY+wLaJEvfladdjIvuPQcXDelMet4i2c1Z1pEJlNazZ4rJpyBKeNOw5WPbswq+2Aoxcljh2b5HF5pP5wls53zeMnanTky3jt7Ij49sg7/d+cfcfbwOjSPGpyRy+6mlEimUBtTPdnlrWOMfslmn0ZPKo3Lzz0dL7xzIKeBjZ/SGYLgRGAJXERE0Gz4HzHz35u2LwXQaXLunsbMC92OJQlc7pgjZIiyY8uLGSniNfLF77GszswTvUkQEWKqgpO9KSgKYUBEzbrh+E20susXsPfICXx5+WuWhDAFl507As9tP5B1HPP1NOY3N71nBlLpdJYD2UsCmlsiXUwl3DrjnKx6QZVW+18oL+VI4JoG4EYAbxFRm77tHwDcD2AlEf0tgN0Arg1QhqrhX17KLjeQSDGQ4pzVYF8KiZlr3vd11WmWw+54AKNXj9lPpRi9qWRmnlcWzcAri2Z4uoG59Qs4crwnR9kmkukcpQ9kX89XFs3AmgXTM1FETpnPXhLQ3BLpelKMZS+1Z56GAGmfKBSHIKN6NjIzMfMkZm7Rf55j5k5mvoyZz9F/fxSUDNWCnd3ZwByREuR8fZnHTX4rZmXqJ5rIDi0SKNtnYONCsJ3fKYrIjH0CmrdEOgOFSO88plHsQnGSQFadSMmGfkAxG7EXOl9f5vETG1/M82kYVgNSsgsOqQoh5dArwDq/VeaoSlAIjo3lN7YfRsq0T1TVTEdNIwdh8axmfPfX23Pm+6RHK4S3dM7kjDmrWIXiJFegepGSDf0Au/LLRpPyIMoGF3vVaT1eRNGUovV1sc/HmDceIQyMqohHCA9dOxnzpuaGD1uvp1XmeETBN2ecg7W3XmTbWN4wZ5l9AAoB05qGAwAmjh6C2phqK2ciyTlx/F5yL9xW8pIrUN3Iij9E9MXxal4F+mnEXiiFrDrdGsVbjwcg8/rI8R5sbD+M4XUxnHvGYGx47xAAxoTRQzyVrHCb1y4j+b6rP4t5fzoOG9sPIR5RMWhABINrYjlN3s0lr5et34XlG/6Ax9a3Y8GlTfjixDOyZNqx7xgUS75ATFWxY98xDKmJoTamIuUSaOElb8I4T2vWs91Kvpi5GULlIYo/JBTjsbvUHaD8zGecnzkSBkDWedo5Yle3deD2lW055RYA7Ungry4cmxWiab1uTpU0jZpERkaygeGkbho5CDv2f+zpM9Ec65xpD/nw8+/h4effQzxCICLMbW3Af/7PnpxsZWs/A2t1TzP5TFzGeapEmZwCN8e7OImrGzH1hID+/thtPj+jmbpR3dLtPDu7E1i4aqut0ge0kg1Pvrbb8bq5VdJc+PQ27Nh3zLarmJ+yCW6O6URSm+vJ13bnKP2YqoCZkUhy5vh2Sr82ruaYuKxmHLOs1mQ06zkZ+xVqrvPrDBbncTiRFX8I6O+P3W5lGtzOc++RE1q9feQqMyfMx8s376u/78zpKuanbALgzzFtJs1pqERIwtm8UxtTce9VE3Dpuadn5rR7Mjyzvta1YmhvOo3tHcfw5eWv5Ty9+DHX+X0qFedxeJEVfwjo74/dhUYdNQyrQYr9KVXz8dzm7Uml8G+vfJCzffGsZseyEXaynnIQ+/tXSqaRefpxIsWcpfSdnkLsSjwAp54WFs9qzil657fInt+n0v7+FFvpiOIPAf29ibf5/Iy4eSMCye086+viWDpnsmPNnahKmDe10fG62UU7RRWtXeOCS89BzBKHXxtTMXH0kJx93Zq8A5qT99W7ZuDv/uxsX9dlQFRBzBSx5HQuhrlkx75jtvkT1jpFA6IKvv8XE/GT68/H8htbMXbYwJz9rPkB+bBzTrvlbhSa6yGmodIgpp6Q0N+beBcadWTst2PfxwAYo4fUYN+xkzBH9XzrsvGO1y078qZdbzbPOK0uZlsIzryiz9fk3Wy2qK+LY9EXP4OG0wbie89sh3UxH48oSHNuyefnvnlR1rWwnotdrwAzTnWKNrYfxvyfb8nU/klZztUuP8AJa/8E69x2FPIUK6ah0iHN1oV+j1ONHKNIWz5F46fRfWd3Ap+7/8UcJfn9L03EoHjEc40hp3kjCqAqSqZrmd0xNBnWZUUrRVUCATmlrY1aQU43YjsZAO1GtnSOu/x+m9db54lFFDx363RpWt8HpNm6ULU4OWonjhniqeaP374HMVXNhHYCp0xI1lV5IX0QaqIRLLv+fAypiToeY8Wm3VlKHwAGRFTc/oXxePD/vItPek85tDmtNbOJO9xI7GQYGFXxrzdegIvHu5dL9/MUazdPTzKNK3/8Mh66Nv9TieAPUfxCv8fN7OAlF8GL2cKcJOZmQvKT++A0rzWRzExndwLL1u/K2d6TSmN603Dcj3eytmsOZs6UubbG/NvJkAZjwujBns7B6/k6OeJ7bAoNCn1HnLtCv6evMesAXPc3N0aZ+ehGTP9UPaIqoTaWG4PvBa/z2u2zY98xxNTc0g8LLm3SagLN1JrZ1MZUxPQyFGasDlgv187OIWtsaz/Q5fie1YF7yyVNOQ53O5nszrsQZ3A1O5JlxS9UBcWIWbczC9mVlH7hnUMAAOY07r16gi8zhdd5nfaxcwDHI4TrpjRidVsHlqzdiVhEa2N5x59/Gj98/r2ssXYOWLdrZycvA45Z2sZ7duO1KCCGml03z9Ep3BdncLU7ksW5KwgW/Dhz7RqjmIlHFLx6V+5+fZ3XbR87B/C0puH2Du5Zza7RSn7ljUcIAOX4GLT3NMVudnzbjbercmrnwPZ7rYqxb6Uhzl1B8IgfZ26+zF1VIc8Z2IVkcHt1AG+1afYSVRRMHK05uLWYfnK03dsVwrPNjGaCU2sFVSG9EN4p57JKSk7u8oCIimXXn4chNTHHp5y+ZLv390x5L4jiFwQLfmLQDRv4nau22a5yU2l2jV0vdN58+1gdwG7HztdG08ks0jCsBid6s590Eqm0Y4WNVFpzJGeNT6Zych40+Ye4KuG+ZLv390x5LwTm3CWiJ4joIBFtN227h4g6iKhN/7kyqPkFwQ4vDj03h6bd/tOahuOn81rxd392dtY/VFQlLJ3j7Ni1HiufI9U83uwAXjyrGTFVQU1UQTxivw+ATGmJgTE1Mw6Aa2kFu9ILd6zahvYDXQAAIueWZTH9YhiZ00vnTMLSOZMz5xdT7ff/9uXjXR3YfSky5+U6VwNBrvj/HcBjAJ60bH+EmR8KcF5BsMWPQ8/OoenmyDSaxKsqIa4oSKXTuHu2s2PXSRYnR6pdM/oBERUnkykkU6yto1PIKm9hnWPuBQ0AWDe3aMvsfGYPt/j6W2ecA1WhnGxkQKsTdO9VE9AydmhOcpiRSf3oul1IWj3RAB767bs4Y8gAT08dhWa79/dM+XwE6twlonEA1jDzRP3vewB0+1X84twV+kpfHXr2jsxcZ6UZt+xeP7I4Zc86EY8oWHvrdMx6bKPrPkbWrnWcWRa3uWMq5WQCF+t88snQX52xxcbJuVuOOP4FRLRNNwUNcxpERPOJaDMRbT506FAp5RP6IX1tEG+3v6qQXjbaHqfj+5XFTzN6Q6423ZnrhlOBN7tCdzGbSnkRVUFUyTXVxCLupjEv52O+Hn397IRcSu3c/QmAJdCeM5cAeBjAV+0GMvNyAMsBbcVfKgGF/klfHXp2+9s5K70c368sfmv+p9KMlrFD8+7jVODNrtBd86jBuPLHL2et8FNp1iJ4TNPEVMrU13FzCnuVzen8q80ZW2xKuuJn5gPMnGLmNICfAriwlPML1UtfHXp2+1udlV4bw/uVxTrePI9RfM3AcCg3jRyUM0e+EtZudfmbRg7CQ9dOdj3/AVEFD107GU0jB7nW47c7f6/ltavVGVtsSm3jH8XM+/XXtwGYwsxfyXccsfELfnFqwN6XhvZO+5u3AfB8fOux8slsLmdtngdApmy1NQzS6xxe5dQa3x/C8LoBmPqpen3uY/j4RG9WQ/qte47i+sdfz+pwVhNVseiK8bhq8piMLNb8AXP5bbNTWBurvTcwquKDzk/QMnao78qddteyP99AnGz8gSl+IvolgEsADAdwAMDd+t8t0J6PPwDwdeNG4IYofsEPlZiO7yRzuc/FPP/JZAqpNGdKQhCAiEpQFcppZN91MonvPrPd9phRlfDwtZNzyjfMbW3Ays17c0o9GNujioLuRHazynlTG3Hf1Z/1dS52pSTC/v0olJIr/mIiil/wSiVGgDjJnC/iphxyeSFftJPXMV554baL86783c4l7N+PvhCmqB5BCIxKjABxktkuMqeU5+I3msggX7RTZpyHMV5o0xPU3HA7l7B/P4JASjYI/YpKjABxktkuMqeU5+I3msggX7RTZhz7P7YdLWOH5h3jdi5h/34Egaz4hX5FofXjy4mTzHaROV7Pxbq9kHFmuWpjKlQFsEbtq4SMrTyu1/e3i3Yyh/tHVcKCS5tw++c/nekNYET2xCMEo0WAUerB2D4wqubMP7e1Acd7Unk/S/O5xFXtKDFVK3GxeFYz9h45kXUNNrx3EBveO1T270hQ31Wx8Qv9EqfolXI7S93wG4nk1SE894IGrNyyN/84kxPVPG7F6x/i3jU7EVO18gxNI2qxY39XRo6ZE8/AbZ8fnxMlY4122rHvY7z2+8N44pX3Ada6f8UjWnXOu69qRl08goWrtkFVCKk0Y8GlTbhuSiM2th/ObE/0pkCklZ02GsjXRCOeP8vO7gRWbNqNx9a9h4iiIpFMQVG08hfGNfjlG3syZSgiCvDDuS1l+Y4U47sqzl2h6qlEx68TfhzCVoo9Dui7g9WuLr+f+Y3x+T7LQhzW8Qjh1bsuK+l3pFjfVXHuClVPJTp+nfDjELZS7HFA3x2sKilavf4C5zfG5/ssC3FYq1T670jQ31Vx7gpVQyU6fp3w4xC2UuxxQN8drClO61VDC5vfGJ/vsyzEYZ3i0n9Hgv6uyopfqBr6U+q/H4ewXTmEvoy7qKk+S5Z5Uxs9ZdDaOVhP1eqfjKVz8p+PUa5iYExFVCWoBAyMqohHyNNn6Vb+wrgGUfXUDSiiAEvnTPZcwdXsiG0/0IV/f+UPWLN1n2/nbNDfVbHxC1VHX8s2hAmvDuFij2s/0IW2PUeLXjYh3/zbO47hvjU7oJKSccxGVQWpNGPpHH/9gp3KbFhLSXj5jlgdsX9y5jC83N6ZeZ8A/PNX/DuJ+/pdFeeuIAgVTT7HbLkc9V4dxjEVeO07l5dUPnHuCoJQ0eRzzJbLUe/VYUwITyCBKH5BECqCfI7ZcjnqvTqMGeEJJBDFLwghJmxZxqXC7rzzOWa9OD+DuJ52jlirA5wA3H3VxKwM4XIiNn5BCClhzjIOknznXWj/g6Cvp50D3OhdcOxEL5as3Vnyz1Kcu4JQQfSnLGM/BHXe5bye5ZxbnLuCUEH0pyxjPwR13uW8nmH8LEXxC0II6U9Zxn4I6rzLeT3D+FkGpviJ6AkiOkhE203bTiOi54lol/57WFDzC0Il05+yjP0Q1HmX83qG8bMMsufuxQC6ATxparb+IICPmPl+IroLwDBmXpTvWGLjF6qV/pRl7Ie+ZAbbEYYm6+X4LJ1s/IEVaWPmDUQ0zrL5amgN1wHgZwBeApBX8QtCtVJfF68qhQ8UP/rG7niTPRSVKzZh+ixLbeMfycz7AUD/fXqJ5xcEIcR0diew6OltONmbRlciiZO9aSx8elvBse/FPl5/IbTOXSKaT0SbiWjzoUOHyi2OIAgloNgRMGGMqAkDpVb8B4hoFADovw86DWTm5czcysytI0aMKJmAgiCUj2JHwIQxoiYMlFrxPwvgJv31TQBWl3h+QRBCTLEjYPwer1pKZAQZ1fNLaI7c4QAOALgbwDMAVgJoBLAbwLXM/FG+Y0lUjyBUF8WOgPFyvP5YIkNKNgiCIDjQX0tkSMkGQRAEB6rNCSyKXxCEqqfanMCi+AVBqHrCWFYhSALL3BUEQagkZreMwbSm4VVRIkMUvyAIgk6YyioEiZh6BEEQqgxR/IIgCFWGKH5BEIQqQxS/IAhClSGKXxAEocqoiJINRHQIwIeWzcMBHC6DOIVSSfKKrMEgsgaDyOrMmcycU964IhS/HUS02a4GRVipJHlF1mAQWYNBZPWPmHoEQRCqDFH8giAIVUYlK/7l5RbAJ5Ukr8gaDCJrMIisPqlYG78gCIJQGJW84hcEQRAKQBS/IAhClREqxU9ETxDRQSLabtp2DxF1EFGb/nOl6b3vEFE7Eb1LRH9u2n4BEb2lv/djIqIAZB1LROuJ6G0i2kFE39K3n0ZEzxPRLv33sHLL6yJr6K4tEQ0gojeIaKsu67369jBeVydZQ3ddTfOoRPQ7Ilqj/x266+oia5iv6wf6PG1EtFnfFtprC2YOzQ+AiwGcD2C7ads9AO6wGdsMYCuAOICzAPwegKq/9waAqQAIwH8D+GIAso4CcL7+ehCA93SZHgRwl779LgAPlFteF1lDd23149bpr6MANgH405BeVydZQ3ddTTJ8G8AvAKzR/w7ddXWRNczX9QMAwy3bQnttQ7XiZ+YNAD7yOPxqAP/BzAlmfh9AO4ALiWgUgMHM/BprV/JJAH8RgKz7mflN/XUXgLcBjNHl+pk+7Gemucsmr4usTpRTVmbmbv3PqP7DCOd1dZLVibJ+Z4moAcBMAI9bZArVdXWR1YmyyppHrtBdWyBkph4XFhDRNtJMQcbj0hgAe0xj9urbxuivrdsDg4jGATgP2opvJDPvBzSFC+D0MMlrkRUI4bXVH/HbABwE8Dwzh/a6OsgKhPC6AvgRgIUAzM1lQ3ldHWQFwnldAe2G/1si2kJE8/VtYb22FaH4fwLgUwBaAOwH8LC+3c72xS7bA4GI6gA8DeDvmfljt6E220oqr42soby2zJxi5hYADdBWQhNdhodR1tBdVyKaBeAgM2/xuovNtnLLGrrramIaM58P4IsAbiGii13Gll3e0Ct+Zj6g/3OlAfwUwIX6W3sBjDUNbQCwT9/eYLO96BBRFJoiXcHM/6VvPqA/skH/fTAM8trJGuZrq8t3FMBLAK5ASK+rnawhva7TAMwmog8A/AeAGUT0FMJ5XW1lDel1BQAw8z7990EAv9ZlC+O1zQgcqh8A45Dt3B1len0bNNsYAExAtoPkDzjlIPkfaE42w0FyZQByEjQb3I8s25ci26HzYLnldZE1dNcWwAgAQ/XXNQBeBjArpNfVSdbQXVeL3JfglMM0dNfVRdZQXlcAtQAGmV6/Cm2xEtprG8iH1YcL+Etoj3C90O5+fwvg5wDeArANwLOWD/+70Dzi78Lk/QbQCmC7/t5j0DOUiyzrdGiPYdsAtOk/VwKoB/AigF3679PKLa+LrKG7tgAmAfidLtN2AN/Tt4fxujrJGrrrapH7EpxSpqG7ri6yhvK6AjgbmiLfCmAHgO+G/dpKyQZBEIQqI/Q2fkEQBKG4iOIXBEGoMkTxC4IgVBmi+AVBEKoMUfyCIAhVhih+QSgAInqciJrzjPl3Ippjs30cEV0XnHSC4I4ofkEoAGa+mZl3Frj7OACi+IWyIYpfqGqIaCERfVN//QgRrdNfX0ZETxHRF4joNSJ6k4h+pdc7AhG9RESt+uu/JaL39G0/JaLHTFNcTESvEtEfTKv/+wFcpNduv62EpysIAETxC8IGABfpr1sB1Ol1jaZDyxL9RwCXs1aAazO0GvEZiGg0gMXQ0uw/D+Bcy/FH6ceaBU3hA1r6/svM3MLMjxT9jAQhD5FyCyAIZWYLgAuIaBCABIA3od0ALoJWFqAZwCt6I6QYgNcs+18I4P8x80cAQES/AjDe9P4zrBUV20lEI4M8EUHwiih+oaph5l69CuTfQCuutQ3ApdDK/74Prcb+X7kcIl9rvISPsYJQEsTUIwiauecO/ffLAL4BrZDd6wCmEVETABDRQCIab9n3DQB/RkTDiCgC4BoP83VBa4EpCGVBFL8gaMp+FIDXmPkAgJPQbPCHAPw1gF8S0TZoN4IsGz4zdwD4AbSOZi8A2AngWJ75tgFIktakXZy7QsmR6pyC0EeIqI6Zu/UV/68BPMHMvy63XILghKz4BaHv3KP33d0OzS/wTFmlEYQ8yIpfEAShypAVvyAIQpUhil8QBKHKEMUvCIJQZYjiFwRBqDJE8QuCIFQZ/x/hjpQAUBLnkgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df.plot(kind=\"scatter\",x=\"weight\",y=\"mpg\")\n",
    "plt.show()\n",
    "\n",
    "df.plot(kind=\"scatter\",x=\"acceleration\",y=\"mpg\")\n",
    "plt.show()\n",
    "\n",
    "df.plot(kind=\"scatter\",x=\"model-year\",y=\"mpg\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "570de588",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df.dropna(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "48640021",
   "metadata": {},
   "outputs": [],
   "source": [
    "x=df[[\"weight\"]]\n",
    "y=df[\"mpg\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9d3f4f2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.3,random_state=11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "cba69286",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr=LinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "25b8eb4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4ab61f88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "기울기: [-0.00768305]\n",
      "절편: 46.22446799785152\n"
     ]
    }
   ],
   "source": [
    "print(\"기울기:\",lr.coef_)\n",
    "print(\"절편:\",lr.intercept_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "944fd0c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred=lr.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "622d3bab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6436669710145108\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "score=r2_score(y_test,pred)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "961b7cc7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a2e1d64f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "0b5b31e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv(\"C://Users//whs38//OneDrive//바탕 화면//archive//auto_mpg.csv\")\n",
    "\n",
    "df=df.dropna(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "751592a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "x=df[[\"horsepower\"]]\n",
    "y=df[\"mpg\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "cb85de38",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.2,random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "9cc95f83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression()"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr=LinearRegression()\n",
    "lr.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "abdacf50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6130658928397561\n"
     ]
    }
   ],
   "source": [
    "pred=lr.predict(x_test)\n",
    "\n",
    "from sklearn.metrics import r2_score\n",
    "score=r2_score(y_test,pred)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "3bbd01d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "기울기: [-0.15897476]\n",
      "절편: 39.97935798986938\n"
     ]
    }
   ],
   "source": [
    "print(\"기울기:\",lr.coef_)\n",
    "print(\"절편:\",lr.intercept_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "9d59fc80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   mpg  cylinders  displacement  horsepower    weight  \\\n",
      "mpg           1.000000  -0.775680     -0.804711   -0.777575 -0.832725   \n",
      "cylinders    -0.775680   1.000000      0.950706    0.843751  0.896058   \n",
      "displacement -0.804711   0.950706      1.000000    0.897787  0.932729   \n",
      "horsepower   -0.777575   0.843751      0.897787    1.000000  0.864350   \n",
      "weight       -0.832725   0.896058      0.932729    0.864350  1.000000   \n",
      "acceleration  0.421159  -0.504844     -0.542713   -0.687241 -0.415462   \n",
      "model-year    0.581144  -0.352554     -0.374620   -0.420697 -0.311774   \n",
      "\n",
      "              acceleration  model-year  \n",
      "mpg               0.421159    0.581144  \n",
      "cylinders        -0.504844   -0.352554  \n",
      "displacement     -0.542713   -0.374620  \n",
      "horsepower       -0.687241   -0.420697  \n",
      "weight           -0.415462   -0.311774  \n",
      "acceleration      1.000000    0.294588  \n",
      "model-year        0.294588    1.000000  \n"
     ]
    }
   ],
   "source": [
    "corr=df.corr(method=\"pearson\")\n",
    "print(corr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "343b841a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "679b41da",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "026eca9f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "3f444eef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#다중 선형 회귀"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "09eb91a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "9ca78680",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv(\"C://Users//whs38//OneDrive//바탕 화면//archive//housing.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "8bc71017",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 20640 entries, 0 to 20639\n",
      "Data columns (total 10 columns):\n",
      " #   Column              Non-Null Count  Dtype  \n",
      "---  ------              --------------  -----  \n",
      " 0   longitude           20640 non-null  float64\n",
      " 1   latitude            20640 non-null  float64\n",
      " 2   housing_median_age  20640 non-null  float64\n",
      " 3   total_rooms         20640 non-null  float64\n",
      " 4   total_bedrooms      20433 non-null  float64\n",
      " 5   population          20640 non-null  float64\n",
      " 6   households          20640 non-null  float64\n",
      " 7   median_income       20640 non-null  float64\n",
      " 8   median_house_value  20640 non-null  float64\n",
      " 9   ocean_proximity     20640 non-null  object \n",
      "dtypes: float64(9), object(1)\n",
      "memory usage: 1.6+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "4599670e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df.dropna(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "75e3d25d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df.drop(\"ocean_proximity\",axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "2196e638",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    longitude  latitude  housing_median_age  total_rooms  \\\n",
      "longitude            1.000000 -0.924616           -0.109357     0.045480   \n",
      "latitude            -0.924616  1.000000            0.011899    -0.036667   \n",
      "housing_median_age  -0.109357  0.011899            1.000000    -0.360628   \n",
      "total_rooms          0.045480 -0.036667           -0.360628     1.000000   \n",
      "total_bedrooms       0.069608 -0.066983           -0.320451     0.930380   \n",
      "population           0.100270 -0.108997           -0.295787     0.857281   \n",
      "households           0.056513 -0.071774           -0.302768     0.918992   \n",
      "median_income       -0.015550 -0.079626           -0.118278     0.197882   \n",
      "median_house_value  -0.045398 -0.144638            0.106432     0.133294   \n",
      "\n",
      "                    total_bedrooms  population  households  median_income  \\\n",
      "longitude                 0.069608    0.100270    0.056513      -0.015550   \n",
      "latitude                 -0.066983   -0.108997   -0.071774      -0.079626   \n",
      "housing_median_age       -0.320451   -0.295787   -0.302768      -0.118278   \n",
      "total_rooms               0.930380    0.857281    0.918992       0.197882   \n",
      "total_bedrooms            1.000000    0.877747    0.979728      -0.007723   \n",
      "population                0.877747    1.000000    0.907186       0.005087   \n",
      "households                0.979728    0.907186    1.000000       0.013434   \n",
      "median_income            -0.007723    0.005087    0.013434       1.000000   \n",
      "median_house_value        0.049686   -0.025300    0.064894       0.688355   \n",
      "\n",
      "                    median_house_value  \n",
      "longitude                    -0.045398  \n",
      "latitude                     -0.144638  \n",
      "housing_median_age            0.106432  \n",
      "total_rooms                   0.133294  \n",
      "total_bedrooms                0.049686  \n",
      "population                   -0.025300  \n",
      "households                    0.064894  \n",
      "median_income                 0.688355  \n",
      "median_house_value            1.000000  \n"
     ]
    }
   ],
   "source": [
    "corr=df.corr(method=\"pearson\")\n",
    "print(corr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "e88d7b8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "x=df.drop(\"median_house_value\",axis=1)\n",
    "y=df[\"median_house_value\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "68b4c3fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.3,random_state=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "52e940f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression()"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr=LinearRegression()\n",
    "lr.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "4b001487",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "기울기: [-4.34257762e+04 -4.31184775e+04  1.13945280e+03 -6.89459317e+00\n",
      "  1.06058402e+02 -3.72757736e+01  4.73379117e+01  3.96923365e+04]\n",
      "절편: -3645409.8910566545\n"
     ]
    }
   ],
   "source": [
    "print(\"기울기:\",lr.coef_)\n",
    "print(\"절편:\",lr.intercept_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "4fc80be6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6367741114899351\n"
     ]
    }
   ],
   "source": [
    "pred=lr.predict(x_test)\n",
    "\n",
    "from sklearn.metrics import r2_score\n",
    "score=r2_score(y_test,pred)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7bdfa6a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69e5e8c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "a2a10935",
   "metadata": {},
   "outputs": [],
   "source": [
    "#의사결정나무"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "aef6340e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "6ce9e12f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "4c6cdabb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv(\"C://Users//whs38//OneDrive//바탕 화면//archive//housing.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "fc564eac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 20640 entries, 0 to 20639\n",
      "Data columns (total 10 columns):\n",
      " #   Column              Non-Null Count  Dtype  \n",
      "---  ------              --------------  -----  \n",
      " 0   longitude           20640 non-null  float64\n",
      " 1   latitude            20640 non-null  float64\n",
      " 2   housing_median_age  20640 non-null  float64\n",
      " 3   total_rooms         20640 non-null  float64\n",
      " 4   total_bedrooms      20433 non-null  float64\n",
      " 5   population          20640 non-null  float64\n",
      " 6   households          20640 non-null  float64\n",
      " 7   median_income       20640 non-null  float64\n",
      " 8   median_house_value  20640 non-null  float64\n",
      " 9   ocean_proximity     20640 non-null  object \n",
      "dtypes: float64(9), object(1)\n",
      "memory usage: 1.6+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "af8420aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df.dropna(axis=0)\n",
    "\n",
    "df=df.drop(\"ocean_proximity\",axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "141a33c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "x=df.drop(\"median_house_value\",axis=1)\n",
    "y=df[\"median_house_value\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "62479e4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.2,random_state=17)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "ac26ab20",
   "metadata": {},
   "outputs": [],
   "source": [
    "dtr=DecisionTreeRegressor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "9934f339",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeRegressor()"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dtr.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "88b75455",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4499093958.247615\n"
     ]
    }
   ],
   "source": [
    "pred=dtr.predict(x_test)\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "mse=mean_squared_error(y_test,pred)\n",
    "print(mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47ccc152",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b547348e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "78fcbd59",
   "metadata": {},
   "outputs": [],
   "source": [
    "#랜덤포레스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "41245c1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "f24b8fe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv(\"C://Users//whs38//OneDrive//바탕 화면//archive//housing.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "e6b445a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 20640 entries, 0 to 20639\n",
      "Data columns (total 10 columns):\n",
      " #   Column              Non-Null Count  Dtype  \n",
      "---  ------              --------------  -----  \n",
      " 0   longitude           20640 non-null  float64\n",
      " 1   latitude            20640 non-null  float64\n",
      " 2   housing_median_age  20640 non-null  float64\n",
      " 3   total_rooms         20640 non-null  float64\n",
      " 4   total_bedrooms      20433 non-null  float64\n",
      " 5   population          20640 non-null  float64\n",
      " 6   households          20640 non-null  float64\n",
      " 7   median_income       20640 non-null  float64\n",
      " 8   median_house_value  20640 non-null  float64\n",
      " 9   ocean_proximity     20640 non-null  object \n",
      "dtypes: float64(9), object(1)\n",
      "memory usage: 1.6+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "25698e8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df.dropna(axis=0)\n",
    "\n",
    "df=df.drop(\"ocean_proximity\",axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "125715c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "x=df.drop(\"median_house_value\",axis=1)\n",
    "y=df[\"median_house_value\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "b67ed765",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "66b25186",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestRegressor()"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rfr=RandomForestRegressor()\n",
    "\n",
    "rfr.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "6704ec7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2473168340.8455915\n"
     ]
    }
   ],
   "source": [
    "pred=rfr.predict(x_test)\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "mse=mean_squared_error(y_test,pred)\n",
    "print(mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa4e6d81",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
